{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batches, Batch Normalization and Dropout\n",
    "\n",
    "In this workbook you can experiment what you learnt about how to make batches out of your data, how to perform batch normalization and dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data from data/batches_norm_drop.csv, then take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "data = pd.read_csv(r'D:\\DATASCIENCE\\DeepLearning-DS\\Chapter 03\\04. DataLoader\\data\\batches_norm_drop.csv', header=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.350140</td>\n",
       "      <td>4.248592</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.950728</td>\n",
       "      <td>3.528855</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.371517</td>\n",
       "      <td>3.149416</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.268221</td>\n",
       "      <td>4.337209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.881996</td>\n",
       "      <td>1.515387</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>-3.425455</td>\n",
       "      <td>3.349783</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>746</th>\n",
       "      <td>-1.513002</td>\n",
       "      <td>2.789840</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>747</th>\n",
       "      <td>-1.070356</td>\n",
       "      <td>3.484981</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>-2.970848</td>\n",
       "      <td>3.443924</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>749</th>\n",
       "      <td>-2.575695</td>\n",
       "      <td>2.140739</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1  2\n",
       "0    0.350140  4.248592  0\n",
       "1    0.950728  3.528855  0\n",
       "2    1.371517  3.149416  0\n",
       "3    0.268221  4.337209  0\n",
       "4    1.881996  1.515387  0\n",
       "..        ...       ... ..\n",
       "745 -3.425455  3.349783  2\n",
       "746 -1.513002  2.789840  2\n",
       "747 -1.070356  3.484981  2\n",
       "748 -2.970848  3.443924  2\n",
       "749 -2.575695  2.140739  2\n",
       "\n",
       "[750 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.iloc[:,:-1].values\n",
    "y = data.iloc[:,-1].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\VARUNK~1\\AppData\\Local\\Temp/ipykernel_17460/378936645.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'unique'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.1, random_state = 0)\n",
    "\n",
    "x_train = torch.tensor(x_train.astype(np.float32))\n",
    "x_test = torch.tensor(x_test.astype(np.float32))\n",
    "\n",
    "y_train = torch.tensor(y_train.astype(np.float32))\n",
    "y_test = torch.tensor(y_test.astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you are ready to code your own function to create batches. If needed rewatch the video we provided in Eduflow.\n",
    "\n",
    "**Extra challange:**    Are you able to split between train and test _**without**_ using sklearn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchify(x_train, x_test, y_train, y_test, batch_size):\n",
    "#YOUR CODE HERE\n",
    "    n_batches = x_train.shape[0] // batch_size # 11 / 3 = 3.66 -> 3\n",
    "    n_batches_test = x_test.shape[0] // batch_size\n",
    "\n",
    "    indexes = np.random.permutation(x_train.shape[0])\n",
    "    indexes_test = np.random.permutation(x_test.shape[0])\n",
    "\n",
    "\n",
    "    x_train = x_train[indexes]\n",
    "    y_train = y_train[indexes]\n",
    "\n",
    "    x_test = x_test[indexes_test]\n",
    "    y_test = y_test[indexes_test]\n",
    "\n",
    "    x_train = x_train[ :batch_size * n_batches ].reshape(n_batches, batch_size, x_train.shape[1])\n",
    "    y_train = y_train[ :batch_size * n_batches ].reshape(n_batches, batch_size, 1)\n",
    "    \n",
    "    x_test = x_test[ :batch_size * n_batches_test ].reshape(n_batches_test, batch_size, x_test.shape[1])\n",
    "    y_test = y_test[ :batch_size * n_batches_test ].reshape(n_batches_test, batch_size, 1)\n",
    "\n",
    "\n",
    "    return x_train, x_test, y_train, y_test    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to create your model! Remember to include the new tricks you learnt (batch normalization and dropout)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "class Classify(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classify, self).__init__()\n",
    "        self.input_layer = nn.Linear(2, 56)\n",
    "        self.batchnorm = nn.BatchNorm1d(56) # applying batch norm \n",
    "        self.hidden1 = nn.Linear(56, 49)\n",
    "        self.drop = nn.Dropout(0.1) \n",
    "        self.hidden2 = nn.Linear(49, 8) \n",
    "        self.output = nn.Linear(8, 3)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        first_layer = self.input_layer(x)\n",
    "        act1 = F.relu(first_layer)\n",
    "        second_layer = self.hidden1(act1)\n",
    "        act2 = F.relu(second_layer)\n",
    "        third_layer = self.hidden2(act2)\n",
    "        act3 = F.relu(third_layer)\n",
    "        out_layer = self.output(act3)\n",
    "        x = F.softmax(out_layer)\n",
    "        return out_layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model and evaluate it. **Extra challenge**: try to figure out how you can tell if batch norm and dropout are effective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\envs\\strive\\lib\\site-packages\\ipykernel_launcher.py:23: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss: 1.5410572290420532 | test loss: 1.7179533243179321\n",
      "Epoch: 1 | loss: 1.9332613945007324 | test loss: 1.7133086919784546\n",
      "Epoch: 1 | loss: 2.382766008377075 | test loss: 1.7076481580734253\n",
      "Epoch: 1 | loss: 1.686457872390747 | test loss: 1.7030812501907349\n",
      "Epoch: 1 | loss: 1.787933588027954 | test loss: 1.6977607011795044\n",
      "Epoch: 1 | loss: 1.966483473777771 | test loss: 1.6934256553649902\n",
      "Epoch: 1 | loss: 2.175502300262451 | test loss: 1.6877787113189697\n",
      "Epoch: 1 | loss: 1.8532322645187378 | test loss: 1.682885766029358\n",
      "Epoch: 1 | loss: 1.805775761604309 | test loss: 1.6779247522354126\n",
      "Epoch: 1 | loss: 1.3539459705352783 | test loss: 1.6742123365402222\n",
      "Epoch: 1 | loss: 1.3890548944473267 | test loss: 1.6707489490509033\n",
      "Epoch: 1 | loss: 1.995659351348877 | test loss: 1.666052222251892\n",
      "Epoch: 1 | loss: 1.7342387437820435 | test loss: 1.6621406078338623\n",
      "Epoch: 1 | loss: 0.7842833995819092 | test loss: 1.6598368883132935\n",
      "Epoch: 1 | loss: 1.9953714609146118 | test loss: 1.654881238937378\n",
      "Epoch: 1 | loss: 1.3334568738937378 | test loss: 1.6509908437728882\n",
      "Epoch: 1 | loss: 0.8751465678215027 | test loss: 1.648669719696045\n",
      "Epoch: 1 | loss: 2.068791151046753 | test loss: 1.6439208984375\n",
      "Epoch: 1 | loss: 1.5748388767242432 | test loss: 1.6402233839035034\n",
      "Epoch: 1 | loss: 1.8078120946884155 | test loss: 1.6364006996154785\n",
      "Epoch: 1 | loss: 1.7475945949554443 | test loss: 1.6324337720870972\n",
      "Epoch: 1 | loss: 1.8981813192367554 | test loss: 1.6276289224624634\n",
      "Epoch: 1 | loss: 1.359917402267456 | test loss: 1.6244101524353027\n",
      "Epoch: 1 | loss: 0.7088703513145447 | test loss: 1.6222695112228394\n",
      "Epoch: 1 | loss: 1.38962721824646 | test loss: 1.6190539598464966\n",
      "Epoch: 1 | loss: 1.9391852617263794 | test loss: 1.6143723726272583\n",
      "Epoch: 1 | loss: 2.320559024810791 | test loss: 1.6092387437820435\n",
      "Epoch: 1 | loss: 1.8234699964523315 | test loss: 1.604537844657898\n",
      "Epoch: 1 | loss: 1.6071215867996216 | test loss: 1.600603699684143\n",
      "Epoch: 1 | loss: 1.4689610004425049 | test loss: 1.5970313549041748\n",
      "Epoch: 1 | loss: 1.9496482610702515 | test loss: 1.59201979637146\n",
      "Epoch: 1 | loss: 1.1323463916778564 | test loss: 1.5890711545944214\n",
      "Epoch: 1 | loss: 2.084519386291504 | test loss: 1.58465576171875\n",
      "Epoch: 2 | loss: 1.8101227283477783 | test loss: 1.8568507432937622\n",
      "Epoch: 2 | loss: 1.8331456184387207 | test loss: 1.8519392013549805\n",
      "Epoch: 2 | loss: 1.8856720924377441 | test loss: 1.8471237421035767\n",
      "Epoch: 2 | loss: 1.3556389808654785 | test loss: 1.8438466787338257\n",
      "Epoch: 2 | loss: 1.6885550022125244 | test loss: 1.839328646659851\n",
      "Epoch: 2 | loss: 2.2521674633026123 | test loss: 1.8334718942642212\n",
      "Epoch: 2 | loss: 2.0050301551818848 | test loss: 1.828176736831665\n",
      "Epoch: 2 | loss: 0.9124408960342407 | test loss: 1.8253511190414429\n",
      "Epoch: 2 | loss: 0.9116533398628235 | test loss: 1.8225467205047607\n",
      "Epoch: 2 | loss: 1.707061529159546 | test loss: 1.8177348375320435\n",
      "Epoch: 2 | loss: 1.1074867248535156 | test loss: 1.8142824172973633\n",
      "Epoch: 2 | loss: 1.1043521165847778 | test loss: 1.811280369758606\n",
      "Epoch: 2 | loss: 1.5997121334075928 | test loss: 1.8069061040878296\n",
      "Epoch: 2 | loss: 1.4226927757263184 | test loss: 1.8037197589874268\n",
      "Epoch: 2 | loss: 1.1883809566497803 | test loss: 1.8004685640335083\n",
      "Epoch: 2 | loss: 2.6277036666870117 | test loss: 1.7939751148223877\n",
      "Epoch: 2 | loss: 1.2262898683547974 | test loss: 1.7908867597579956\n",
      "Epoch: 2 | loss: 2.0062687397003174 | test loss: 1.7858442068099976\n",
      "Epoch: 2 | loss: 1.8689838647842407 | test loss: 1.7812250852584839\n",
      "Epoch: 2 | loss: 1.4395711421966553 | test loss: 1.7773149013519287\n",
      "Epoch: 2 | loss: 1.8433846235275269 | test loss: 1.7725697755813599\n",
      "Epoch: 2 | loss: 1.208195447921753 | test loss: 1.7693793773651123\n",
      "Epoch: 2 | loss: 1.4641711711883545 | test loss: 1.7651365995407104\n",
      "Epoch: 2 | loss: 1.2801287174224854 | test loss: 1.7618557214736938\n",
      "Epoch: 2 | loss: 1.2340795993804932 | test loss: 1.7582950592041016\n",
      "Epoch: 2 | loss: 1.270329236984253 | test loss: 1.754636526107788\n",
      "Epoch: 2 | loss: 1.4343221187591553 | test loss: 1.7512110471725464\n",
      "Epoch: 2 | loss: 1.6874152421951294 | test loss: 1.7469028234481812\n",
      "Epoch: 2 | loss: 1.3625892400741577 | test loss: 1.7429174184799194\n",
      "Epoch: 2 | loss: 1.1910936832427979 | test loss: 1.7398539781570435\n",
      "Epoch: 2 | loss: 1.6047507524490356 | test loss: 1.7359305620193481\n",
      "Epoch: 2 | loss: 1.5565516948699951 | test loss: 1.7327097654342651\n",
      "Epoch: 2 | loss: 1.819897174835205 | test loss: 1.7277824878692627\n",
      "Epoch: 3 | loss: 1.1964784860610962 | test loss: 1.8007960319519043\n",
      "Epoch: 3 | loss: 1.1183359622955322 | test loss: 1.7975553274154663\n",
      "Epoch: 3 | loss: 2.136441707611084 | test loss: 1.7920798063278198\n",
      "Epoch: 3 | loss: 1.608828067779541 | test loss: 1.7880043983459473\n",
      "Epoch: 3 | loss: 0.9889190196990967 | test loss: 1.7852320671081543\n",
      "Epoch: 3 | loss: 1.6407310962677002 | test loss: 1.781058430671692\n",
      "Epoch: 3 | loss: 1.8960479497909546 | test loss: 1.7766213417053223\n",
      "Epoch: 3 | loss: 2.095510959625244 | test loss: 1.7713764905929565\n",
      "Epoch: 3 | loss: 1.2439786195755005 | test loss: 1.768201231956482\n",
      "Epoch: 3 | loss: 1.4994635581970215 | test loss: 1.764345407485962\n",
      "Epoch: 3 | loss: 1.4969233274459839 | test loss: 1.7607053518295288\n",
      "Epoch: 3 | loss: 1.0699256658554077 | test loss: 1.758274793624878\n",
      "Epoch: 3 | loss: 1.8688623905181885 | test loss: 1.7541022300720215\n",
      "Epoch: 3 | loss: 0.8296990990638733 | test loss: 1.7523220777511597\n",
      "Epoch: 3 | loss: 1.1689646244049072 | test loss: 1.7491511106491089\n",
      "Epoch: 3 | loss: 1.6206424236297607 | test loss: 1.7443171739578247\n",
      "Epoch: 3 | loss: 1.1472183465957642 | test loss: 1.7420670986175537\n",
      "Epoch: 3 | loss: 1.669812560081482 | test loss: 1.7374303340911865\n",
      "Epoch: 3 | loss: 1.8015373945236206 | test loss: 1.7334352731704712\n",
      "Epoch: 3 | loss: 1.2200464010238647 | test loss: 1.7302205562591553\n",
      "Epoch: 3 | loss: 1.5291640758514404 | test loss: 1.7266656160354614\n",
      "Epoch: 3 | loss: 0.9031940698623657 | test loss: 1.7241253852844238\n",
      "Epoch: 3 | loss: 1.1276394128799438 | test loss: 1.7212316989898682\n",
      "Epoch: 3 | loss: 1.1121249198913574 | test loss: 1.7182700634002686\n",
      "Epoch: 3 | loss: 1.4263006448745728 | test loss: 1.714726448059082\n",
      "Epoch: 3 | loss: 1.111665964126587 | test loss: 1.7121392488479614\n",
      "Epoch: 3 | loss: 1.2875399589538574 | test loss: 1.7089545726776123\n",
      "Epoch: 3 | loss: 1.382676601409912 | test loss: 1.7049673795700073\n",
      "Epoch: 3 | loss: 1.816668152809143 | test loss: 1.7000399827957153\n",
      "Epoch: 3 | loss: 1.8762893676757812 | test loss: 1.695108413696289\n",
      "Epoch: 3 | loss: 1.7294281721115112 | test loss: 1.6905659437179565\n",
      "Epoch: 3 | loss: 1.3175408840179443 | test loss: 1.6875864267349243\n",
      "Epoch: 3 | loss: 1.2775695323944092 | test loss: 1.6843911409378052\n",
      "Epoch: 4 | loss: 0.8876821994781494 | test loss: 1.3841837644577026\n",
      "Epoch: 4 | loss: 1.3011332750320435 | test loss: 1.3811239004135132\n",
      "Epoch: 4 | loss: 1.1565210819244385 | test loss: 1.3791234493255615\n",
      "Epoch: 4 | loss: 1.1858723163604736 | test loss: 1.3760590553283691\n",
      "Epoch: 4 | loss: 1.8634028434753418 | test loss: 1.3722450733184814\n",
      "Epoch: 4 | loss: 1.3855959177017212 | test loss: 1.3690390586853027\n",
      "Epoch: 4 | loss: 1.4798797369003296 | test loss: 1.3653368949890137\n",
      "Epoch: 4 | loss: 0.9544659852981567 | test loss: 1.3632121086120605\n",
      "Epoch: 4 | loss: 1.5640121698379517 | test loss: 1.3599611520767212\n",
      "Epoch: 4 | loss: 1.0330108404159546 | test loss: 1.357470154762268\n",
      "Epoch: 4 | loss: 1.2573158740997314 | test loss: 1.3538709878921509\n",
      "Epoch: 4 | loss: 1.799668550491333 | test loss: 1.349755883216858\n",
      "Epoch: 4 | loss: 0.9534336924552917 | test loss: 1.3471969366073608\n",
      "Epoch: 4 | loss: 1.2193052768707275 | test loss: 1.3440126180648804\n",
      "Epoch: 4 | loss: 1.68096125125885 | test loss: 1.3392853736877441\n",
      "Epoch: 4 | loss: 1.5052566528320312 | test loss: 1.3353513479232788\n",
      "Epoch: 4 | loss: 0.9042608141899109 | test loss: 1.3333383798599243\n",
      "Epoch: 4 | loss: 1.5647785663604736 | test loss: 1.3296939134597778\n",
      "Epoch: 4 | loss: 1.3109086751937866 | test loss: 1.3267494440078735\n",
      "Epoch: 4 | loss: 1.315593957901001 | test loss: 1.3240751028060913\n",
      "Epoch: 4 | loss: 1.4232933521270752 | test loss: 1.3200061321258545\n",
      "Epoch: 4 | loss: 1.021276831626892 | test loss: 1.3177481889724731\n",
      "Epoch: 4 | loss: 1.831945538520813 | test loss: 1.3125890493392944\n",
      "Epoch: 4 | loss: 0.9476540088653564 | test loss: 1.310727834701538\n",
      "Epoch: 4 | loss: 1.6081416606903076 | test loss: 1.3060731887817383\n",
      "Epoch: 4 | loss: 1.6823824644088745 | test loss: 1.3016706705093384\n",
      "Epoch: 4 | loss: 1.5681571960449219 | test loss: 1.2969969511032104\n",
      "Epoch: 4 | loss: 1.2085844278335571 | test loss: 1.2943339347839355\n",
      "Epoch: 4 | loss: 1.4633911848068237 | test loss: 1.2905253171920776\n",
      "Epoch: 4 | loss: 1.0359070301055908 | test loss: 1.2875460386276245\n",
      "Epoch: 4 | loss: 0.758267343044281 | test loss: 1.2861578464508057\n",
      "Epoch: 4 | loss: 1.3546390533447266 | test loss: 1.2823933362960815\n",
      "Epoch: 4 | loss: 1.2797605991363525 | test loss: 1.2789275646209717\n",
      "Epoch: 5 | loss: 0.9601419568061829 | test loss: 1.4491790533065796\n",
      "Epoch: 5 | loss: 1.589226245880127 | test loss: 1.4442781209945679\n",
      "Epoch: 5 | loss: 1.0717436075210571 | test loss: 1.441050410270691\n",
      "Epoch: 5 | loss: 1.3425689935684204 | test loss: 1.4369350671768188\n",
      "Epoch: 5 | loss: 1.241929531097412 | test loss: 1.433456540107727\n",
      "Epoch: 5 | loss: 1.448600172996521 | test loss: 1.4297846555709839\n",
      "Epoch: 5 | loss: 1.5484883785247803 | test loss: 1.4252455234527588\n",
      "Epoch: 5 | loss: 1.4806435108184814 | test loss: 1.4208743572235107\n",
      "Epoch: 5 | loss: 1.199637532234192 | test loss: 1.4172635078430176\n",
      "Epoch: 5 | loss: 0.9544480443000793 | test loss: 1.4154268503189087\n",
      "Epoch: 5 | loss: 1.533950924873352 | test loss: 1.411095142364502\n",
      "Epoch: 5 | loss: 1.5392837524414062 | test loss: 1.4063974618911743\n",
      "Epoch: 5 | loss: 0.9659400582313538 | test loss: 1.4034487009048462\n",
      "Epoch: 5 | loss: 1.2748908996582031 | test loss: 1.400183916091919\n",
      "Epoch: 5 | loss: 1.4434247016906738 | test loss: 1.3960559368133545\n",
      "Epoch: 5 | loss: 1.740874171257019 | test loss: 1.3906140327453613\n",
      "Epoch: 5 | loss: 1.4586819410324097 | test loss: 1.3874564170837402\n",
      "Epoch: 5 | loss: 0.7041455507278442 | test loss: 1.3858457803726196\n",
      "Epoch: 5 | loss: 0.9979775547981262 | test loss: 1.3831437826156616\n",
      "Epoch: 5 | loss: 1.4157520532608032 | test loss: 1.3789465427398682\n",
      "Epoch: 5 | loss: 0.9842941164970398 | test loss: 1.375578761100769\n",
      "Epoch: 5 | loss: 1.2996481657028198 | test loss: 1.3713430166244507\n",
      "Epoch: 5 | loss: 0.6278647184371948 | test loss: 1.3699965476989746\n",
      "Epoch: 5 | loss: 0.9395148158073425 | test loss: 1.3670964241027832\n",
      "Epoch: 5 | loss: 1.196531891822815 | test loss: 1.364209532737732\n",
      "Epoch: 5 | loss: 1.526033878326416 | test loss: 1.3604156970977783\n",
      "Epoch: 5 | loss: 1.2915822267532349 | test loss: 1.3562257289886475\n",
      "Epoch: 5 | loss: 1.175170660018921 | test loss: 1.3524785041809082\n",
      "Epoch: 5 | loss: 1.1844075918197632 | test loss: 1.349308729171753\n",
      "Epoch: 5 | loss: 1.6126115322113037 | test loss: 1.344580888748169\n",
      "Epoch: 5 | loss: 0.6723728179931641 | test loss: 1.3433396816253662\n",
      "Epoch: 5 | loss: 0.8422282338142395 | test loss: 1.3409122228622437\n",
      "Epoch: 5 | loss: 1.5215336084365845 | test loss: 1.336686372756958\n",
      "Epoch: 6 | loss: 1.3211543560028076 | test loss: 1.362134337425232\n",
      "Epoch: 6 | loss: 1.1756923198699951 | test loss: 1.3585437536239624\n",
      "Epoch: 6 | loss: 1.4099159240722656 | test loss: 1.3552535772323608\n",
      "Epoch: 6 | loss: 1.4183903932571411 | test loss: 1.352117657661438\n",
      "Epoch: 6 | loss: 1.4149785041809082 | test loss: 1.346979022026062\n",
      "Epoch: 6 | loss: 0.7497391104698181 | test loss: 1.3454710245132446\n",
      "Epoch: 6 | loss: 1.7399322986602783 | test loss: 1.3405810594558716\n",
      "Epoch: 6 | loss: 0.9410082697868347 | test loss: 1.3383517265319824\n",
      "Epoch: 6 | loss: 1.5764344930648804 | test loss: 1.3343920707702637\n",
      "Epoch: 6 | loss: 1.1609359979629517 | test loss: 1.3309720754623413\n",
      "Epoch: 6 | loss: 1.3031059503555298 | test loss: 1.326322317123413\n",
      "Epoch: 6 | loss: 1.2221657037734985 | test loss: 1.3228610754013062\n",
      "Epoch: 6 | loss: 1.1918245553970337 | test loss: 1.319609522819519\n",
      "Epoch: 6 | loss: 1.1173774003982544 | test loss: 1.3168545961380005\n",
      "Epoch: 6 | loss: 1.102469563484192 | test loss: 1.3141077756881714\n",
      "Epoch: 6 | loss: 0.9335950016975403 | test loss: 1.311303734779358\n",
      "Epoch: 6 | loss: 1.043394684791565 | test loss: 1.3091329336166382\n",
      "Epoch: 6 | loss: 1.1514619588851929 | test loss: 1.306552767753601\n",
      "Epoch: 6 | loss: 1.4060946702957153 | test loss: 1.3019897937774658\n",
      "Epoch: 6 | loss: 0.8385264277458191 | test loss: 1.300229787826538\n",
      "Epoch: 6 | loss: 0.839084804058075 | test loss: 1.2983392477035522\n",
      "Epoch: 6 | loss: 1.0580222606658936 | test loss: 1.295530915260315\n",
      "Epoch: 6 | loss: 0.7290568947792053 | test loss: 1.2934507131576538\n",
      "Epoch: 6 | loss: 0.9325265288352966 | test loss: 1.292256474494934\n",
      "Epoch: 6 | loss: 1.22337007522583 | test loss: 1.2890599966049194\n",
      "Epoch: 6 | loss: 1.332600712776184 | test loss: 1.2856138944625854\n",
      "Epoch: 6 | loss: 1.2546882629394531 | test loss: 1.2821911573410034\n",
      "Epoch: 6 | loss: 0.8145614862442017 | test loss: 1.2798703908920288\n",
      "Epoch: 6 | loss: 0.7993690371513367 | test loss: 1.2780314683914185\n",
      "Epoch: 6 | loss: 1.0074608325958252 | test loss: 1.2757536172866821\n",
      "Epoch: 6 | loss: 1.1021791696548462 | test loss: 1.2734438180923462\n",
      "Epoch: 6 | loss: 1.437752366065979 | test loss: 1.268546223640442\n",
      "Epoch: 6 | loss: 1.2681909799575806 | test loss: 1.264865517616272\n",
      "Epoch: 7 | loss: 1.2699341773986816 | test loss: 1.2231650352478027\n",
      "Epoch: 7 | loss: 1.2506130933761597 | test loss: 1.2201669216156006\n",
      "Epoch: 7 | loss: 1.5360276699066162 | test loss: 1.2169239521026611\n",
      "Epoch: 7 | loss: 0.9932600855827332 | test loss: 1.2146878242492676\n",
      "Epoch: 7 | loss: 1.4154919385910034 | test loss: 1.210952639579773\n",
      "Epoch: 7 | loss: 1.262462854385376 | test loss: 1.2082374095916748\n",
      "Epoch: 7 | loss: 1.1574594974517822 | test loss: 1.2059800624847412\n",
      "Epoch: 7 | loss: 1.161155343055725 | test loss: 1.203504204750061\n",
      "Epoch: 7 | loss: 1.2038705348968506 | test loss: 1.2011826038360596\n",
      "Epoch: 7 | loss: 0.9098600745201111 | test loss: 1.2000930309295654\n",
      "Epoch: 7 | loss: 1.0551971197128296 | test loss: 1.19729745388031\n",
      "Epoch: 7 | loss: 0.9471471905708313 | test loss: 1.1952356100082397\n",
      "Epoch: 7 | loss: 1.3836071491241455 | test loss: 1.1922248601913452\n",
      "Epoch: 7 | loss: 0.8538337349891663 | test loss: 1.190496563911438\n",
      "Epoch: 7 | loss: 1.1132127046585083 | test loss: 1.1878812313079834\n",
      "Epoch: 7 | loss: 1.1560025215148926 | test loss: 1.1847034692764282\n",
      "Epoch: 7 | loss: 0.8523475527763367 | test loss: 1.183814287185669\n",
      "Epoch: 7 | loss: 1.1254088878631592 | test loss: 1.1819251775741577\n",
      "Epoch: 7 | loss: 0.9880781769752502 | test loss: 1.1796079874038696\n",
      "Epoch: 7 | loss: 0.921136200428009 | test loss: 1.1776347160339355\n",
      "Epoch: 7 | loss: 0.963575005531311 | test loss: 1.1747007369995117\n",
      "Epoch: 7 | loss: 0.8933967351913452 | test loss: 1.1738578081130981\n",
      "Epoch: 7 | loss: 0.8947868347167969 | test loss: 1.171449065208435\n",
      "Epoch: 7 | loss: 1.126592755317688 | test loss: 1.1682014465332031\n",
      "Epoch: 7 | loss: 1.1347612142562866 | test loss: 1.1663674116134644\n",
      "Epoch: 7 | loss: 0.8259829878807068 | test loss: 1.165964961051941\n",
      "Epoch: 7 | loss: 0.6546716690063477 | test loss: 1.1647684574127197\n",
      "Epoch: 7 | loss: 1.1844762563705444 | test loss: 1.161317229270935\n",
      "Epoch: 7 | loss: 1.344881534576416 | test loss: 1.15834379196167\n",
      "Epoch: 7 | loss: 1.1010327339172363 | test loss: 1.1558746099472046\n",
      "Epoch: 7 | loss: 1.0547407865524292 | test loss: 1.1544623374938965\n",
      "Epoch: 7 | loss: 0.8858087062835693 | test loss: 1.1529966592788696\n",
      "Epoch: 7 | loss: 1.155962586402893 | test loss: 1.1499204635620117\n",
      "Epoch: 8 | loss: 1.2556796073913574 | test loss: 1.2528598308563232\n",
      "Epoch: 8 | loss: 1.2444933652877808 | test loss: 1.2495412826538086\n",
      "Epoch: 8 | loss: 1.206542730331421 | test loss: 1.2462280988693237\n",
      "Epoch: 8 | loss: 0.5437522530555725 | test loss: 1.2467374801635742\n",
      "Epoch: 8 | loss: 1.1236083507537842 | test loss: 1.2448866367340088\n",
      "Epoch: 8 | loss: 0.7875706553459167 | test loss: 1.2432078123092651\n",
      "Epoch: 8 | loss: 1.3371371030807495 | test loss: 1.2389914989471436\n",
      "Epoch: 8 | loss: 0.8216488361358643 | test loss: 1.237224817276001\n",
      "Epoch: 8 | loss: 1.2351508140563965 | test loss: 1.233967661857605\n",
      "Epoch: 8 | loss: 1.2380458116531372 | test loss: 1.2314883470535278\n",
      "Epoch: 8 | loss: 0.9561522006988525 | test loss: 1.2295145988464355\n",
      "Epoch: 8 | loss: 1.2059423923492432 | test loss: 1.2269084453582764\n",
      "Epoch: 8 | loss: 0.8509745001792908 | test loss: 1.2243844270706177\n",
      "Epoch: 8 | loss: 1.2757595777511597 | test loss: 1.220560073852539\n",
      "Epoch: 8 | loss: 1.131618618965149 | test loss: 1.217339038848877\n",
      "Epoch: 8 | loss: 1.2394530773162842 | test loss: 1.2133653163909912\n",
      "Epoch: 8 | loss: 0.8670716881752014 | test loss: 1.211612582206726\n",
      "Epoch: 8 | loss: 1.0819447040557861 | test loss: 1.2089847326278687\n",
      "Epoch: 8 | loss: 0.7704576253890991 | test loss: 1.2081001996994019\n",
      "Epoch: 8 | loss: 1.1034482717514038 | test loss: 1.2058740854263306\n",
      "Epoch: 8 | loss: 1.3675906658172607 | test loss: 1.2023909091949463\n",
      "Epoch: 8 | loss: 0.760177731513977 | test loss: 1.2008689641952515\n",
      "Epoch: 8 | loss: 0.8843414783477783 | test loss: 1.1980243921279907\n",
      "Epoch: 8 | loss: 1.261892318725586 | test loss: 1.1947535276412964\n",
      "Epoch: 8 | loss: 1.163754940032959 | test loss: 1.1922880411148071\n",
      "Epoch: 8 | loss: 0.8051128387451172 | test loss: 1.190546989440918\n",
      "Epoch: 8 | loss: 0.7122263312339783 | test loss: 1.189214825630188\n",
      "Epoch: 8 | loss: 0.9511174559593201 | test loss: 1.1868484020233154\n",
      "Epoch: 8 | loss: 1.065645456314087 | test loss: 1.1849201917648315\n",
      "Epoch: 8 | loss: 0.9451637268066406 | test loss: 1.182823657989502\n",
      "Epoch: 8 | loss: 0.847096860408783 | test loss: 1.1813899278640747\n",
      "Epoch: 8 | loss: 1.0116689205169678 | test loss: 1.1781607866287231\n",
      "Epoch: 8 | loss: 0.9408224821090698 | test loss: 1.1756680011749268\n",
      "Epoch: 9 | loss: 1.1168822050094604 | test loss: 1.0449771881103516\n",
      "Epoch: 9 | loss: 1.0437595844268799 | test loss: 1.043136715888977\n",
      "Epoch: 9 | loss: 0.8916391730308533 | test loss: 1.0415780544281006\n",
      "Epoch: 9 | loss: 0.9372278451919556 | test loss: 1.0398789644241333\n",
      "Epoch: 9 | loss: 1.072900414466858 | test loss: 1.0393993854522705\n",
      "Epoch: 9 | loss: 0.9087051749229431 | test loss: 1.038568377494812\n",
      "Epoch: 9 | loss: 0.8333456516265869 | test loss: 1.0379173755645752\n",
      "Epoch: 9 | loss: 0.9030687212944031 | test loss: 1.036367654800415\n",
      "Epoch: 9 | loss: 0.9477466940879822 | test loss: 1.0347390174865723\n",
      "Epoch: 9 | loss: 1.098492980003357 | test loss: 1.0319851636886597\n",
      "Epoch: 9 | loss: 1.0239964723587036 | test loss: 1.0301942825317383\n",
      "Epoch: 9 | loss: 0.4805660843849182 | test loss: 1.0300228595733643\n",
      "Epoch: 9 | loss: 1.0042345523834229 | test loss: 1.0278335809707642\n",
      "Epoch: 9 | loss: 0.9870898723602295 | test loss: 1.0259283781051636\n",
      "Epoch: 9 | loss: 0.9645676016807556 | test loss: 1.0249913930892944\n",
      "Epoch: 9 | loss: 1.3138508796691895 | test loss: 1.0218983888626099\n",
      "Epoch: 9 | loss: 0.8522499203681946 | test loss: 1.019923210144043\n",
      "Epoch: 9 | loss: 1.3919968605041504 | test loss: 1.016748070716858\n",
      "Epoch: 9 | loss: 1.1962435245513916 | test loss: 1.0139673948287964\n",
      "Epoch: 9 | loss: 1.172892451286316 | test loss: 1.0120140314102173\n",
      "Epoch: 9 | loss: 1.0252059698104858 | test loss: 1.0118294954299927\n",
      "Epoch: 9 | loss: 0.972000241279602 | test loss: 1.0101101398468018\n",
      "Epoch: 9 | loss: 0.9177939295768738 | test loss: 1.0082613229751587\n",
      "Epoch: 9 | loss: 0.8149988651275635 | test loss: 1.0065855979919434\n",
      "Epoch: 9 | loss: 0.9004737734794617 | test loss: 1.0044116973876953\n",
      "Epoch: 9 | loss: 0.8829756379127502 | test loss: 1.0027493238449097\n",
      "Epoch: 9 | loss: 0.6915135979652405 | test loss: 1.0020664930343628\n",
      "Epoch: 9 | loss: 0.8750939965248108 | test loss: 1.0017567873001099\n",
      "Epoch: 9 | loss: 0.9806476831436157 | test loss: 1.0004799365997314\n",
      "Epoch: 9 | loss: 1.1033861637115479 | test loss: 0.9980301856994629\n",
      "Epoch: 9 | loss: 0.8882438540458679 | test loss: 0.996288537979126\n",
      "Epoch: 9 | loss: 0.9566562175750732 | test loss: 0.9948154091835022\n",
      "Epoch: 9 | loss: 1.100658893585205 | test loss: 0.9929961562156677\n",
      "Epoch: 10 | loss: 0.8393398523330688 | test loss: 1.0217105150222778\n",
      "Epoch: 10 | loss: 0.9768704175949097 | test loss: 1.020020604133606\n",
      "Epoch: 10 | loss: 1.1382049322128296 | test loss: 1.0177632570266724\n",
      "Epoch: 10 | loss: 0.8197786211967468 | test loss: 1.0166926383972168\n",
      "Epoch: 10 | loss: 0.8808343410491943 | test loss: 1.0146498680114746\n",
      "Epoch: 10 | loss: 1.1101828813552856 | test loss: 1.0124151706695557\n",
      "Epoch: 10 | loss: 0.722335934638977 | test loss: 1.011994481086731\n",
      "Epoch: 10 | loss: 0.8270246386528015 | test loss: 1.0109912157058716\n",
      "Epoch: 10 | loss: 1.0284554958343506 | test loss: 1.0082627534866333\n",
      "Epoch: 10 | loss: 0.7835038900375366 | test loss: 1.0076720714569092\n",
      "Epoch: 10 | loss: 1.0941716432571411 | test loss: 1.0049519538879395\n",
      "Epoch: 10 | loss: 0.7683872580528259 | test loss: 1.0040334463119507\n",
      "Epoch: 10 | loss: 0.9428322911262512 | test loss: 1.0015090703964233\n",
      "Epoch: 10 | loss: 0.9037011861801147 | test loss: 0.9991934299468994\n",
      "Epoch: 10 | loss: 1.127771019935608 | test loss: 0.9970131516456604\n",
      "Epoch: 10 | loss: 1.196319580078125 | test loss: 0.9951258897781372\n",
      "Epoch: 10 | loss: 0.6687973141670227 | test loss: 0.9946590065956116\n",
      "Epoch: 10 | loss: 0.797906756401062 | test loss: 0.9931569695472717\n",
      "Epoch: 10 | loss: 1.1362584829330444 | test loss: 0.9899168014526367\n",
      "Epoch: 10 | loss: 0.986724853515625 | test loss: 0.9875905513763428\n",
      "Epoch: 10 | loss: 0.9933741688728333 | test loss: 0.9859586954116821\n",
      "Epoch: 10 | loss: 0.7578805685043335 | test loss: 0.9846285581588745\n",
      "Epoch: 10 | loss: 0.9280365109443665 | test loss: 0.9836291670799255\n",
      "Epoch: 10 | loss: 0.7769941687583923 | test loss: 0.9819003343582153\n",
      "Epoch: 10 | loss: 1.3131639957427979 | test loss: 0.9796349406242371\n",
      "Epoch: 10 | loss: 0.8060508966445923 | test loss: 0.9783543944358826\n",
      "Epoch: 10 | loss: 0.8268285393714905 | test loss: 0.9781512022018433\n",
      "Epoch: 10 | loss: 0.9023088812828064 | test loss: 0.9769636392593384\n",
      "Epoch: 10 | loss: 0.8318989276885986 | test loss: 0.9766716957092285\n",
      "Epoch: 10 | loss: 1.0545073747634888 | test loss: 0.9746405482292175\n",
      "Epoch: 10 | loss: 0.9542098641395569 | test loss: 0.9728482365608215\n",
      "Epoch: 10 | loss: 0.8963385224342346 | test loss: 0.9719641804695129\n",
      "Epoch: 10 | loss: 0.9360754489898682 | test loss: 0.9702889323234558\n",
      "Epoch: 11 | loss: 1.0362675189971924 | test loss: 0.9300861954689026\n",
      "Epoch: 11 | loss: 1.2317492961883545 | test loss: 0.9279137253761292\n",
      "Epoch: 11 | loss: 0.9836421608924866 | test loss: 0.9268820881843567\n",
      "Epoch: 11 | loss: 0.79055255651474 | test loss: 0.9261605739593506\n",
      "Epoch: 11 | loss: 0.9932926297187805 | test loss: 0.9245251417160034\n",
      "Epoch: 11 | loss: 0.861492395401001 | test loss: 0.922519862651825\n",
      "Epoch: 11 | loss: 0.9365092515945435 | test loss: 0.9213933348655701\n",
      "Epoch: 11 | loss: 0.8014358878135681 | test loss: 0.9195301532745361\n",
      "Epoch: 11 | loss: 0.9788539409637451 | test loss: 0.9185599684715271\n",
      "Epoch: 11 | loss: 0.7764328122138977 | test loss: 0.9182859659194946\n",
      "Epoch: 11 | loss: 0.89642333984375 | test loss: 0.9171508550643921\n",
      "Epoch: 11 | loss: 0.6659568548202515 | test loss: 0.9169928431510925\n",
      "Epoch: 11 | loss: 0.9941885471343994 | test loss: 0.9155823588371277\n",
      "Epoch: 11 | loss: 0.9342417120933533 | test loss: 0.9143326878547668\n",
      "Epoch: 11 | loss: 0.8887622952461243 | test loss: 0.9128795862197876\n",
      "Epoch: 11 | loss: 0.8725088834762573 | test loss: 0.911740779876709\n",
      "Epoch: 11 | loss: 0.9394298791885376 | test loss: 0.9095296263694763\n",
      "Epoch: 11 | loss: 1.0469764471054077 | test loss: 0.9076104164123535\n",
      "Epoch: 11 | loss: 0.8025743365287781 | test loss: 0.9072209596633911\n",
      "Epoch: 11 | loss: 0.8947985172271729 | test loss: 0.9063485264778137\n",
      "Epoch: 11 | loss: 0.7424046397209167 | test loss: 0.9061391353607178\n",
      "Epoch: 11 | loss: 0.6395342350006104 | test loss: 0.9060725569725037\n",
      "Epoch: 11 | loss: 0.8090935349464417 | test loss: 0.9052784442901611\n",
      "Epoch: 11 | loss: 1.0450217723846436 | test loss: 0.9042498469352722\n",
      "Epoch: 11 | loss: 0.7106770873069763 | test loss: 0.9036684036254883\n",
      "Epoch: 11 | loss: 0.7027021050453186 | test loss: 0.9028213620185852\n",
      "Epoch: 11 | loss: 1.0162426233291626 | test loss: 0.9008207321166992\n",
      "Epoch: 11 | loss: 0.7423141598701477 | test loss: 0.9002501368522644\n",
      "Epoch: 11 | loss: 1.0644868612289429 | test loss: 0.8977916240692139\n",
      "Epoch: 11 | loss: 0.7657079696655273 | test loss: 0.8966169357299805\n",
      "Epoch: 11 | loss: 0.9258163571357727 | test loss: 0.8947277665138245\n",
      "Epoch: 11 | loss: 0.9935585856437683 | test loss: 0.8940951824188232\n",
      "Epoch: 11 | loss: 0.8425294756889343 | test loss: 0.8935170769691467\n",
      "Epoch: 12 | loss: 0.7078815698623657 | test loss: 0.9675662517547607\n",
      "Epoch: 12 | loss: 0.9809973239898682 | test loss: 0.9668875932693481\n",
      "Epoch: 12 | loss: 0.6842729449272156 | test loss: 0.9676730036735535\n",
      "Epoch: 12 | loss: 0.8645504117012024 | test loss: 0.9661218523979187\n",
      "Epoch: 12 | loss: 0.9237959384918213 | test loss: 0.9641774296760559\n",
      "Epoch: 12 | loss: 0.7797240614891052 | test loss: 0.9645560383796692\n",
      "Epoch: 12 | loss: 0.807253897190094 | test loss: 0.9611567258834839\n",
      "Epoch: 12 | loss: 1.1050621271133423 | test loss: 0.9591437578201294\n",
      "Epoch: 12 | loss: 0.9762938022613525 | test loss: 0.956907331943512\n",
      "Epoch: 12 | loss: 0.8763152956962585 | test loss: 0.9565459489822388\n",
      "Epoch: 12 | loss: 0.8708872199058533 | test loss: 0.954730749130249\n",
      "Epoch: 12 | loss: 0.8522309064865112 | test loss: 0.9542596340179443\n",
      "Epoch: 12 | loss: 0.7208157777786255 | test loss: 0.9545449018478394\n",
      "Epoch: 12 | loss: 0.7774757146835327 | test loss: 0.953122615814209\n",
      "Epoch: 12 | loss: 0.7167645692825317 | test loss: 0.9517356157302856\n",
      "Epoch: 12 | loss: 0.9503113627433777 | test loss: 0.948943555355072\n",
      "Epoch: 12 | loss: 1.0092041492462158 | test loss: 0.9479382634162903\n",
      "Epoch: 12 | loss: 0.9446712732315063 | test loss: 0.946295440196991\n",
      "Epoch: 12 | loss: 0.6132308840751648 | test loss: 0.9439392685890198\n",
      "Epoch: 12 | loss: 0.9277154803276062 | test loss: 0.9428211450576782\n",
      "Epoch: 12 | loss: 0.760647714138031 | test loss: 0.9403938055038452\n",
      "Epoch: 12 | loss: 0.8633463382720947 | test loss: 0.9380918741226196\n",
      "Epoch: 12 | loss: 0.7418968081474304 | test loss: 0.9361765384674072\n",
      "Epoch: 12 | loss: 0.9183233976364136 | test loss: 0.9354473352432251\n",
      "Epoch: 12 | loss: 1.0083590745925903 | test loss: 0.9328439235687256\n",
      "Epoch: 12 | loss: 1.1497026681900024 | test loss: 0.9288312196731567\n",
      "Epoch: 12 | loss: 0.7542649507522583 | test loss: 0.9291080236434937\n",
      "Epoch: 12 | loss: 0.8937138915061951 | test loss: 0.9265263080596924\n",
      "Epoch: 12 | loss: 0.7277477383613586 | test loss: 0.9239739179611206\n",
      "Epoch: 12 | loss: 0.9650655388832092 | test loss: 0.923176646232605\n",
      "Epoch: 12 | loss: 0.7027696371078491 | test loss: 0.9214205145835876\n",
      "Epoch: 12 | loss: 0.7878897190093994 | test loss: 0.9214389324188232\n",
      "Epoch: 12 | loss: 0.9175688028335571 | test loss: 0.9191963076591492\n",
      "Epoch: 13 | loss: 0.7425065636634827 | test loss: 0.8749253749847412\n",
      "Epoch: 13 | loss: 0.9511203765869141 | test loss: 0.8732273578643799\n",
      "Epoch: 13 | loss: 0.6498151421546936 | test loss: 0.8725243210792542\n",
      "Epoch: 13 | loss: 0.9638597369194031 | test loss: 0.8713154196739197\n",
      "Epoch: 13 | loss: 1.0932146310806274 | test loss: 0.8688274025917053\n",
      "Epoch: 13 | loss: 0.6962414979934692 | test loss: 0.8673932552337646\n",
      "Epoch: 13 | loss: 0.762374758720398 | test loss: 0.8666303157806396\n",
      "Epoch: 13 | loss: 0.913647472858429 | test loss: 0.8651994466781616\n",
      "Epoch: 13 | loss: 1.0212615728378296 | test loss: 0.8630492091178894\n",
      "Epoch: 13 | loss: 0.6069110631942749 | test loss: 0.8637251257896423\n",
      "Epoch: 13 | loss: 0.9171974062919617 | test loss: 0.8625562191009521\n",
      "Epoch: 13 | loss: 0.8469797372817993 | test loss: 0.8617225885391235\n",
      "Epoch: 13 | loss: 0.9166145324707031 | test loss: 0.8595196008682251\n",
      "Epoch: 13 | loss: 0.956877589225769 | test loss: 0.8575724959373474\n",
      "Epoch: 13 | loss: 0.7766372561454773 | test loss: 0.8570799231529236\n",
      "Epoch: 13 | loss: 0.9542055130004883 | test loss: 0.8556142449378967\n",
      "Epoch: 13 | loss: 0.7718568444252014 | test loss: 0.8540123701095581\n",
      "Epoch: 13 | loss: 0.8066898584365845 | test loss: 0.8531279563903809\n",
      "Epoch: 13 | loss: 0.7020852565765381 | test loss: 0.8516771197319031\n",
      "Epoch: 13 | loss: 0.8811103701591492 | test loss: 0.8512609004974365\n",
      "Epoch: 13 | loss: 0.9253406524658203 | test loss: 0.8495625853538513\n",
      "Epoch: 13 | loss: 0.4515540897846222 | test loss: 0.8497886061668396\n",
      "Epoch: 13 | loss: 0.7771154642105103 | test loss: 0.8487005233764648\n",
      "Epoch: 13 | loss: 0.917229950428009 | test loss: 0.8460010886192322\n",
      "Epoch: 13 | loss: 0.6355665922164917 | test loss: 0.8459495306015015\n",
      "Epoch: 13 | loss: 0.7516875863075256 | test loss: 0.8445243239402771\n",
      "Epoch: 13 | loss: 0.9009141325950623 | test loss: 0.8427799940109253\n",
      "Epoch: 13 | loss: 0.9053722023963928 | test loss: 0.8414627909660339\n",
      "Epoch: 13 | loss: 0.7610118389129639 | test loss: 0.8418262004852295\n",
      "Epoch: 13 | loss: 1.0302174091339111 | test loss: 0.8409082889556885\n",
      "Epoch: 13 | loss: 0.8223041296005249 | test loss: 0.8401617407798767\n",
      "Epoch: 13 | loss: 0.8057839274406433 | test loss: 0.839027464389801\n",
      "Epoch: 13 | loss: 0.6473626494407654 | test loss: 0.8387280106544495\n",
      "Epoch: 14 | loss: 0.7984020709991455 | test loss: 0.7968000769615173\n",
      "Epoch: 14 | loss: 0.7950387597084045 | test loss: 0.7954623103141785\n",
      "Epoch: 14 | loss: 0.8477235436439514 | test loss: 0.7948133945465088\n",
      "Epoch: 14 | loss: 0.7109165191650391 | test loss: 0.7941818833351135\n",
      "Epoch: 14 | loss: 0.7752666473388672 | test loss: 0.7929856777191162\n",
      "Epoch: 14 | loss: 0.6874148845672607 | test loss: 0.7918472290039062\n",
      "Epoch: 14 | loss: 0.9330828189849854 | test loss: 0.7911266088485718\n",
      "Epoch: 14 | loss: 0.7794865369796753 | test loss: 0.7907005548477173\n",
      "Epoch: 14 | loss: 0.8133000731468201 | test loss: 0.7899156212806702\n",
      "Epoch: 14 | loss: 0.7224661707878113 | test loss: 0.7889426350593567\n",
      "Epoch: 14 | loss: 0.7667791843414307 | test loss: 0.7877891659736633\n",
      "Epoch: 14 | loss: 0.7927852272987366 | test loss: 0.7866228818893433\n",
      "Epoch: 14 | loss: 0.913813054561615 | test loss: 0.7853939533233643\n",
      "Epoch: 14 | loss: 0.8969092965126038 | test loss: 0.7843606472015381\n",
      "Epoch: 14 | loss: 0.865748941898346 | test loss: 0.7837918400764465\n",
      "Epoch: 14 | loss: 0.7332777380943298 | test loss: 0.7832258343696594\n",
      "Epoch: 14 | loss: 0.8024871349334717 | test loss: 0.7823230028152466\n",
      "Epoch: 14 | loss: 0.763069748878479 | test loss: 0.7818012833595276\n",
      "Epoch: 14 | loss: 0.8883697390556335 | test loss: 0.7813929915428162\n",
      "Epoch: 14 | loss: 0.69756680727005 | test loss: 0.7806908488273621\n",
      "Epoch: 14 | loss: 0.7770406007766724 | test loss: 0.7798393964767456\n",
      "Epoch: 14 | loss: 1.1159133911132812 | test loss: 0.7785629034042358\n",
      "Epoch: 14 | loss: 0.8567611575126648 | test loss: 0.7774702310562134\n",
      "Epoch: 14 | loss: 0.7662470936775208 | test loss: 0.7769221663475037\n",
      "Epoch: 14 | loss: 0.7452957630157471 | test loss: 0.7760581374168396\n",
      "Epoch: 14 | loss: 0.7937456965446472 | test loss: 0.7750483751296997\n",
      "Epoch: 14 | loss: 0.8372814059257507 | test loss: 0.7745632529258728\n",
      "Epoch: 14 | loss: 0.7748202681541443 | test loss: 0.7737756371498108\n",
      "Epoch: 14 | loss: 0.6047275066375732 | test loss: 0.7733003497123718\n",
      "Epoch: 14 | loss: 0.8731759786605835 | test loss: 0.7724645137786865\n",
      "Epoch: 14 | loss: 0.871674120426178 | test loss: 0.7716361284255981\n",
      "Epoch: 14 | loss: 0.6287755370140076 | test loss: 0.7711212038993835\n",
      "Epoch: 14 | loss: 0.68965083360672 | test loss: 0.7701886296272278\n",
      "Epoch: 15 | loss: 0.7763570547103882 | test loss: 0.8473109006881714\n",
      "Epoch: 15 | loss: 0.802145779132843 | test loss: 0.8462983965873718\n",
      "Epoch: 15 | loss: 0.9291818141937256 | test loss: 0.8449331521987915\n",
      "Epoch: 15 | loss: 0.6313602924346924 | test loss: 0.8444283604621887\n",
      "Epoch: 15 | loss: 0.6688805818557739 | test loss: 0.8441047668457031\n",
      "Epoch: 15 | loss: 0.6809927225112915 | test loss: 0.8435943722724915\n",
      "Epoch: 15 | loss: 0.8316251635551453 | test loss: 0.8426774144172668\n",
      "Epoch: 15 | loss: 0.91341233253479 | test loss: 0.8417222499847412\n",
      "Epoch: 15 | loss: 0.7729601263999939 | test loss: 0.8404616713523865\n",
      "Epoch: 15 | loss: 0.7314055562019348 | test loss: 0.8395219445228577\n",
      "Epoch: 15 | loss: 0.9996948838233948 | test loss: 0.8375314474105835\n",
      "Epoch: 15 | loss: 0.8477599024772644 | test loss: 0.8363677859306335\n",
      "Epoch: 15 | loss: 0.7548736929893494 | test loss: 0.835039496421814\n",
      "Epoch: 15 | loss: 0.819444477558136 | test loss: 0.8335962891578674\n",
      "Epoch: 15 | loss: 0.7286631464958191 | test loss: 0.8325380086898804\n",
      "Epoch: 15 | loss: 0.7701388001441956 | test loss: 0.8329315781593323\n",
      "Epoch: 15 | loss: 0.8307105898857117 | test loss: 0.8312479853630066\n",
      "Epoch: 15 | loss: 0.8888145685195923 | test loss: 0.829801082611084\n",
      "Epoch: 15 | loss: 0.6948457956314087 | test loss: 0.8302148580551147\n",
      "Epoch: 15 | loss: 0.7558317184448242 | test loss: 0.8287932276725769\n",
      "Epoch: 15 | loss: 0.7667420506477356 | test loss: 0.8274126052856445\n",
      "Epoch: 15 | loss: 0.836084246635437 | test loss: 0.8267045021057129\n",
      "Epoch: 15 | loss: 0.9162999391555786 | test loss: 0.825441837310791\n",
      "Epoch: 15 | loss: 0.6772079467773438 | test loss: 0.8247631788253784\n",
      "Epoch: 15 | loss: 0.6382768154144287 | test loss: 0.8239060044288635\n",
      "Epoch: 15 | loss: 0.8862794041633606 | test loss: 0.82243412733078\n",
      "Epoch: 15 | loss: 0.5400296449661255 | test loss: 0.8224591016769409\n",
      "Epoch: 15 | loss: 0.746555745601654 | test loss: 0.8213972449302673\n",
      "Epoch: 15 | loss: 0.9162777662277222 | test loss: 0.8203510046005249\n",
      "Epoch: 15 | loss: 0.6636125445365906 | test loss: 0.819970965385437\n",
      "Epoch: 15 | loss: 0.8326213955879211 | test loss: 0.8186360001564026\n",
      "Epoch: 15 | loss: 0.8594342470169067 | test loss: 0.8185306191444397\n",
      "Epoch: 15 | loss: 0.6319023966789246 | test loss: 0.8178641200065613\n",
      "Epoch: 16 | loss: 0.6688193082809448 | test loss: 0.7730304002761841\n",
      "Epoch: 16 | loss: 0.8335109353065491 | test loss: 0.7725651860237122\n",
      "Epoch: 16 | loss: 0.8795922994613647 | test loss: 0.7700044512748718\n",
      "Epoch: 16 | loss: 0.8969686627388 | test loss: 0.7691553831100464\n",
      "Epoch: 16 | loss: 0.6357015371322632 | test loss: 0.7672109007835388\n",
      "Epoch: 16 | loss: 0.7079833745956421 | test loss: 0.7658194899559021\n",
      "Epoch: 16 | loss: 0.8221559524536133 | test loss: 0.7638152837753296\n",
      "Epoch: 16 | loss: 0.7477037906646729 | test loss: 0.7645307183265686\n",
      "Epoch: 16 | loss: 0.7131773233413696 | test loss: 0.7642602920532227\n",
      "Epoch: 16 | loss: 0.7571492791175842 | test loss: 0.7630665898323059\n",
      "Epoch: 16 | loss: 1.0972105264663696 | test loss: 0.7611770033836365\n",
      "Epoch: 16 | loss: 0.7765132784843445 | test loss: 0.760528028011322\n",
      "Epoch: 16 | loss: 0.8037295937538147 | test loss: 0.759914755821228\n",
      "Epoch: 16 | loss: 1.1394822597503662 | test loss: 0.7582936882972717\n",
      "Epoch: 16 | loss: 0.7574213743209839 | test loss: 0.7567576169967651\n",
      "Epoch: 16 | loss: 0.8884595036506653 | test loss: 0.755142867565155\n",
      "Epoch: 16 | loss: 0.759898841381073 | test loss: 0.7531156539916992\n",
      "Epoch: 16 | loss: 0.7201747894287109 | test loss: 0.7518778443336487\n",
      "Epoch: 16 | loss: 0.7560031414031982 | test loss: 0.7520802617073059\n",
      "Epoch: 16 | loss: 0.6433275938034058 | test loss: 0.7517263889312744\n",
      "Epoch: 16 | loss: 0.7286374568939209 | test loss: 0.7526013851165771\n",
      "Epoch: 16 | loss: 0.48897412419319153 | test loss: 0.75215083360672\n",
      "Epoch: 16 | loss: 0.7037314176559448 | test loss: 0.7501024603843689\n",
      "Epoch: 16 | loss: 0.714094877243042 | test loss: 0.7496458888053894\n",
      "Epoch: 16 | loss: 0.8105171322822571 | test loss: 0.7487863302230835\n",
      "Epoch: 16 | loss: 0.6349042654037476 | test loss: 0.7474784851074219\n",
      "Epoch: 16 | loss: 0.7448229193687439 | test loss: 0.7470247149467468\n",
      "Epoch: 16 | loss: 0.7328117489814758 | test loss: 0.7460084557533264\n",
      "Epoch: 16 | loss: 0.9064310789108276 | test loss: 0.7448618412017822\n",
      "Epoch: 16 | loss: 0.4992387592792511 | test loss: 0.744451642036438\n",
      "Epoch: 16 | loss: 0.6598575711250305 | test loss: 0.7439768314361572\n",
      "Epoch: 16 | loss: 0.745851457118988 | test loss: 0.7433003783226013\n",
      "Epoch: 16 | loss: 0.557267963886261 | test loss: 0.7421698570251465\n",
      "Epoch: 17 | loss: 0.8072549104690552 | test loss: 0.801715075969696\n",
      "Epoch: 17 | loss: 0.7173158526420593 | test loss: 0.8000528216362\n",
      "Epoch: 17 | loss: 0.6796178221702576 | test loss: 0.7980338335037231\n",
      "Epoch: 17 | loss: 0.8498179316520691 | test loss: 0.7970996499061584\n",
      "Epoch: 17 | loss: 0.673586905002594 | test loss: 0.7952031493186951\n",
      "Epoch: 17 | loss: 0.6282919645309448 | test loss: 0.7942346930503845\n",
      "Epoch: 17 | loss: 0.7348912358283997 | test loss: 0.7940533757209778\n",
      "Epoch: 17 | loss: 0.608394205570221 | test loss: 0.7941491007804871\n",
      "Epoch: 17 | loss: 0.6996873617172241 | test loss: 0.7927834391593933\n",
      "Epoch: 17 | loss: 0.7875530123710632 | test loss: 0.7909473776817322\n",
      "Epoch: 17 | loss: 0.6916196346282959 | test loss: 0.7891724705696106\n",
      "Epoch: 17 | loss: 0.535921037197113 | test loss: 0.7886354327201843\n",
      "Epoch: 17 | loss: 0.8867256045341492 | test loss: 0.7891561985015869\n",
      "Epoch: 17 | loss: 0.7449468970298767 | test loss: 0.788496732711792\n",
      "Epoch: 17 | loss: 0.5991448163986206 | test loss: 0.7888863682746887\n",
      "Epoch: 17 | loss: 0.8496695756912231 | test loss: 0.7876992225646973\n",
      "Epoch: 17 | loss: 0.5795018672943115 | test loss: 0.7871111631393433\n",
      "Epoch: 17 | loss: 0.7902332544326782 | test loss: 0.7861911058425903\n",
      "Epoch: 17 | loss: 0.6963475346565247 | test loss: 0.7859609723091125\n",
      "Epoch: 17 | loss: 0.6330278515815735 | test loss: 0.7858899235725403\n",
      "Epoch: 17 | loss: 0.9781806468963623 | test loss: 0.7838467359542847\n",
      "Epoch: 17 | loss: 0.7379592657089233 | test loss: 0.7826442122459412\n",
      "Epoch: 17 | loss: 0.6705126166343689 | test loss: 0.78228360414505\n",
      "Epoch: 17 | loss: 0.7410395741462708 | test loss: 0.7816042304039001\n",
      "Epoch: 17 | loss: 0.7646433711051941 | test loss: 0.7813940048217773\n",
      "Epoch: 17 | loss: 0.41952139139175415 | test loss: 0.7809233665466309\n",
      "Epoch: 17 | loss: 0.8080493807792664 | test loss: 0.7801429629325867\n",
      "Epoch: 17 | loss: 0.624187171459198 | test loss: 0.7791414856910706\n",
      "Epoch: 17 | loss: 0.6766408085823059 | test loss: 0.7788880467414856\n",
      "Epoch: 17 | loss: 0.6403819918632507 | test loss: 0.7781375646591187\n",
      "Epoch: 17 | loss: 0.8575182557106018 | test loss: 0.7771463990211487\n",
      "Epoch: 17 | loss: 0.9138500690460205 | test loss: 0.775831937789917\n",
      "Epoch: 17 | loss: 0.8467236757278442 | test loss: 0.7756993770599365\n",
      "Epoch: 18 | loss: 0.7562013268470764 | test loss: 0.7311511635780334\n",
      "Epoch: 18 | loss: 0.5951948761940002 | test loss: 0.730975866317749\n",
      "Epoch: 18 | loss: 0.9384793043136597 | test loss: 0.7295805215835571\n",
      "Epoch: 18 | loss: 0.7272360920906067 | test loss: 0.7295789122581482\n",
      "Epoch: 18 | loss: 0.6655375957489014 | test loss: 0.7284196615219116\n",
      "Epoch: 18 | loss: 0.8845843076705933 | test loss: 0.7272380590438843\n",
      "Epoch: 18 | loss: 0.581770122051239 | test loss: 0.7277560830116272\n",
      "Epoch: 18 | loss: 0.7715990543365479 | test loss: 0.7272351384162903\n",
      "Epoch: 18 | loss: 0.661081850528717 | test loss: 0.726340651512146\n",
      "Epoch: 18 | loss: 0.8995170593261719 | test loss: 0.7246864438056946\n",
      "Epoch: 18 | loss: 0.7083892822265625 | test loss: 0.7241436243057251\n",
      "Epoch: 18 | loss: 0.7336328029632568 | test loss: 0.7227919101715088\n",
      "Epoch: 18 | loss: 0.6421051621437073 | test loss: 0.7219473123550415\n",
      "Epoch: 18 | loss: 0.7451307773590088 | test loss: 0.720834493637085\n",
      "Epoch: 18 | loss: 0.5687872767448425 | test loss: 0.7196769118309021\n",
      "Epoch: 18 | loss: 0.6048542261123657 | test loss: 0.7187505960464478\n",
      "Epoch: 18 | loss: 0.6556841731071472 | test loss: 0.7178153395652771\n",
      "Epoch: 18 | loss: 0.7648083567619324 | test loss: 0.7175082564353943\n",
      "Epoch: 18 | loss: 0.7256544828414917 | test loss: 0.7161470055580139\n",
      "Epoch: 18 | loss: 0.9187480807304382 | test loss: 0.7145980596542358\n",
      "Epoch: 18 | loss: 0.4814091622829437 | test loss: 0.7138141393661499\n",
      "Epoch: 18 | loss: 0.555334210395813 | test loss: 0.7136994004249573\n",
      "Epoch: 18 | loss: 0.6934148669242859 | test loss: 0.7130128741264343\n",
      "Epoch: 18 | loss: 0.5388225317001343 | test loss: 0.712723433971405\n",
      "Epoch: 18 | loss: 0.7734755277633667 | test loss: 0.7123503088951111\n",
      "Epoch: 18 | loss: 0.4805961549282074 | test loss: 0.7124419212341309\n",
      "Epoch: 18 | loss: 0.5515350103378296 | test loss: 0.7124300599098206\n",
      "Epoch: 18 | loss: 0.6603413820266724 | test loss: 0.7122764587402344\n",
      "Epoch: 18 | loss: 1.0359916687011719 | test loss: 0.710780680179596\n",
      "Epoch: 18 | loss: 0.8866984248161316 | test loss: 0.7105674743652344\n",
      "Epoch: 18 | loss: 0.634799063205719 | test loss: 0.7098679542541504\n",
      "Epoch: 18 | loss: 0.8488103151321411 | test loss: 0.7085465788841248\n",
      "Epoch: 18 | loss: 0.5638952255249023 | test loss: 0.7078921794891357\n",
      "Epoch: 19 | loss: 0.731881856918335 | test loss: 0.6560269594192505\n",
      "Epoch: 19 | loss: 0.5547264218330383 | test loss: 0.6548767685890198\n",
      "Epoch: 19 | loss: 0.7551872730255127 | test loss: 0.6554206609725952\n",
      "Epoch: 19 | loss: 0.6177326440811157 | test loss: 0.6544277667999268\n",
      "Epoch: 19 | loss: 0.8719145655632019 | test loss: 0.6531345844268799\n",
      "Epoch: 19 | loss: 0.6802467703819275 | test loss: 0.65192711353302\n",
      "Epoch: 19 | loss: 0.5266396999359131 | test loss: 0.6520307064056396\n",
      "Epoch: 19 | loss: 0.6341190338134766 | test loss: 0.6514272093772888\n",
      "Epoch: 19 | loss: 0.6322879791259766 | test loss: 0.6511306762695312\n",
      "Epoch: 19 | loss: 0.6460813879966736 | test loss: 0.649919867515564\n",
      "Epoch: 19 | loss: 0.5674558877944946 | test loss: 0.6487847566604614\n",
      "Epoch: 19 | loss: 0.7806270122528076 | test loss: 0.64759361743927\n",
      "Epoch: 19 | loss: 0.6128687262535095 | test loss: 0.6461874842643738\n",
      "Epoch: 19 | loss: 0.8490148186683655 | test loss: 0.6456661820411682\n",
      "Epoch: 19 | loss: 0.8323028087615967 | test loss: 0.6464678645133972\n",
      "Epoch: 19 | loss: 0.6866567730903625 | test loss: 0.6458413600921631\n",
      "Epoch: 19 | loss: 0.9092736840248108 | test loss: 0.6458138227462769\n",
      "Epoch: 19 | loss: 0.6167708039283752 | test loss: 0.6449843645095825\n",
      "Epoch: 19 | loss: 0.4894721806049347 | test loss: 0.6441265344619751\n",
      "Epoch: 19 | loss: 0.7618186473846436 | test loss: 0.642195463180542\n",
      "Epoch: 19 | loss: 0.8057295680046082 | test loss: 0.6403672695159912\n",
      "Epoch: 19 | loss: 0.557765543460846 | test loss: 0.6399971842765808\n",
      "Epoch: 19 | loss: 0.6030587553977966 | test loss: 0.6391271948814392\n",
      "Epoch: 19 | loss: 0.5870673060417175 | test loss: 0.6381140351295471\n",
      "Epoch: 19 | loss: 0.6067607998847961 | test loss: 0.6369330286979675\n",
      "Epoch: 19 | loss: 0.615020215511322 | test loss: 0.6359479427337646\n",
      "Epoch: 19 | loss: 0.600174069404602 | test loss: 0.6342382431030273\n",
      "Epoch: 19 | loss: 0.8411055207252502 | test loss: 0.6333293914794922\n",
      "Epoch: 19 | loss: 0.8596768975257874 | test loss: 0.6331983208656311\n",
      "Epoch: 19 | loss: 0.5609291791915894 | test loss: 0.6323753595352173\n",
      "Epoch: 19 | loss: 0.6951344609260559 | test loss: 0.6326866745948792\n",
      "Epoch: 19 | loss: 0.7772689461708069 | test loss: 0.6316832900047302\n",
      "Epoch: 19 | loss: 0.6881148815155029 | test loss: 0.6308220624923706\n",
      "Epoch: 20 | loss: 0.6408352255821228 | test loss: 0.7160598039627075\n",
      "Epoch: 20 | loss: 0.7407336831092834 | test loss: 0.7149381637573242\n",
      "Epoch: 20 | loss: 0.9007235765457153 | test loss: 0.7137002944946289\n",
      "Epoch: 20 | loss: 0.6095802187919617 | test loss: 0.7129597067832947\n",
      "Epoch: 20 | loss: 0.42895156145095825 | test loss: 0.7126398682594299\n",
      "Epoch: 20 | loss: 0.433856338262558 | test loss: 0.7121192216873169\n",
      "Epoch: 20 | loss: 0.4194975793361664 | test loss: 0.7115876078605652\n",
      "Epoch: 20 | loss: 0.6635701060295105 | test loss: 0.7111629843711853\n",
      "Epoch: 20 | loss: 0.6521579027175903 | test loss: 0.7109877467155457\n",
      "Epoch: 20 | loss: 0.5704978108406067 | test loss: 0.7103632688522339\n",
      "Epoch: 20 | loss: 0.834610641002655 | test loss: 0.7096381783485413\n",
      "Epoch: 20 | loss: 0.629115879535675 | test loss: 0.7091823220252991\n",
      "Epoch: 20 | loss: 0.542279839515686 | test loss: 0.7088098526000977\n",
      "Epoch: 20 | loss: 0.8211507201194763 | test loss: 0.708219051361084\n",
      "Epoch: 20 | loss: 0.6138378977775574 | test loss: 0.7075985670089722\n",
      "Epoch: 20 | loss: 0.6487475037574768 | test loss: 0.7064333558082581\n",
      "Epoch: 20 | loss: 0.7689747214317322 | test loss: 0.7061405181884766\n",
      "Epoch: 20 | loss: 0.7828578948974609 | test loss: 0.7055164575576782\n",
      "Epoch: 20 | loss: 0.6353304386138916 | test loss: 0.7049179673194885\n",
      "Epoch: 20 | loss: 0.6062245965003967 | test loss: 0.7045406699180603\n",
      "Epoch: 20 | loss: 0.6448214650154114 | test loss: 0.7041270732879639\n",
      "Epoch: 20 | loss: 0.5833959579467773 | test loss: 0.7033472061157227\n",
      "Epoch: 20 | loss: 0.607833981513977 | test loss: 0.7025217413902283\n",
      "Epoch: 20 | loss: 0.7400162816047668 | test loss: 0.70283043384552\n",
      "Epoch: 20 | loss: 0.8404775857925415 | test loss: 0.7017279863357544\n",
      "Epoch: 20 | loss: 0.7184732556343079 | test loss: 0.7004410028457642\n",
      "Epoch: 20 | loss: 0.6983863711357117 | test loss: 0.7003540396690369\n",
      "Epoch: 20 | loss: 0.8022366166114807 | test loss: 0.6993215084075928\n",
      "Epoch: 20 | loss: 0.4948960840702057 | test loss: 0.6988915205001831\n",
      "Epoch: 20 | loss: 0.7040838599205017 | test loss: 0.6976954340934753\n",
      "Epoch: 20 | loss: 0.728631854057312 | test loss: 0.6967631578445435\n",
      "Epoch: 20 | loss: 0.6720128655433655 | test loss: 0.6957299113273621\n",
      "Epoch: 20 | loss: 0.7332618832588196 | test loss: 0.694805920124054\n",
      "Epoch: 21 | loss: 0.6269413232803345 | test loss: 0.6761616468429565\n",
      "Epoch: 21 | loss: 0.7202359437942505 | test loss: 0.6749510765075684\n",
      "Epoch: 21 | loss: 0.6537506580352783 | test loss: 0.6747362613677979\n",
      "Epoch: 21 | loss: 0.6707594394683838 | test loss: 0.6737420558929443\n",
      "Epoch: 21 | loss: 0.9008773565292358 | test loss: 0.672277569770813\n",
      "Epoch: 21 | loss: 0.621458888053894 | test loss: 0.6713407635688782\n",
      "Epoch: 21 | loss: 0.48476171493530273 | test loss: 0.6704056859016418\n",
      "Epoch: 21 | loss: 0.624442458152771 | test loss: 0.6708107590675354\n",
      "Epoch: 21 | loss: 0.652075469493866 | test loss: 0.6704111099243164\n",
      "Epoch: 21 | loss: 0.5173932909965515 | test loss: 0.6692249774932861\n",
      "Epoch: 21 | loss: 0.5926666855812073 | test loss: 0.6692180037498474\n",
      "Epoch: 21 | loss: 0.6820794939994812 | test loss: 0.6673594117164612\n",
      "Epoch: 21 | loss: 0.6484843492507935 | test loss: 0.667750895023346\n",
      "Epoch: 21 | loss: 0.5806547999382019 | test loss: 0.6676287055015564\n",
      "Epoch: 21 | loss: 0.6647645235061646 | test loss: 0.667263925075531\n",
      "Epoch: 21 | loss: 0.5859679579734802 | test loss: 0.6665830612182617\n",
      "Epoch: 21 | loss: 0.7652378678321838 | test loss: 0.6663206815719604\n",
      "Epoch: 21 | loss: 0.6879425048828125 | test loss: 0.6662936210632324\n",
      "Epoch: 21 | loss: 0.6631079316139221 | test loss: 0.6653241515159607\n",
      "Epoch: 21 | loss: 0.7742498517036438 | test loss: 0.6639776825904846\n",
      "Epoch: 21 | loss: 0.595436155796051 | test loss: 0.6619876027107239\n",
      "Epoch: 21 | loss: 0.5374076962471008 | test loss: 0.6618577837944031\n",
      "Epoch: 21 | loss: 0.50429767370224 | test loss: 0.6610704660415649\n",
      "Epoch: 21 | loss: 0.7425921559333801 | test loss: 0.6599907875061035\n",
      "Epoch: 21 | loss: 0.5403515100479126 | test loss: 0.6601914167404175\n",
      "Epoch: 21 | loss: 0.7617948651313782 | test loss: 0.6587923765182495\n",
      "Epoch: 21 | loss: 0.7956864237785339 | test loss: 0.6566115617752075\n",
      "Epoch: 21 | loss: 0.640839695930481 | test loss: 0.65590500831604\n",
      "Epoch: 21 | loss: 0.6862193942070007 | test loss: 0.6544793844223022\n",
      "Epoch: 21 | loss: 0.682499349117279 | test loss: 0.6548299789428711\n",
      "Epoch: 21 | loss: 0.5490572452545166 | test loss: 0.6536852717399597\n",
      "Epoch: 21 | loss: 0.6061660647392273 | test loss: 0.6528990268707275\n",
      "Epoch: 21 | loss: 0.5433058142662048 | test loss: 0.6522635817527771\n",
      "Epoch: 22 | loss: 0.647546112537384 | test loss: 0.6764905452728271\n",
      "Epoch: 22 | loss: 0.6017360091209412 | test loss: 0.6761876344680786\n",
      "Epoch: 22 | loss: 0.5326593518257141 | test loss: 0.6743451952934265\n",
      "Epoch: 22 | loss: 0.6225001811981201 | test loss: 0.6722464561462402\n",
      "Epoch: 22 | loss: 0.8925608992576599 | test loss: 0.6705960035324097\n",
      "Epoch: 22 | loss: 0.563187837600708 | test loss: 0.6689262390136719\n",
      "Epoch: 22 | loss: 0.5330705642700195 | test loss: 0.66776442527771\n",
      "Epoch: 22 | loss: 0.7195783853530884 | test loss: 0.6667866110801697\n",
      "Epoch: 22 | loss: 0.4904864728450775 | test loss: 0.6662992835044861\n",
      "Epoch: 22 | loss: 0.6550096869468689 | test loss: 0.6651049852371216\n",
      "Epoch: 22 | loss: 0.6337989568710327 | test loss: 0.6643096208572388\n",
      "Epoch: 22 | loss: 0.4816651940345764 | test loss: 0.6638115048408508\n",
      "Epoch: 22 | loss: 0.7174042463302612 | test loss: 0.6627445816993713\n",
      "Epoch: 22 | loss: 0.6831668019294739 | test loss: 0.6620966792106628\n",
      "Epoch: 22 | loss: 0.7613513469696045 | test loss: 0.6623607873916626\n",
      "Epoch: 22 | loss: 0.7113214731216431 | test loss: 0.6605873107910156\n",
      "Epoch: 22 | loss: 0.6758893132209778 | test loss: 0.660298764705658\n",
      "Epoch: 22 | loss: 0.5929638743400574 | test loss: 0.6603578925132751\n",
      "Epoch: 22 | loss: 0.6522925496101379 | test loss: 0.6606385707855225\n",
      "Epoch: 22 | loss: 0.5566489696502686 | test loss: 0.6608808040618896\n",
      "Epoch: 22 | loss: 0.6020733714103699 | test loss: 0.6605686545372009\n",
      "Epoch: 22 | loss: 0.5673537850379944 | test loss: 0.6593939065933228\n",
      "Epoch: 22 | loss: 0.7125642895698547 | test loss: 0.658997654914856\n",
      "Epoch: 22 | loss: 0.5677534937858582 | test loss: 0.6572269201278687\n",
      "Epoch: 22 | loss: 0.6101964712142944 | test loss: 0.6564124822616577\n",
      "Epoch: 22 | loss: 0.6232364773750305 | test loss: 0.6558326482772827\n",
      "Epoch: 22 | loss: 0.573573648929596 | test loss: 0.6546452045440674\n",
      "Epoch: 22 | loss: 0.6904981136322021 | test loss: 0.6566025018692017\n",
      "Epoch: 22 | loss: 0.5345459580421448 | test loss: 0.6568468809127808\n",
      "Epoch: 22 | loss: 0.7838508486747742 | test loss: 0.6564815640449524\n",
      "Epoch: 22 | loss: 0.6552416682243347 | test loss: 0.6553386449813843\n",
      "Epoch: 22 | loss: 0.6165978908538818 | test loss: 0.6560508608818054\n",
      "Epoch: 22 | loss: 0.48220616579055786 | test loss: 0.6560760736465454\n",
      "Epoch: 23 | loss: 0.6583856344223022 | test loss: 0.6272794008255005\n",
      "Epoch: 23 | loss: 0.6895981431007385 | test loss: 0.6266635656356812\n",
      "Epoch: 23 | loss: 0.525809645652771 | test loss: 0.6265792846679688\n",
      "Epoch: 23 | loss: 0.6032362580299377 | test loss: 0.6261342763900757\n",
      "Epoch: 23 | loss: 0.7346130013465881 | test loss: 0.6255179643630981\n",
      "Epoch: 23 | loss: 0.5618035793304443 | test loss: 0.6245272159576416\n",
      "Epoch: 23 | loss: 0.6507269740104675 | test loss: 0.6241037249565125\n",
      "Epoch: 23 | loss: 0.6082307696342468 | test loss: 0.6223874688148499\n",
      "Epoch: 23 | loss: 0.5263105034828186 | test loss: 0.6213670372962952\n",
      "Epoch: 23 | loss: 0.5678605437278748 | test loss: 0.6210287809371948\n",
      "Epoch: 23 | loss: 0.6771087050437927 | test loss: 0.6198573112487793\n",
      "Epoch: 23 | loss: 0.5653355717658997 | test loss: 0.6185712814331055\n",
      "Epoch: 23 | loss: 0.5304374694824219 | test loss: 0.6181067228317261\n",
      "Epoch: 23 | loss: 0.5613613128662109 | test loss: 0.6183474063873291\n",
      "Epoch: 23 | loss: 0.6814467310905457 | test loss: 0.6180945634841919\n",
      "Epoch: 23 | loss: 0.7233795523643494 | test loss: 0.6174858808517456\n",
      "Epoch: 23 | loss: 0.6942460536956787 | test loss: 0.6162927746772766\n",
      "Epoch: 23 | loss: 0.590935468673706 | test loss: 0.6156599521636963\n",
      "Epoch: 23 | loss: 0.654400110244751 | test loss: 0.6153447031974792\n",
      "Epoch: 23 | loss: 0.6332083344459534 | test loss: 0.6145567297935486\n",
      "Epoch: 23 | loss: 0.6137258410453796 | test loss: 0.6140602231025696\n",
      "Epoch: 23 | loss: 0.6094937920570374 | test loss: 0.6134731769561768\n",
      "Epoch: 23 | loss: 0.44411516189575195 | test loss: 0.6134424805641174\n",
      "Epoch: 23 | loss: 0.49590998888015747 | test loss: 0.6131633520126343\n",
      "Epoch: 23 | loss: 0.6002627015113831 | test loss: 0.6132301092147827\n",
      "Epoch: 23 | loss: 0.38895949721336365 | test loss: 0.6128004789352417\n",
      "Epoch: 23 | loss: 0.7547038793563843 | test loss: 0.6119521856307983\n",
      "Epoch: 23 | loss: 0.6619172692298889 | test loss: 0.6109974980354309\n",
      "Epoch: 23 | loss: 0.47646018862724304 | test loss: 0.6102807521820068\n",
      "Epoch: 23 | loss: 0.7807520627975464 | test loss: 0.6101523637771606\n",
      "Epoch: 23 | loss: 0.5200449824333191 | test loss: 0.6095556616783142\n",
      "Epoch: 23 | loss: 0.590597927570343 | test loss: 0.6083345413208008\n",
      "Epoch: 23 | loss: 0.6189050674438477 | test loss: 0.6070787310600281\n",
      "Epoch: 24 | loss: 0.6877777576446533 | test loss: 0.5545495748519897\n",
      "Epoch: 24 | loss: 0.7219682335853577 | test loss: 0.5541642308235168\n",
      "Epoch: 24 | loss: 0.6373323202133179 | test loss: 0.5534561276435852\n",
      "Epoch: 24 | loss: 0.5401556491851807 | test loss: 0.5526934862136841\n",
      "Epoch: 24 | loss: 0.5715216398239136 | test loss: 0.5522040724754333\n",
      "Epoch: 24 | loss: 0.46787121891975403 | test loss: 0.5518279671669006\n",
      "Epoch: 24 | loss: 0.535172700881958 | test loss: 0.5514209270477295\n",
      "Epoch: 24 | loss: 0.4774796664714813 | test loss: 0.5508027672767639\n",
      "Epoch: 24 | loss: 0.74979168176651 | test loss: 0.5499340295791626\n",
      "Epoch: 24 | loss: 0.6562452912330627 | test loss: 0.5490794777870178\n",
      "Epoch: 24 | loss: 0.759081244468689 | test loss: 0.54836505651474\n",
      "Epoch: 24 | loss: 0.4484891891479492 | test loss: 0.5478033423423767\n",
      "Epoch: 24 | loss: 0.5253394842147827 | test loss: 0.5470377802848816\n",
      "Epoch: 24 | loss: 0.5080877542495728 | test loss: 0.546627402305603\n",
      "Epoch: 24 | loss: 0.499244749546051 | test loss: 0.5459849238395691\n",
      "Epoch: 24 | loss: 0.603458821773529 | test loss: 0.5454626679420471\n",
      "Epoch: 24 | loss: 0.7953189611434937 | test loss: 0.544592022895813\n",
      "Epoch: 24 | loss: 0.6093785762786865 | test loss: 0.5438124537467957\n",
      "Epoch: 24 | loss: 0.43757447600364685 | test loss: 0.5431982278823853\n",
      "Epoch: 24 | loss: 0.5919373631477356 | test loss: 0.5426502823829651\n",
      "Epoch: 24 | loss: 0.6495203375816345 | test loss: 0.5424096584320068\n",
      "Epoch: 24 | loss: 0.6106986403465271 | test loss: 0.5420858263969421\n",
      "Epoch: 24 | loss: 0.4991139769554138 | test loss: 0.5410569310188293\n",
      "Epoch: 24 | loss: 0.6228895783424377 | test loss: 0.5404072403907776\n",
      "Epoch: 24 | loss: 0.654403567314148 | test loss: 0.539926290512085\n",
      "Epoch: 24 | loss: 0.6431996822357178 | test loss: 0.5390840768814087\n",
      "Epoch: 24 | loss: 0.4978700578212738 | test loss: 0.5386047959327698\n",
      "Epoch: 24 | loss: 0.7484298944473267 | test loss: 0.5380250215530396\n",
      "Epoch: 24 | loss: 0.5600529909133911 | test loss: 0.5368964076042175\n",
      "Epoch: 24 | loss: 0.413112998008728 | test loss: 0.5364008545875549\n",
      "Epoch: 24 | loss: 0.5694111585617065 | test loss: 0.535788357257843\n",
      "Epoch: 24 | loss: 0.7429690957069397 | test loss: 0.5353070497512817\n",
      "Epoch: 24 | loss: 0.4432131052017212 | test loss: 0.5345552563667297\n",
      "Epoch: 25 | loss: 0.6658397316932678 | test loss: 0.5904644131660461\n",
      "Epoch: 25 | loss: 0.5018302202224731 | test loss: 0.590002715587616\n",
      "Epoch: 25 | loss: 0.556287407875061 | test loss: 0.5894808173179626\n",
      "Epoch: 25 | loss: 0.4859478771686554 | test loss: 0.5890675783157349\n",
      "Epoch: 25 | loss: 0.6347854733467102 | test loss: 0.5881333351135254\n",
      "Epoch: 25 | loss: 0.7212451696395874 | test loss: 0.5873580574989319\n",
      "Epoch: 25 | loss: 0.4888690412044525 | test loss: 0.5870286822319031\n",
      "Epoch: 25 | loss: 0.5815597772598267 | test loss: 0.5869243741035461\n",
      "Epoch: 25 | loss: 0.6413416266441345 | test loss: 0.5860141515731812\n",
      "Epoch: 25 | loss: 0.6403353810310364 | test loss: 0.5854468941688538\n",
      "Epoch: 25 | loss: 0.6072138547897339 | test loss: 0.5853339433670044\n",
      "Epoch: 25 | loss: 0.48110705614089966 | test loss: 0.5852451920509338\n",
      "Epoch: 25 | loss: 0.5230900645256042 | test loss: 0.5845175981521606\n",
      "Epoch: 25 | loss: 0.6048939824104309 | test loss: 0.5841219425201416\n",
      "Epoch: 25 | loss: 0.5988910794258118 | test loss: 0.583400547504425\n",
      "Epoch: 25 | loss: 0.5323784947395325 | test loss: 0.5830418467521667\n",
      "Epoch: 25 | loss: 0.32831159234046936 | test loss: 0.582596480846405\n",
      "Epoch: 25 | loss: 0.6078107953071594 | test loss: 0.5822560787200928\n",
      "Epoch: 25 | loss: 0.8577135801315308 | test loss: 0.5813625454902649\n",
      "Epoch: 25 | loss: 0.4172171354293823 | test loss: 0.5805063843727112\n",
      "Epoch: 25 | loss: 0.6090124249458313 | test loss: 0.5799806118011475\n",
      "Epoch: 25 | loss: 0.620574414730072 | test loss: 0.5791481733322144\n",
      "Epoch: 25 | loss: 0.5068654417991638 | test loss: 0.5789728760719299\n",
      "Epoch: 25 | loss: 0.512018084526062 | test loss: 0.5785741209983826\n",
      "Epoch: 25 | loss: 0.5237661600112915 | test loss: 0.5773982405662537\n",
      "Epoch: 25 | loss: 0.4610876441001892 | test loss: 0.5770251750946045\n",
      "Epoch: 25 | loss: 0.8515357971191406 | test loss: 0.576507568359375\n",
      "Epoch: 25 | loss: 0.5675596594810486 | test loss: 0.5761147141456604\n",
      "Epoch: 25 | loss: 0.5705506205558777 | test loss: 0.5756904482841492\n",
      "Epoch: 25 | loss: 0.6733055710792542 | test loss: 0.5749516487121582\n",
      "Epoch: 25 | loss: 0.7590475678443909 | test loss: 0.5748307704925537\n",
      "Epoch: 25 | loss: 0.4313354194164276 | test loss: 0.5742766261100769\n",
      "Epoch: 25 | loss: 0.43705374002456665 | test loss: 0.5738446712493896\n",
      "Epoch: 26 | loss: 0.5628163814544678 | test loss: 0.5882928371429443\n",
      "Epoch: 26 | loss: 0.6802574396133423 | test loss: 0.5879408717155457\n",
      "Epoch: 26 | loss: 0.45045143365859985 | test loss: 0.5877404808998108\n",
      "Epoch: 26 | loss: 0.5263012647628784 | test loss: 0.5872930884361267\n",
      "Epoch: 26 | loss: 0.7013682723045349 | test loss: 0.5873451232910156\n",
      "Epoch: 26 | loss: 0.44706860184669495 | test loss: 0.5872427821159363\n",
      "Epoch: 26 | loss: 0.6591097712516785 | test loss: 0.5867553353309631\n",
      "Epoch: 26 | loss: 0.3518471121788025 | test loss: 0.5857006311416626\n",
      "Epoch: 26 | loss: 0.5387714505195618 | test loss: 0.5849387049674988\n",
      "Epoch: 26 | loss: 0.5818175077438354 | test loss: 0.5836136937141418\n",
      "Epoch: 26 | loss: 0.7403684258460999 | test loss: 0.5826382040977478\n",
      "Epoch: 26 | loss: 0.46347638964653015 | test loss: 0.5816164016723633\n",
      "Epoch: 26 | loss: 0.7318345904350281 | test loss: 0.5799806118011475\n",
      "Epoch: 26 | loss: 0.47225677967071533 | test loss: 0.5798443555831909\n",
      "Epoch: 26 | loss: 0.4752449095249176 | test loss: 0.5799619555473328\n",
      "Epoch: 26 | loss: 0.6997595429420471 | test loss: 0.579327404499054\n",
      "Epoch: 26 | loss: 0.5240609645843506 | test loss: 0.578978955745697\n",
      "Epoch: 26 | loss: 0.46915873885154724 | test loss: 0.5794838666915894\n",
      "Epoch: 26 | loss: 0.4552353322505951 | test loss: 0.5787772536277771\n",
      "Epoch: 26 | loss: 0.4912680387496948 | test loss: 0.5784189105033875\n",
      "Epoch: 26 | loss: 0.6054319143295288 | test loss: 0.5784167647361755\n",
      "Epoch: 26 | loss: 0.4938678741455078 | test loss: 0.5775265097618103\n",
      "Epoch: 26 | loss: 0.6759321093559265 | test loss: 0.5776563286781311\n",
      "Epoch: 26 | loss: 0.5977262258529663 | test loss: 0.5766265988349915\n",
      "Epoch: 26 | loss: 0.5919932126998901 | test loss: 0.5758885145187378\n",
      "Epoch: 26 | loss: 0.6665611863136292 | test loss: 0.575620174407959\n",
      "Epoch: 26 | loss: 0.5006901621818542 | test loss: 0.5752050876617432\n",
      "Epoch: 26 | loss: 0.4640921950340271 | test loss: 0.5748637914657593\n",
      "Epoch: 26 | loss: 0.7352898120880127 | test loss: 0.5747008323669434\n",
      "Epoch: 26 | loss: 0.39217764139175415 | test loss: 0.5746468305587769\n",
      "Epoch: 26 | loss: 0.6150416135787964 | test loss: 0.5734072923660278\n",
      "Epoch: 26 | loss: 0.5948313474655151 | test loss: 0.5727841258049011\n",
      "Epoch: 26 | loss: 0.602755606174469 | test loss: 0.573470413684845\n",
      "Epoch: 27 | loss: 0.46446260809898376 | test loss: 0.5424531102180481\n",
      "Epoch: 27 | loss: 0.6321638226509094 | test loss: 0.5416123867034912\n",
      "Epoch: 27 | loss: 0.5502737164497375 | test loss: 0.541613757610321\n",
      "Epoch: 27 | loss: 0.6819469928741455 | test loss: 0.5412062406539917\n",
      "Epoch: 27 | loss: 0.5004237294197083 | test loss: 0.5405824184417725\n",
      "Epoch: 27 | loss: 0.6284826993942261 | test loss: 0.5395436882972717\n",
      "Epoch: 27 | loss: 0.44059449434280396 | test loss: 0.5386966466903687\n",
      "Epoch: 27 | loss: 0.6250070929527283 | test loss: 0.5377559065818787\n",
      "Epoch: 27 | loss: 0.5871121883392334 | test loss: 0.5377194881439209\n",
      "Epoch: 27 | loss: 0.424637109041214 | test loss: 0.5372877717018127\n",
      "Epoch: 27 | loss: 0.6083763837814331 | test loss: 0.5373799800872803\n",
      "Epoch: 27 | loss: 0.5065786838531494 | test loss: 0.5365773439407349\n",
      "Epoch: 27 | loss: 0.5312478542327881 | test loss: 0.5361543893814087\n",
      "Epoch: 27 | loss: 0.6465069651603699 | test loss: 0.536581814289093\n",
      "Epoch: 27 | loss: 0.5109449028968811 | test loss: 0.5358666181564331\n",
      "Epoch: 27 | loss: 0.49462267756462097 | test loss: 0.5348119735717773\n",
      "Epoch: 27 | loss: 0.5559388995170593 | test loss: 0.534066915512085\n",
      "Epoch: 27 | loss: 0.38734397292137146 | test loss: 0.5336809754371643\n",
      "Epoch: 27 | loss: 0.5640510320663452 | test loss: 0.5331150889396667\n",
      "Epoch: 27 | loss: 0.5134190917015076 | test loss: 0.5325184464454651\n",
      "Epoch: 27 | loss: 0.6868304014205933 | test loss: 0.5320929884910583\n",
      "Epoch: 27 | loss: 0.4514114260673523 | test loss: 0.5314307808876038\n",
      "Epoch: 27 | loss: 0.4794846177101135 | test loss: 0.5303453803062439\n",
      "Epoch: 27 | loss: 0.4773911237716675 | test loss: 0.5297964215278625\n",
      "Epoch: 27 | loss: 0.6561678051948547 | test loss: 0.5289469957351685\n",
      "Epoch: 27 | loss: 0.44062185287475586 | test loss: 0.5279786586761475\n",
      "Epoch: 27 | loss: 0.6475681066513062 | test loss: 0.5276481509208679\n",
      "Epoch: 27 | loss: 0.44908201694488525 | test loss: 0.5272063612937927\n",
      "Epoch: 27 | loss: 0.6095470190048218 | test loss: 0.5267317295074463\n",
      "Epoch: 27 | loss: 0.6411566138267517 | test loss: 0.5261996984481812\n",
      "Epoch: 27 | loss: 0.5750483870506287 | test loss: 0.5257481932640076\n",
      "Epoch: 27 | loss: 0.5516319870948792 | test loss: 0.5253159403800964\n",
      "Epoch: 27 | loss: 0.37129536271095276 | test loss: 0.525227427482605\n",
      "Epoch: 28 | loss: 0.6362401247024536 | test loss: 0.5243382453918457\n",
      "Epoch: 28 | loss: 0.5109548568725586 | test loss: 0.5241624712944031\n",
      "Epoch: 28 | loss: 0.5801994800567627 | test loss: 0.523479700088501\n",
      "Epoch: 28 | loss: 0.5985990762710571 | test loss: 0.5226063132286072\n",
      "Epoch: 28 | loss: 0.6619346737861633 | test loss: 0.5219129323959351\n",
      "Epoch: 28 | loss: 0.46727022528648376 | test loss: 0.5211400985717773\n",
      "Epoch: 28 | loss: 0.5808725357055664 | test loss: 0.5206472277641296\n",
      "Epoch: 28 | loss: 0.402283251285553 | test loss: 0.5206230282783508\n",
      "Epoch: 28 | loss: 0.6307291388511658 | test loss: 0.520076334476471\n",
      "Epoch: 28 | loss: 0.5387832522392273 | test loss: 0.5200088620185852\n",
      "Epoch: 28 | loss: 0.5593028664588928 | test loss: 0.5191097855567932\n",
      "Epoch: 28 | loss: 0.5034244060516357 | test loss: 0.5185382962226868\n",
      "Epoch: 28 | loss: 0.448803573846817 | test loss: 0.5181640386581421\n",
      "Epoch: 28 | loss: 0.7865715622901917 | test loss: 0.518061101436615\n",
      "Epoch: 28 | loss: 0.39811891317367554 | test loss: 0.5178307294845581\n",
      "Epoch: 28 | loss: 0.5676186680793762 | test loss: 0.5170907974243164\n",
      "Epoch: 28 | loss: 0.47403019666671753 | test loss: 0.5167759656906128\n",
      "Epoch: 28 | loss: 0.523085355758667 | test loss: 0.5163278579711914\n",
      "Epoch: 28 | loss: 0.5901850461959839 | test loss: 0.5158370733261108\n",
      "Epoch: 28 | loss: 0.5127299427986145 | test loss: 0.5151687860488892\n",
      "Epoch: 28 | loss: 0.5583665370941162 | test loss: 0.5145172476768494\n",
      "Epoch: 28 | loss: 0.4879909157752991 | test loss: 0.5142451524734497\n",
      "Epoch: 28 | loss: 0.544083833694458 | test loss: 0.5135008692741394\n",
      "Epoch: 28 | loss: 0.41542038321495056 | test loss: 0.5133389234542847\n",
      "Epoch: 28 | loss: 0.5221066474914551 | test loss: 0.5127192139625549\n",
      "Epoch: 28 | loss: 0.49616381525993347 | test loss: 0.512350857257843\n",
      "Epoch: 28 | loss: 0.42965251207351685 | test loss: 0.5119648575782776\n",
      "Epoch: 28 | loss: 0.4730163514614105 | test loss: 0.5113216042518616\n",
      "Epoch: 28 | loss: 0.47682273387908936 | test loss: 0.5109371542930603\n",
      "Epoch: 28 | loss: 0.737137496471405 | test loss: 0.5107370615005493\n",
      "Epoch: 28 | loss: 0.4464271664619446 | test loss: 0.5101655125617981\n",
      "Epoch: 28 | loss: 0.47975730895996094 | test loss: 0.5096750259399414\n",
      "Epoch: 28 | loss: 0.4491030275821686 | test loss: 0.5090665221214294\n",
      "Epoch: 29 | loss: 0.5625807046890259 | test loss: 0.5266939997673035\n",
      "Epoch: 29 | loss: 0.5665486454963684 | test loss: 0.5262022018432617\n",
      "Epoch: 29 | loss: 0.5313675403594971 | test loss: 0.5259279012680054\n",
      "Epoch: 29 | loss: 0.45160531997680664 | test loss: 0.525413453578949\n",
      "Epoch: 29 | loss: 0.660168468952179 | test loss: 0.5250153541564941\n",
      "Epoch: 29 | loss: 0.5455635190010071 | test loss: 0.5240594744682312\n",
      "Epoch: 29 | loss: 0.5183849930763245 | test loss: 0.5238510370254517\n",
      "Epoch: 29 | loss: 0.5770300030708313 | test loss: 0.5233777165412903\n",
      "Epoch: 29 | loss: 0.4584864377975464 | test loss: 0.5230091214179993\n",
      "Epoch: 29 | loss: 0.48036104440689087 | test loss: 0.5224570631980896\n",
      "Epoch: 29 | loss: 0.45518893003463745 | test loss: 0.5220064520835876\n",
      "Epoch: 29 | loss: 0.45400843024253845 | test loss: 0.5213208794593811\n",
      "Epoch: 29 | loss: 0.4640868604183197 | test loss: 0.5209171175956726\n",
      "Epoch: 29 | loss: 0.3536226451396942 | test loss: 0.5206275582313538\n",
      "Epoch: 29 | loss: 0.4637717008590698 | test loss: 0.5200191736221313\n",
      "Epoch: 29 | loss: 0.40501096844673157 | test loss: 0.5196273922920227\n",
      "Epoch: 29 | loss: 0.6052249670028687 | test loss: 0.5191736221313477\n",
      "Epoch: 29 | loss: 0.35815683007240295 | test loss: 0.5187407732009888\n",
      "Epoch: 29 | loss: 0.5817043781280518 | test loss: 0.5181918740272522\n",
      "Epoch: 29 | loss: 0.7787706255912781 | test loss: 0.5180196166038513\n",
      "Epoch: 29 | loss: 0.42592209577560425 | test loss: 0.5176522731781006\n",
      "Epoch: 29 | loss: 0.6724709272384644 | test loss: 0.5171088576316833\n",
      "Epoch: 29 | loss: 0.49958160519599915 | test loss: 0.5169888734817505\n",
      "Epoch: 29 | loss: 0.4442192316055298 | test loss: 0.5163442492485046\n",
      "Epoch: 29 | loss: 0.5961722731590271 | test loss: 0.5158612728118896\n",
      "Epoch: 29 | loss: 0.41037696599960327 | test loss: 0.5153122544288635\n",
      "Epoch: 29 | loss: 0.5335100293159485 | test loss: 0.5150299668312073\n",
      "Epoch: 29 | loss: 0.5690244436264038 | test loss: 0.5147618651390076\n",
      "Epoch: 29 | loss: 0.6328343749046326 | test loss: 0.5143998265266418\n",
      "Epoch: 29 | loss: 0.4486442804336548 | test loss: 0.5139712691307068\n",
      "Epoch: 29 | loss: 0.5882500410079956 | test loss: 0.5138005018234253\n",
      "Epoch: 29 | loss: 0.5058782696723938 | test loss: 0.5132889747619629\n",
      "Epoch: 29 | loss: 0.44905152916908264 | test loss: 0.5127511620521545\n",
      "Epoch: 30 | loss: 0.5572383403778076 | test loss: 0.5167113542556763\n",
      "Epoch: 30 | loss: 0.6101900935173035 | test loss: 0.5157198309898376\n",
      "Epoch: 30 | loss: 0.6972032785415649 | test loss: 0.5156704187393188\n",
      "Epoch: 30 | loss: 0.5993484258651733 | test loss: 0.5148512721061707\n",
      "Epoch: 30 | loss: 0.5619397759437561 | test loss: 0.5149778127670288\n",
      "Epoch: 30 | loss: 0.6024416089057922 | test loss: 0.5137668251991272\n",
      "Epoch: 30 | loss: 0.3733043372631073 | test loss: 0.5132794976234436\n",
      "Epoch: 30 | loss: 0.39046332240104675 | test loss: 0.5131663680076599\n",
      "Epoch: 30 | loss: 0.49614766240119934 | test loss: 0.5126211643218994\n",
      "Epoch: 30 | loss: 0.34786078333854675 | test loss: 0.5118051171302795\n",
      "Epoch: 30 | loss: 0.5670263767242432 | test loss: 0.5118358135223389\n",
      "Epoch: 30 | loss: 0.5320088863372803 | test loss: 0.5106602907180786\n",
      "Epoch: 30 | loss: 0.6861178278923035 | test loss: 0.5109282732009888\n",
      "Epoch: 30 | loss: 0.4233017861843109 | test loss: 0.5101794600486755\n",
      "Epoch: 30 | loss: 0.5935736298561096 | test loss: 0.5104091763496399\n",
      "Epoch: 30 | loss: 0.4988361597061157 | test loss: 0.5095109939575195\n",
      "Epoch: 30 | loss: 0.4519161283969879 | test loss: 0.5092630982398987\n",
      "Epoch: 30 | loss: 0.44008442759513855 | test loss: 0.5095198154449463\n",
      "Epoch: 30 | loss: 0.5406412482261658 | test loss: 0.5091667771339417\n",
      "Epoch: 30 | loss: 0.6427804231643677 | test loss: 0.5092364549636841\n",
      "Epoch: 30 | loss: 0.48957952857017517 | test loss: 0.5078040361404419\n",
      "Epoch: 30 | loss: 0.33229121565818787 | test loss: 0.5075439214706421\n",
      "Epoch: 30 | loss: 0.33822163939476013 | test loss: 0.5081049799919128\n",
      "Epoch: 30 | loss: 0.5510693788528442 | test loss: 0.5072323083877563\n",
      "Epoch: 30 | loss: 0.48264792561531067 | test loss: 0.5070717334747314\n",
      "Epoch: 30 | loss: 0.29715242981910706 | test loss: 0.5068091154098511\n",
      "Epoch: 30 | loss: 0.5686861872673035 | test loss: 0.5050608515739441\n",
      "Epoch: 30 | loss: 0.5468435883522034 | test loss: 0.5045084953308105\n",
      "Epoch: 30 | loss: 0.5017281770706177 | test loss: 0.504624605178833\n",
      "Epoch: 30 | loss: 0.5033434629440308 | test loss: 0.5045412182807922\n",
      "Epoch: 30 | loss: 0.34929031133651733 | test loss: 0.5041917562484741\n",
      "Epoch: 30 | loss: 0.45378822088241577 | test loss: 0.5031125545501709\n",
      "Epoch: 30 | loss: 0.5829165577888489 | test loss: 0.5032609105110168\n",
      "Epoch: 31 | loss: 0.378468781709671 | test loss: 0.4118456542491913\n",
      "Epoch: 31 | loss: 0.3517160415649414 | test loss: 0.4120560586452484\n",
      "Epoch: 31 | loss: 0.49014630913734436 | test loss: 0.4114612936973572\n",
      "Epoch: 31 | loss: 0.518044114112854 | test loss: 0.4103441536426544\n",
      "Epoch: 31 | loss: 0.5447649359703064 | test loss: 0.40958040952682495\n",
      "Epoch: 31 | loss: 0.40081366896629333 | test loss: 0.40880873799324036\n",
      "Epoch: 31 | loss: 0.5496116876602173 | test loss: 0.40823718905448914\n",
      "Epoch: 31 | loss: 0.6126166582107544 | test loss: 0.4081074297428131\n",
      "Epoch: 31 | loss: 0.47127726674079895 | test loss: 0.4076780080795288\n",
      "Epoch: 31 | loss: 0.49986231327056885 | test loss: 0.40723782777786255\n",
      "Epoch: 31 | loss: 0.4167742431163788 | test loss: 0.4068751037120819\n",
      "Epoch: 31 | loss: 0.5251749157905579 | test loss: 0.40667495131492615\n",
      "Epoch: 31 | loss: 0.5635516047477722 | test loss: 0.4075707793235779\n",
      "Epoch: 31 | loss: 0.4424861967563629 | test loss: 0.4074673652648926\n",
      "Epoch: 31 | loss: 0.34753546118736267 | test loss: 0.40726161003112793\n",
      "Epoch: 31 | loss: 0.41020190715789795 | test loss: 0.4069652855396271\n",
      "Epoch: 31 | loss: 0.48748865723609924 | test loss: 0.4069766700267792\n",
      "Epoch: 31 | loss: 0.4360244572162628 | test loss: 0.40567782521247864\n",
      "Epoch: 31 | loss: 0.7836140990257263 | test loss: 0.40481114387512207\n",
      "Epoch: 31 | loss: 0.6131322383880615 | test loss: 0.40410253405570984\n",
      "Epoch: 31 | loss: 0.49251195788383484 | test loss: 0.4040655493736267\n",
      "Epoch: 31 | loss: 0.5909380316734314 | test loss: 0.4030284881591797\n",
      "Epoch: 31 | loss: 0.4942052960395813 | test loss: 0.4024524688720703\n",
      "Epoch: 31 | loss: 0.42692670226097107 | test loss: 0.4022846519947052\n",
      "Epoch: 31 | loss: 0.6210313439369202 | test loss: 0.40233853459358215\n",
      "Epoch: 31 | loss: 0.4356410801410675 | test loss: 0.40183207392692566\n",
      "Epoch: 31 | loss: 0.5532078146934509 | test loss: 0.40110892057418823\n",
      "Epoch: 31 | loss: 0.47253477573394775 | test loss: 0.4005260169506073\n",
      "Epoch: 31 | loss: 0.4143293499946594 | test loss: 0.4001479744911194\n",
      "Epoch: 31 | loss: 0.40426820516586304 | test loss: 0.40011870861053467\n",
      "Epoch: 31 | loss: 0.4658808410167694 | test loss: 0.39907824993133545\n",
      "Epoch: 31 | loss: 0.5258067846298218 | test loss: 0.399401992559433\n",
      "Epoch: 31 | loss: 0.3763844072818756 | test loss: 0.39898741245269775\n",
      "Epoch: 32 | loss: 0.4650718569755554 | test loss: 0.47894227504730225\n",
      "Epoch: 32 | loss: 0.29989004135131836 | test loss: 0.478526771068573\n",
      "Epoch: 32 | loss: 0.4258630871772766 | test loss: 0.477916419506073\n",
      "Epoch: 32 | loss: 0.3183309733867645 | test loss: 0.4775083661079407\n",
      "Epoch: 32 | loss: 0.5366879105567932 | test loss: 0.47713056206703186\n",
      "Epoch: 32 | loss: 0.4347594678401947 | test loss: 0.47685644030570984\n",
      "Epoch: 32 | loss: 0.548675537109375 | test loss: 0.4766089618206024\n",
      "Epoch: 32 | loss: 0.43833160400390625 | test loss: 0.4764493703842163\n",
      "Epoch: 32 | loss: 0.49134963750839233 | test loss: 0.47602593898773193\n",
      "Epoch: 32 | loss: 0.620958685874939 | test loss: 0.47550666332244873\n",
      "Epoch: 32 | loss: 0.395600825548172 | test loss: 0.47498539090156555\n",
      "Epoch: 32 | loss: 0.5639265775680542 | test loss: 0.4746638834476471\n",
      "Epoch: 32 | loss: 0.3238257169723511 | test loss: 0.4743470549583435\n",
      "Epoch: 32 | loss: 0.4039405882358551 | test loss: 0.47398966550827026\n",
      "Epoch: 32 | loss: 0.3974025547504425 | test loss: 0.4734819233417511\n",
      "Epoch: 32 | loss: 0.6162133812904358 | test loss: 0.47329869866371155\n",
      "Epoch: 32 | loss: 0.48214682936668396 | test loss: 0.47286444902420044\n",
      "Epoch: 32 | loss: 0.4319016635417938 | test loss: 0.472323477268219\n",
      "Epoch: 32 | loss: 0.47801873087882996 | test loss: 0.4719838798046112\n",
      "Epoch: 32 | loss: 0.5773400664329529 | test loss: 0.4719199240207672\n",
      "Epoch: 32 | loss: 0.5970040559768677 | test loss: 0.47133153676986694\n",
      "Epoch: 32 | loss: 0.4194440245628357 | test loss: 0.4710659980773926\n",
      "Epoch: 32 | loss: 0.5934044122695923 | test loss: 0.4709513783454895\n",
      "Epoch: 32 | loss: 0.28308573365211487 | test loss: 0.4706806540489197\n",
      "Epoch: 32 | loss: 0.4336254894733429 | test loss: 0.47029197216033936\n",
      "Epoch: 32 | loss: 0.6744881868362427 | test loss: 0.47005942463874817\n",
      "Epoch: 32 | loss: 0.43179044127464294 | test loss: 0.46979013085365295\n",
      "Epoch: 32 | loss: 0.46596118807792664 | test loss: 0.46925103664398193\n",
      "Epoch: 32 | loss: 0.44383418560028076 | test loss: 0.4687861502170563\n",
      "Epoch: 32 | loss: 0.458340048789978 | test loss: 0.4683494567871094\n",
      "Epoch: 32 | loss: 0.5956372618675232 | test loss: 0.4679924547672272\n",
      "Epoch: 32 | loss: 0.5509608387947083 | test loss: 0.46756401658058167\n",
      "Epoch: 32 | loss: 0.5497028827667236 | test loss: 0.4674932360649109\n",
      "Epoch: 33 | loss: 0.3935646712779999 | test loss: 0.43024447560310364\n",
      "Epoch: 33 | loss: 0.431400865316391 | test loss: 0.4308010935783386\n",
      "Epoch: 33 | loss: 0.6433077454566956 | test loss: 0.43035972118377686\n",
      "Epoch: 33 | loss: 0.6142255663871765 | test loss: 0.42990928888320923\n",
      "Epoch: 33 | loss: 0.478144109249115 | test loss: 0.43030858039855957\n",
      "Epoch: 33 | loss: 0.3105425536632538 | test loss: 0.4295791685581207\n",
      "Epoch: 33 | loss: 0.47637271881103516 | test loss: 0.4283013939857483\n",
      "Epoch: 33 | loss: 0.3428772985935211 | test loss: 0.4285147786140442\n",
      "Epoch: 33 | loss: 0.40070483088493347 | test loss: 0.42826831340789795\n",
      "Epoch: 33 | loss: 0.3041999638080597 | test loss: 0.4275544583797455\n",
      "Epoch: 33 | loss: 0.5712071061134338 | test loss: 0.4267585277557373\n",
      "Epoch: 33 | loss: 0.35082098841667175 | test loss: 0.4257371127605438\n",
      "Epoch: 33 | loss: 0.5327562689781189 | test loss: 0.4250284731388092\n",
      "Epoch: 33 | loss: 0.574634313583374 | test loss: 0.42468610405921936\n",
      "Epoch: 33 | loss: 0.5991880893707275 | test loss: 0.42434751987457275\n",
      "Epoch: 33 | loss: 0.4604797959327698 | test loss: 0.4242883026599884\n",
      "Epoch: 33 | loss: 0.3531284034252167 | test loss: 0.42398783564567566\n",
      "Epoch: 33 | loss: 0.506420373916626 | test loss: 0.423618346452713\n",
      "Epoch: 33 | loss: 0.5892418026924133 | test loss: 0.4239759147167206\n",
      "Epoch: 33 | loss: 0.4015650749206543 | test loss: 0.42304372787475586\n",
      "Epoch: 33 | loss: 0.5215585231781006 | test loss: 0.4233422875404358\n",
      "Epoch: 33 | loss: 0.31493932008743286 | test loss: 0.422432541847229\n",
      "Epoch: 33 | loss: 0.49577799439430237 | test loss: 0.42264193296432495\n",
      "Epoch: 33 | loss: 0.427641361951828 | test loss: 0.4218136668205261\n",
      "Epoch: 33 | loss: 0.5048087239265442 | test loss: 0.42067718505859375\n",
      "Epoch: 33 | loss: 0.48957476019859314 | test loss: 0.4204549491405487\n",
      "Epoch: 33 | loss: 0.6193132400512695 | test loss: 0.4199885129928589\n",
      "Epoch: 33 | loss: 0.46803608536720276 | test loss: 0.41870149970054626\n",
      "Epoch: 33 | loss: 0.3028164505958557 | test loss: 0.4184042513370514\n",
      "Epoch: 33 | loss: 0.4479665756225586 | test loss: 0.41763776540756226\n",
      "Epoch: 33 | loss: 0.540999174118042 | test loss: 0.4180140495300293\n",
      "Epoch: 33 | loss: 0.2582974135875702 | test loss: 0.4175190329551697\n",
      "Epoch: 33 | loss: 0.7060672640800476 | test loss: 0.41725167632102966\n",
      "Epoch: 34 | loss: 0.507015585899353 | test loss: 0.5094844102859497\n",
      "Epoch: 34 | loss: 0.4850562512874603 | test loss: 0.5096408724784851\n",
      "Epoch: 34 | loss: 0.3354225158691406 | test loss: 0.5093544721603394\n",
      "Epoch: 34 | loss: 0.2918654978275299 | test loss: 0.5087131261825562\n",
      "Epoch: 34 | loss: 0.49710801243782043 | test loss: 0.5083132982254028\n",
      "Epoch: 34 | loss: 0.6942355036735535 | test loss: 0.5086313486099243\n",
      "Epoch: 34 | loss: 0.3174203038215637 | test loss: 0.5084947347640991\n",
      "Epoch: 34 | loss: 0.5207049250602722 | test loss: 0.5079227089881897\n",
      "Epoch: 34 | loss: 0.41466960310935974 | test loss: 0.5072110891342163\n",
      "Epoch: 34 | loss: 0.4470098912715912 | test loss: 0.5062651634216309\n",
      "Epoch: 34 | loss: 0.4241904020309448 | test loss: 0.5060440897941589\n",
      "Epoch: 34 | loss: 0.31868335604667664 | test loss: 0.5053293108940125\n",
      "Epoch: 34 | loss: 0.6731964945793152 | test loss: 0.5045191049575806\n",
      "Epoch: 34 | loss: 0.5326092839241028 | test loss: 0.5049678087234497\n",
      "Epoch: 34 | loss: 0.37639108300209045 | test loss: 0.5048765540122986\n",
      "Epoch: 34 | loss: 0.4652029871940613 | test loss: 0.504241406917572\n",
      "Epoch: 34 | loss: 0.37490609288215637 | test loss: 0.5036488175392151\n",
      "Epoch: 34 | loss: 0.3694654405117035 | test loss: 0.503763735294342\n",
      "Epoch: 34 | loss: 0.42654526233673096 | test loss: 0.5031940340995789\n",
      "Epoch: 34 | loss: 0.38290488719940186 | test loss: 0.5025914311408997\n",
      "Epoch: 34 | loss: 0.31803905963897705 | test loss: 0.5023481845855713\n",
      "Epoch: 34 | loss: 0.6193807125091553 | test loss: 0.5022625923156738\n",
      "Epoch: 34 | loss: 0.49468740820884705 | test loss: 0.5016258955001831\n",
      "Epoch: 34 | loss: 0.36657923460006714 | test loss: 0.5007087588310242\n",
      "Epoch: 34 | loss: 0.41412100195884705 | test loss: 0.500593364238739\n",
      "Epoch: 34 | loss: 0.3743942379951477 | test loss: 0.4999005198478699\n",
      "Epoch: 34 | loss: 0.7040725350379944 | test loss: 0.49954724311828613\n",
      "Epoch: 34 | loss: 0.4097972512245178 | test loss: 0.49884265661239624\n",
      "Epoch: 34 | loss: 0.5598537921905518 | test loss: 0.4985560476779938\n",
      "Epoch: 34 | loss: 0.5175453424453735 | test loss: 0.4989568889141083\n",
      "Epoch: 34 | loss: 0.4665336608886719 | test loss: 0.49837726354599\n",
      "Epoch: 34 | loss: 0.3919355273246765 | test loss: 0.4982476234436035\n",
      "Epoch: 34 | loss: 0.32382234930992126 | test loss: 0.49783626198768616\n",
      "Epoch: 35 | loss: 0.40335023403167725 | test loss: 0.41334787011146545\n",
      "Epoch: 35 | loss: 0.4055778682231903 | test loss: 0.412322461605072\n",
      "Epoch: 35 | loss: 0.5062323212623596 | test loss: 0.411803662776947\n",
      "Epoch: 35 | loss: 0.39853954315185547 | test loss: 0.41147562861442566\n",
      "Epoch: 35 | loss: 0.4477663040161133 | test loss: 0.4107902944087982\n",
      "Epoch: 35 | loss: 0.46071287989616394 | test loss: 0.4110497236251831\n",
      "Epoch: 35 | loss: 0.5449902415275574 | test loss: 0.4105903208255768\n",
      "Epoch: 35 | loss: 0.3503069579601288 | test loss: 0.4096829295158386\n",
      "Epoch: 35 | loss: 0.6322051286697388 | test loss: 0.41026315093040466\n",
      "Epoch: 35 | loss: 0.3692699372768402 | test loss: 0.41023969650268555\n",
      "Epoch: 35 | loss: 0.49923068284988403 | test loss: 0.4098673164844513\n",
      "Epoch: 35 | loss: 0.31486475467681885 | test loss: 0.4093409776687622\n",
      "Epoch: 35 | loss: 0.5010415315628052 | test loss: 0.4095694124698639\n",
      "Epoch: 35 | loss: 0.33868032693862915 | test loss: 0.4093911051750183\n",
      "Epoch: 35 | loss: 0.4867347776889801 | test loss: 0.4091062843799591\n",
      "Epoch: 35 | loss: 0.32957983016967773 | test loss: 0.4096457064151764\n",
      "Epoch: 35 | loss: 0.5356857180595398 | test loss: 0.4085712432861328\n",
      "Epoch: 35 | loss: 0.48069921135902405 | test loss: 0.4089614152908325\n",
      "Epoch: 35 | loss: 0.4384433925151825 | test loss: 0.40859803557395935\n",
      "Epoch: 35 | loss: 0.5148364305496216 | test loss: 0.40883538126945496\n",
      "Epoch: 35 | loss: 0.4729601740837097 | test loss: 0.40846845507621765\n",
      "Epoch: 35 | loss: 0.41880330443382263 | test loss: 0.4072745442390442\n",
      "Epoch: 35 | loss: 0.4794754981994629 | test loss: 0.4066976308822632\n",
      "Epoch: 35 | loss: 0.36406946182250977 | test loss: 0.4060242176055908\n",
      "Epoch: 35 | loss: 0.3355379104614258 | test loss: 0.40527746081352234\n",
      "Epoch: 35 | loss: 0.514723002910614 | test loss: 0.40498003363609314\n",
      "Epoch: 35 | loss: 0.5160310864448547 | test loss: 0.40464943647384644\n",
      "Epoch: 35 | loss: 0.4750749170780182 | test loss: 0.40461301803588867\n",
      "Epoch: 35 | loss: 0.33732762932777405 | test loss: 0.40431150794029236\n",
      "Epoch: 35 | loss: 0.581102728843689 | test loss: 0.4038669466972351\n",
      "Epoch: 35 | loss: 0.4278467893600464 | test loss: 0.4038235545158386\n",
      "Epoch: 35 | loss: 0.41229721903800964 | test loss: 0.40387797355651855\n",
      "Epoch: 35 | loss: 0.49956992268562317 | test loss: 0.4043015241622925\n",
      "Epoch: 36 | loss: 0.4820942282676697 | test loss: 0.4212579131126404\n",
      "Epoch: 36 | loss: 0.4458923935890198 | test loss: 0.42101144790649414\n",
      "Epoch: 36 | loss: 0.4326316714286804 | test loss: 0.4207056164741516\n",
      "Epoch: 36 | loss: 0.3569624125957489 | test loss: 0.42020222544670105\n",
      "Epoch: 36 | loss: 0.4839479625225067 | test loss: 0.41982078552246094\n",
      "Epoch: 36 | loss: 0.4928131401538849 | test loss: 0.4195229113101959\n",
      "Epoch: 36 | loss: 0.3716273307800293 | test loss: 0.419206827878952\n",
      "Epoch: 36 | loss: 0.38715869188308716 | test loss: 0.41895928978919983\n",
      "Epoch: 36 | loss: 0.3030478060245514 | test loss: 0.4186820685863495\n",
      "Epoch: 36 | loss: 0.5056621432304382 | test loss: 0.41835230588912964\n",
      "Epoch: 36 | loss: 0.5092051029205322 | test loss: 0.41825756430625916\n",
      "Epoch: 36 | loss: 0.41835248470306396 | test loss: 0.4180067777633667\n",
      "Epoch: 36 | loss: 0.4187335968017578 | test loss: 0.4175291657447815\n",
      "Epoch: 36 | loss: 0.30119025707244873 | test loss: 0.4173160195350647\n",
      "Epoch: 36 | loss: 0.38845235109329224 | test loss: 0.4170001149177551\n",
      "Epoch: 36 | loss: 0.4927848279476166 | test loss: 0.4167942404747009\n",
      "Epoch: 36 | loss: 0.5512519478797913 | test loss: 0.41666379570961\n",
      "Epoch: 36 | loss: 0.5844219923019409 | test loss: 0.4164701998233795\n",
      "Epoch: 36 | loss: 0.3486904799938202 | test loss: 0.4161737263202667\n",
      "Epoch: 36 | loss: 0.6062934994697571 | test loss: 0.4159547686576843\n",
      "Epoch: 36 | loss: 0.4959264397621155 | test loss: 0.41571417450904846\n",
      "Epoch: 36 | loss: 0.48724475502967834 | test loss: 0.4154486358165741\n",
      "Epoch: 36 | loss: 0.5025252103805542 | test loss: 0.4150746762752533\n",
      "Epoch: 36 | loss: 0.3367074429988861 | test loss: 0.4147900342941284\n",
      "Epoch: 36 | loss: 0.3084368407726288 | test loss: 0.4145534932613373\n",
      "Epoch: 36 | loss: 0.4566032290458679 | test loss: 0.4142996072769165\n",
      "Epoch: 36 | loss: 0.527362585067749 | test loss: 0.41416019201278687\n",
      "Epoch: 36 | loss: 0.4483036696910858 | test loss: 0.4139387309551239\n",
      "Epoch: 36 | loss: 0.4988749325275421 | test loss: 0.4136449694633484\n",
      "Epoch: 36 | loss: 0.3737489581108093 | test loss: 0.413371205329895\n",
      "Epoch: 36 | loss: 0.3813096582889557 | test loss: 0.413135826587677\n",
      "Epoch: 36 | loss: 0.4081138074398041 | test loss: 0.4127916693687439\n",
      "Epoch: 36 | loss: 0.4089614450931549 | test loss: 0.41239118576049805\n",
      "Epoch: 37 | loss: 0.4321121871471405 | test loss: 0.4033353924751282\n",
      "Epoch: 37 | loss: 0.40322837233543396 | test loss: 0.4020279049873352\n",
      "Epoch: 37 | loss: 0.40494659543037415 | test loss: 0.4021361172199249\n",
      "Epoch: 37 | loss: 0.5425199866294861 | test loss: 0.40207239985466003\n",
      "Epoch: 37 | loss: 0.6404725313186646 | test loss: 0.4020839333534241\n",
      "Epoch: 37 | loss: 0.3657093346118927 | test loss: 0.4012144207954407\n",
      "Epoch: 37 | loss: 0.3726498782634735 | test loss: 0.401151180267334\n",
      "Epoch: 37 | loss: 0.25607284903526306 | test loss: 0.4000747799873352\n",
      "Epoch: 37 | loss: 0.41375845670700073 | test loss: 0.40043410658836365\n",
      "Epoch: 37 | loss: 0.49818745255470276 | test loss: 0.40017327666282654\n",
      "Epoch: 37 | loss: 0.28606051206588745 | test loss: 0.3998638987541199\n",
      "Epoch: 37 | loss: 0.48594141006469727 | test loss: 0.3992190361022949\n",
      "Epoch: 37 | loss: 0.6613103747367859 | test loss: 0.3997141420841217\n",
      "Epoch: 37 | loss: 0.4206297695636749 | test loss: 0.39981237053871155\n",
      "Epoch: 37 | loss: 0.32707303762435913 | test loss: 0.4000711143016815\n",
      "Epoch: 37 | loss: 0.3650158941745758 | test loss: 0.4002026915550232\n",
      "Epoch: 37 | loss: 0.42577221989631653 | test loss: 0.39989590644836426\n",
      "Epoch: 37 | loss: 0.4581941068172455 | test loss: 0.3991315960884094\n",
      "Epoch: 37 | loss: 0.5777828097343445 | test loss: 0.3989950716495514\n",
      "Epoch: 37 | loss: 0.3595637381076813 | test loss: 0.3993576765060425\n",
      "Epoch: 37 | loss: 0.46582457423210144 | test loss: 0.3996565043926239\n",
      "Epoch: 37 | loss: 0.39732423424720764 | test loss: 0.3995947241783142\n",
      "Epoch: 37 | loss: 0.4171813130378723 | test loss: 0.3989226520061493\n",
      "Epoch: 37 | loss: 0.44736939668655396 | test loss: 0.39876696467399597\n",
      "Epoch: 37 | loss: 0.48327386379241943 | test loss: 0.39824455976486206\n",
      "Epoch: 37 | loss: 0.43358123302459717 | test loss: 0.39699190855026245\n",
      "Epoch: 37 | loss: 0.4753805994987488 | test loss: 0.3967694044113159\n",
      "Epoch: 37 | loss: 0.26621726155281067 | test loss: 0.39656680822372437\n",
      "Epoch: 37 | loss: 0.40396758913993835 | test loss: 0.3961077928543091\n",
      "Epoch: 37 | loss: 0.4175890386104584 | test loss: 0.3952568769454956\n",
      "Epoch: 37 | loss: 0.4550805687904358 | test loss: 0.3948521912097931\n",
      "Epoch: 37 | loss: 0.5663384199142456 | test loss: 0.3946457803249359\n",
      "Epoch: 37 | loss: 0.2901310324668884 | test loss: 0.3939584195613861\n",
      "Epoch: 38 | loss: 0.41490164399147034 | test loss: 0.43967539072036743\n",
      "Epoch: 38 | loss: 0.40630394220352173 | test loss: 0.4393801689147949\n",
      "Epoch: 38 | loss: 0.38305792212486267 | test loss: 0.43889591097831726\n",
      "Epoch: 38 | loss: 0.3499877154827118 | test loss: 0.4384770691394806\n",
      "Epoch: 38 | loss: 0.38807523250579834 | test loss: 0.4382955729961395\n",
      "Epoch: 38 | loss: 0.412553995847702 | test loss: 0.4379968047142029\n",
      "Epoch: 38 | loss: 0.3937689960002899 | test loss: 0.4376071095466614\n",
      "Epoch: 38 | loss: 0.3879149854183197 | test loss: 0.4373313784599304\n",
      "Epoch: 38 | loss: 0.47491756081581116 | test loss: 0.4370657205581665\n",
      "Epoch: 38 | loss: 0.38728466629981995 | test loss: 0.43716639280319214\n",
      "Epoch: 38 | loss: 0.35147011280059814 | test loss: 0.43698349595069885\n",
      "Epoch: 38 | loss: 0.4602052867412567 | test loss: 0.43710827827453613\n",
      "Epoch: 38 | loss: 0.34377118945121765 | test loss: 0.4365529417991638\n",
      "Epoch: 38 | loss: 0.2935642600059509 | test loss: 0.4360411465167999\n",
      "Epoch: 38 | loss: 0.25161853432655334 | test loss: 0.4357641935348511\n",
      "Epoch: 38 | loss: 0.41825243830680847 | test loss: 0.4351516664028168\n",
      "Epoch: 38 | loss: 0.6344653964042664 | test loss: 0.43438830971717834\n",
      "Epoch: 38 | loss: 0.6780903339385986 | test loss: 0.4343334138393402\n",
      "Epoch: 38 | loss: 0.2276584655046463 | test loss: 0.43430331349372864\n",
      "Epoch: 38 | loss: 0.6323004364967346 | test loss: 0.43385788798332214\n",
      "Epoch: 38 | loss: 0.4348568320274353 | test loss: 0.43352147936820984\n",
      "Epoch: 38 | loss: 0.637101411819458 | test loss: 0.4336141049861908\n",
      "Epoch: 38 | loss: 0.41145631670951843 | test loss: 0.4331926703453064\n",
      "Epoch: 38 | loss: 0.3747793138027191 | test loss: 0.43264129757881165\n",
      "Epoch: 38 | loss: 0.4685564935207367 | test loss: 0.4324589967727661\n",
      "Epoch: 38 | loss: 0.3207732141017914 | test loss: 0.432341605424881\n",
      "Epoch: 38 | loss: 0.4256766736507416 | test loss: 0.43199265003204346\n",
      "Epoch: 38 | loss: 0.336857408285141 | test loss: 0.43169283866882324\n",
      "Epoch: 38 | loss: 0.312644898891449 | test loss: 0.4316895008087158\n",
      "Epoch: 38 | loss: 0.4621354341506958 | test loss: 0.43179529905319214\n",
      "Epoch: 38 | loss: 0.5629259347915649 | test loss: 0.4314684569835663\n",
      "Epoch: 38 | loss: 0.41130974888801575 | test loss: 0.4310312569141388\n",
      "Epoch: 38 | loss: 0.5046984553337097 | test loss: 0.4308831989765167\n",
      "Epoch: 39 | loss: 0.4159170091152191 | test loss: 0.3965339660644531\n",
      "Epoch: 39 | loss: 0.4238624572753906 | test loss: 0.39631202816963196\n",
      "Epoch: 39 | loss: 0.6230286955833435 | test loss: 0.39572805166244507\n",
      "Epoch: 39 | loss: 0.27866190671920776 | test loss: 0.3957934081554413\n",
      "Epoch: 39 | loss: 0.3887900710105896 | test loss: 0.3952808678150177\n",
      "Epoch: 39 | loss: 0.39080771803855896 | test loss: 0.3948913514614105\n",
      "Epoch: 39 | loss: 0.3910011351108551 | test loss: 0.39470991492271423\n",
      "Epoch: 39 | loss: 0.4535026550292969 | test loss: 0.3945103585720062\n",
      "Epoch: 39 | loss: 0.19302913546562195 | test loss: 0.3945275545120239\n",
      "Epoch: 39 | loss: 0.4411187469959259 | test loss: 0.394021213054657\n",
      "Epoch: 39 | loss: 0.5883849859237671 | test loss: 0.3934362232685089\n",
      "Epoch: 39 | loss: 0.2911475598812103 | test loss: 0.3930230438709259\n",
      "Epoch: 39 | loss: 0.2515018582344055 | test loss: 0.392606645822525\n",
      "Epoch: 39 | loss: 0.36998337507247925 | test loss: 0.3922399878501892\n",
      "Epoch: 39 | loss: 0.6266146898269653 | test loss: 0.39245864748954773\n",
      "Epoch: 39 | loss: 0.537098228931427 | test loss: 0.39263230562210083\n",
      "Epoch: 39 | loss: 0.5081751942634583 | test loss: 0.39290204644203186\n",
      "Epoch: 39 | loss: 0.4177871644496918 | test loss: 0.3926476240158081\n",
      "Epoch: 39 | loss: 0.3841596841812134 | test loss: 0.39234885573387146\n",
      "Epoch: 39 | loss: 0.30711907148361206 | test loss: 0.39232584834098816\n",
      "Epoch: 39 | loss: 0.4647950828075409 | test loss: 0.3921947777271271\n",
      "Epoch: 39 | loss: 0.41729673743247986 | test loss: 0.3922523856163025\n",
      "Epoch: 39 | loss: 0.4329347014427185 | test loss: 0.3916389048099518\n",
      "Epoch: 39 | loss: 0.49924972653388977 | test loss: 0.3915064036846161\n",
      "Epoch: 39 | loss: 0.3897177577018738 | test loss: 0.391316682100296\n",
      "Epoch: 39 | loss: 0.5651251673698425 | test loss: 0.39109092950820923\n",
      "Epoch: 39 | loss: 0.38167664408683777 | test loss: 0.39022397994995117\n",
      "Epoch: 39 | loss: 0.30493003129959106 | test loss: 0.39016249775886536\n",
      "Epoch: 39 | loss: 0.4721508324146271 | test loss: 0.38974499702453613\n",
      "Epoch: 39 | loss: 0.50362628698349 | test loss: 0.3896535634994507\n",
      "Epoch: 39 | loss: 0.21073265373706818 | test loss: 0.389423668384552\n",
      "Epoch: 39 | loss: 0.45032209157943726 | test loss: 0.38950708508491516\n",
      "Epoch: 39 | loss: 0.3108009994029999 | test loss: 0.3891294598579407\n",
      "Epoch: 40 | loss: 0.47364580631256104 | test loss: 0.35104286670684814\n",
      "Epoch: 40 | loss: 0.40685558319091797 | test loss: 0.3508813679218292\n",
      "Epoch: 40 | loss: 0.4264325499534607 | test loss: 0.35061389207839966\n",
      "Epoch: 40 | loss: 0.46484410762786865 | test loss: 0.3505069315433502\n",
      "Epoch: 40 | loss: 0.3845127522945404 | test loss: 0.35033369064331055\n",
      "Epoch: 40 | loss: 0.5102113485336304 | test loss: 0.3501952886581421\n",
      "Epoch: 40 | loss: 0.3089798390865326 | test loss: 0.35003167390823364\n",
      "Epoch: 40 | loss: 0.3923232853412628 | test loss: 0.34961315989494324\n",
      "Epoch: 40 | loss: 0.35095682740211487 | test loss: 0.34955117106437683\n",
      "Epoch: 40 | loss: 0.38926222920417786 | test loss: 0.349235862493515\n",
      "Epoch: 40 | loss: 0.3875880241394043 | test loss: 0.3487420082092285\n",
      "Epoch: 40 | loss: 0.40326064825057983 | test loss: 0.3486901521682739\n",
      "Epoch: 40 | loss: 0.18017734587192535 | test loss: 0.3485291302204132\n",
      "Epoch: 40 | loss: 0.3856160342693329 | test loss: 0.34831124544143677\n",
      "Epoch: 40 | loss: 0.44711557030677795 | test loss: 0.34847545623779297\n",
      "Epoch: 40 | loss: 0.536372184753418 | test loss: 0.34833404421806335\n",
      "Epoch: 40 | loss: 0.4537794888019562 | test loss: 0.3478653132915497\n",
      "Epoch: 40 | loss: 0.3091192841529846 | test loss: 0.3475715219974518\n",
      "Epoch: 40 | loss: 0.573536217212677 | test loss: 0.34739983081817627\n",
      "Epoch: 40 | loss: 0.5208870768547058 | test loss: 0.3471717834472656\n",
      "Epoch: 40 | loss: 0.3657757341861725 | test loss: 0.3470531105995178\n",
      "Epoch: 40 | loss: 0.37036311626434326 | test loss: 0.3466763496398926\n",
      "Epoch: 40 | loss: 0.3714883625507355 | test loss: 0.34633225202560425\n",
      "Epoch: 40 | loss: 0.23125435411930084 | test loss: 0.34628674387931824\n",
      "Epoch: 40 | loss: 0.34565654397010803 | test loss: 0.34603172540664673\n",
      "Epoch: 40 | loss: 0.5043433308601379 | test loss: 0.34579336643218994\n",
      "Epoch: 40 | loss: 0.5330509543418884 | test loss: 0.3453635275363922\n",
      "Epoch: 40 | loss: 0.3636617958545685 | test loss: 0.34498509764671326\n",
      "Epoch: 40 | loss: 0.37414973974227905 | test loss: 0.3450349271297455\n",
      "Epoch: 40 | loss: 0.3721185028553009 | test loss: 0.34468162059783936\n",
      "Epoch: 40 | loss: 0.3642367422580719 | test loss: 0.3445453345775604\n",
      "Epoch: 40 | loss: 0.45597630739212036 | test loss: 0.3439555764198303\n",
      "Epoch: 40 | loss: 0.36080846190452576 | test loss: 0.34373095631599426\n",
      "Epoch: 41 | loss: 0.30869564414024353 | test loss: 0.34630274772644043\n",
      "Epoch: 41 | loss: 0.3042794466018677 | test loss: 0.34600719809532166\n",
      "Epoch: 41 | loss: 0.46479347348213196 | test loss: 0.3456032872200012\n",
      "Epoch: 41 | loss: 0.3082398772239685 | test loss: 0.34546297788619995\n",
      "Epoch: 41 | loss: 0.27386245131492615 | test loss: 0.3449496924877167\n",
      "Epoch: 41 | loss: 0.3885935842990875 | test loss: 0.3445854187011719\n",
      "Epoch: 41 | loss: 0.42372503876686096 | test loss: 0.34432253241539\n",
      "Epoch: 41 | loss: 0.320757657289505 | test loss: 0.3440684974193573\n",
      "Epoch: 41 | loss: 0.5297727584838867 | test loss: 0.3437080681324005\n",
      "Epoch: 41 | loss: 0.5398022532463074 | test loss: 0.3434881865978241\n",
      "Epoch: 41 | loss: 0.4384760558605194 | test loss: 0.34339091181755066\n",
      "Epoch: 41 | loss: 0.479606032371521 | test loss: 0.34310469031333923\n",
      "Epoch: 41 | loss: 0.34401559829711914 | test loss: 0.342768132686615\n",
      "Epoch: 41 | loss: 0.2995522618293762 | test loss: 0.34254446625709534\n",
      "Epoch: 41 | loss: 0.5795096158981323 | test loss: 0.3425373435020447\n",
      "Epoch: 41 | loss: 0.42222297191619873 | test loss: 0.3423425257205963\n",
      "Epoch: 41 | loss: 0.4798923432826996 | test loss: 0.34235259890556335\n",
      "Epoch: 41 | loss: 0.533941388130188 | test loss: 0.34212905168533325\n",
      "Epoch: 41 | loss: 0.38576480746269226 | test loss: 0.34209752082824707\n",
      "Epoch: 41 | loss: 0.3900269865989685 | test loss: 0.34203097224235535\n",
      "Epoch: 41 | loss: 0.3356131613254547 | test loss: 0.3418477475643158\n",
      "Epoch: 41 | loss: 0.3551739454269409 | test loss: 0.34170860052108765\n",
      "Epoch: 41 | loss: 0.45003971457481384 | test loss: 0.3416908383369446\n",
      "Epoch: 41 | loss: 0.34270742535591125 | test loss: 0.3413793444633484\n",
      "Epoch: 41 | loss: 0.3161773085594177 | test loss: 0.3411889970302582\n",
      "Epoch: 41 | loss: 0.48231786489486694 | test loss: 0.3408644497394562\n",
      "Epoch: 41 | loss: 0.3112061619758606 | test loss: 0.3407450318336487\n",
      "Epoch: 41 | loss: 0.5472695827484131 | test loss: 0.3407824635505676\n",
      "Epoch: 41 | loss: 0.22942735254764557 | test loss: 0.3404679596424103\n",
      "Epoch: 41 | loss: 0.34223473072052 | test loss: 0.34037432074546814\n",
      "Epoch: 41 | loss: 0.4092598855495453 | test loss: 0.34018856287002563\n",
      "Epoch: 41 | loss: 0.5511165261268616 | test loss: 0.3396572172641754\n",
      "Epoch: 41 | loss: 0.40148642659187317 | test loss: 0.3392932116985321\n",
      "Epoch: 42 | loss: 0.4145922064781189 | test loss: 0.4102870523929596\n",
      "Epoch: 42 | loss: 0.4564049541950226 | test loss: 0.4104618430137634\n",
      "Epoch: 42 | loss: 0.32937896251678467 | test loss: 0.41028302907943726\n",
      "Epoch: 42 | loss: 0.28441110253334045 | test loss: 0.4101886451244354\n",
      "Epoch: 42 | loss: 0.4504568874835968 | test loss: 0.410064697265625\n",
      "Epoch: 42 | loss: 0.3704093396663666 | test loss: 0.4099290668964386\n",
      "Epoch: 42 | loss: 0.4491035044193268 | test loss: 0.4093787372112274\n",
      "Epoch: 42 | loss: 0.3009163737297058 | test loss: 0.4090363681316376\n",
      "Epoch: 42 | loss: 0.431243360042572 | test loss: 0.4087759554386139\n",
      "Epoch: 42 | loss: 0.44269976019859314 | test loss: 0.4087251126766205\n",
      "Epoch: 42 | loss: 0.3220314681529999 | test loss: 0.4087798297405243\n",
      "Epoch: 42 | loss: 0.4393918216228485 | test loss: 0.4088776409626007\n",
      "Epoch: 42 | loss: 0.45627161860466003 | test loss: 0.40877091884613037\n",
      "Epoch: 42 | loss: 0.34712132811546326 | test loss: 0.4086540937423706\n",
      "Epoch: 42 | loss: 0.3370947241783142 | test loss: 0.40846240520477295\n",
      "Epoch: 42 | loss: 0.3482867181301117 | test loss: 0.408633291721344\n",
      "Epoch: 42 | loss: 0.34848907589912415 | test loss: 0.40847456455230713\n",
      "Epoch: 42 | loss: 0.6241453886032104 | test loss: 0.4082278311252594\n",
      "Epoch: 42 | loss: 0.2998214066028595 | test loss: 0.4078352153301239\n",
      "Epoch: 42 | loss: 0.30815690755844116 | test loss: 0.40784433484077454\n",
      "Epoch: 42 | loss: 0.4964921176433563 | test loss: 0.4077470600605011\n",
      "Epoch: 42 | loss: 0.29431837797164917 | test loss: 0.40737074613571167\n",
      "Epoch: 42 | loss: 0.3900740444660187 | test loss: 0.4073525369167328\n",
      "Epoch: 42 | loss: 0.31468236446380615 | test loss: 0.4074808657169342\n",
      "Epoch: 42 | loss: 0.494171142578125 | test loss: 0.40699946880340576\n",
      "Epoch: 42 | loss: 0.3947727382183075 | test loss: 0.40695780515670776\n",
      "Epoch: 42 | loss: 0.42787763476371765 | test loss: 0.4063957631587982\n",
      "Epoch: 42 | loss: 0.4307154715061188 | test loss: 0.4060249626636505\n",
      "Epoch: 42 | loss: 0.40802592039108276 | test loss: 0.40548455715179443\n",
      "Epoch: 42 | loss: 0.46116262674331665 | test loss: 0.40521180629730225\n",
      "Epoch: 42 | loss: 0.4047430753707886 | test loss: 0.4050183594226837\n",
      "Epoch: 42 | loss: 0.3287076950073242 | test loss: 0.4046616554260254\n",
      "Epoch: 42 | loss: 0.2120881825685501 | test loss: 0.4044416546821594\n",
      "Epoch: 43 | loss: 0.4324328601360321 | test loss: 0.33666086196899414\n",
      "Epoch: 43 | loss: 0.38248732686042786 | test loss: 0.3362531065940857\n",
      "Epoch: 43 | loss: 0.2947491705417633 | test loss: 0.33607053756713867\n",
      "Epoch: 43 | loss: 0.3237212002277374 | test loss: 0.3358287811279297\n",
      "Epoch: 43 | loss: 0.3396233916282654 | test loss: 0.33544498682022095\n",
      "Epoch: 43 | loss: 0.41012436151504517 | test loss: 0.3354001045227051\n",
      "Epoch: 43 | loss: 0.3948870897293091 | test loss: 0.3353649079799652\n",
      "Epoch: 43 | loss: 0.3348504900932312 | test loss: 0.3349715769290924\n",
      "Epoch: 43 | loss: 0.3886163532733917 | test loss: 0.33438050746917725\n",
      "Epoch: 43 | loss: 0.45981135964393616 | test loss: 0.3340493142604828\n",
      "Epoch: 43 | loss: 0.4638752043247223 | test loss: 0.33389297127723694\n",
      "Epoch: 43 | loss: 0.3782108426094055 | test loss: 0.33374351263046265\n",
      "Epoch: 43 | loss: 0.5350300073623657 | test loss: 0.33343446254730225\n",
      "Epoch: 43 | loss: 0.2549309730529785 | test loss: 0.3330685794353485\n",
      "Epoch: 43 | loss: 0.44595804810523987 | test loss: 0.3327794671058655\n",
      "Epoch: 43 | loss: 0.34247228503227234 | test loss: 0.3328530490398407\n",
      "Epoch: 43 | loss: 0.3265838027000427 | test loss: 0.33242806792259216\n",
      "Epoch: 43 | loss: 0.47281649708747864 | test loss: 0.3321438729763031\n",
      "Epoch: 43 | loss: 0.44183966517448425 | test loss: 0.3322821855545044\n",
      "Epoch: 43 | loss: 0.33066514134407043 | test loss: 0.332080215215683\n",
      "Epoch: 43 | loss: 0.44554054737091064 | test loss: 0.3322153091430664\n",
      "Epoch: 43 | loss: 0.256584495306015 | test loss: 0.33219391107559204\n",
      "Epoch: 43 | loss: 0.526109516620636 | test loss: 0.33198410272598267\n",
      "Epoch: 43 | loss: 0.4323548972606659 | test loss: 0.3316807150840759\n",
      "Epoch: 43 | loss: 0.4019761383533478 | test loss: 0.3313348591327667\n",
      "Epoch: 43 | loss: 0.37855926156044006 | test loss: 0.3311552107334137\n",
      "Epoch: 43 | loss: 0.3898548185825348 | test loss: 0.3312973976135254\n",
      "Epoch: 43 | loss: 0.3748267590999603 | test loss: 0.3311406970024109\n",
      "Epoch: 43 | loss: 0.3580268323421478 | test loss: 0.3312946557998657\n",
      "Epoch: 43 | loss: 0.4993818998336792 | test loss: 0.33110225200653076\n",
      "Epoch: 43 | loss: 0.2957061529159546 | test loss: 0.33096882700920105\n",
      "Epoch: 43 | loss: 0.3054213225841522 | test loss: 0.33108630776405334\n",
      "Epoch: 43 | loss: 0.3731655776500702 | test loss: 0.3313983082771301\n",
      "Epoch: 44 | loss: 0.27393457293510437 | test loss: 0.38106685876846313\n",
      "Epoch: 44 | loss: 0.505188524723053 | test loss: 0.38084304332733154\n",
      "Epoch: 44 | loss: 0.29418548941612244 | test loss: 0.380771666765213\n",
      "Epoch: 44 | loss: 0.40331190824508667 | test loss: 0.3807532489299774\n",
      "Epoch: 44 | loss: 0.41039568185806274 | test loss: 0.3808129131793976\n",
      "Epoch: 44 | loss: 0.27769047021865845 | test loss: 0.38060903549194336\n",
      "Epoch: 44 | loss: 0.25147950649261475 | test loss: 0.3803911805152893\n",
      "Epoch: 44 | loss: 0.553741991519928 | test loss: 0.3799634277820587\n",
      "Epoch: 44 | loss: 0.24851137399673462 | test loss: 0.37982994318008423\n",
      "Epoch: 44 | loss: 0.3850412368774414 | test loss: 0.3796677887439728\n",
      "Epoch: 44 | loss: 0.2856437563896179 | test loss: 0.3793102204799652\n",
      "Epoch: 44 | loss: 0.2639389634132385 | test loss: 0.3790539503097534\n",
      "Epoch: 44 | loss: 0.4299870729446411 | test loss: 0.3789556622505188\n",
      "Epoch: 44 | loss: 0.47818487882614136 | test loss: 0.3789059817790985\n",
      "Epoch: 44 | loss: 0.4275505244731903 | test loss: 0.3787505328655243\n",
      "Epoch: 44 | loss: 0.3593156039714813 | test loss: 0.3785935640335083\n",
      "Epoch: 44 | loss: 0.327597439289093 | test loss: 0.3783682584762573\n",
      "Epoch: 44 | loss: 0.32202088832855225 | test loss: 0.37830856442451477\n",
      "Epoch: 44 | loss: 0.3748472034931183 | test loss: 0.3781577944755554\n",
      "Epoch: 44 | loss: 0.37280839681625366 | test loss: 0.37804925441741943\n",
      "Epoch: 44 | loss: 0.3403039872646332 | test loss: 0.37810203433036804\n",
      "Epoch: 44 | loss: 0.46957534551620483 | test loss: 0.3781420886516571\n",
      "Epoch: 44 | loss: 0.262688547372818 | test loss: 0.37784525752067566\n",
      "Epoch: 44 | loss: 0.23159196972846985 | test loss: 0.37755414843559265\n",
      "Epoch: 44 | loss: 0.33019036054611206 | test loss: 0.3773120045661926\n",
      "Epoch: 44 | loss: 0.5901358127593994 | test loss: 0.37713444232940674\n",
      "Epoch: 44 | loss: 0.5687581300735474 | test loss: 0.376814603805542\n",
      "Epoch: 44 | loss: 0.45084768533706665 | test loss: 0.37678706645965576\n",
      "Epoch: 44 | loss: 0.4171260595321655 | test loss: 0.37688925862312317\n",
      "Epoch: 44 | loss: 0.41627147793769836 | test loss: 0.3771013617515564\n",
      "Epoch: 44 | loss: 0.4354032874107361 | test loss: 0.3770603537559509\n",
      "Epoch: 44 | loss: 0.4030378758907318 | test loss: 0.377045601606369\n",
      "Epoch: 44 | loss: 0.3927858769893646 | test loss: 0.3768468499183655\n",
      "Epoch: 45 | loss: 0.4227997660636902 | test loss: 0.3934001922607422\n",
      "Epoch: 45 | loss: 0.4654042422771454 | test loss: 0.393200546503067\n",
      "Epoch: 45 | loss: 0.4085097908973694 | test loss: 0.3930204212665558\n",
      "Epoch: 45 | loss: 0.3606165051460266 | test loss: 0.39277660846710205\n",
      "Epoch: 45 | loss: 0.37189823389053345 | test loss: 0.3924672305583954\n",
      "Epoch: 45 | loss: 0.4148295223712921 | test loss: 0.39230626821517944\n",
      "Epoch: 45 | loss: 0.2938271164894104 | test loss: 0.39232224225997925\n",
      "Epoch: 45 | loss: 0.5587180256843567 | test loss: 0.39258572459220886\n",
      "Epoch: 45 | loss: 0.4241715669631958 | test loss: 0.39215466380119324\n",
      "Epoch: 45 | loss: 0.2977150082588196 | test loss: 0.3919743299484253\n",
      "Epoch: 45 | loss: 0.47681549191474915 | test loss: 0.3915771543979645\n",
      "Epoch: 45 | loss: 0.429364413022995 | test loss: 0.391073614358902\n",
      "Epoch: 45 | loss: 0.4185301661491394 | test loss: 0.3910192847251892\n",
      "Epoch: 45 | loss: 0.42918652296066284 | test loss: 0.39062362909317017\n",
      "Epoch: 45 | loss: 0.38416755199432373 | test loss: 0.3906051516532898\n",
      "Epoch: 45 | loss: 0.24394674599170685 | test loss: 0.3902859091758728\n",
      "Epoch: 45 | loss: 0.3428025245666504 | test loss: 0.38996756076812744\n",
      "Epoch: 45 | loss: 0.4274899363517761 | test loss: 0.38982313871383667\n",
      "Epoch: 45 | loss: 0.25559860467910767 | test loss: 0.38986238837242126\n",
      "Epoch: 45 | loss: 0.2389763444662094 | test loss: 0.38982120156288147\n",
      "Epoch: 45 | loss: 0.5072563886642456 | test loss: 0.3894606828689575\n",
      "Epoch: 45 | loss: 0.373367041349411 | test loss: 0.3891764283180237\n",
      "Epoch: 45 | loss: 0.4724539518356323 | test loss: 0.38935762643814087\n",
      "Epoch: 45 | loss: 0.1874619871377945 | test loss: 0.38916289806365967\n",
      "Epoch: 45 | loss: 0.4016459286212921 | test loss: 0.3891495168209076\n",
      "Epoch: 45 | loss: 0.48999467492103577 | test loss: 0.38917458057403564\n",
      "Epoch: 45 | loss: 0.3209828734397888 | test loss: 0.38887497782707214\n",
      "Epoch: 45 | loss: 0.344570517539978 | test loss: 0.38862693309783936\n",
      "Epoch: 45 | loss: 0.4758419990539551 | test loss: 0.38846555352211\n",
      "Epoch: 45 | loss: 0.381399929523468 | test loss: 0.388400137424469\n",
      "Epoch: 45 | loss: 0.29824259877204895 | test loss: 0.38820981979370117\n",
      "Epoch: 45 | loss: 0.20324453711509705 | test loss: 0.3884230852127075\n",
      "Epoch: 45 | loss: 0.2997283637523651 | test loss: 0.38835006952285767\n",
      "Epoch: 46 | loss: 0.4992632269859314 | test loss: 0.385347843170166\n",
      "Epoch: 46 | loss: 0.35563501715660095 | test loss: 0.38528692722320557\n",
      "Epoch: 46 | loss: 0.4183415174484253 | test loss: 0.38476258516311646\n",
      "Epoch: 46 | loss: 0.27262160181999207 | test loss: 0.384580135345459\n",
      "Epoch: 46 | loss: 0.31909021735191345 | test loss: 0.38449832797050476\n",
      "Epoch: 46 | loss: 0.3058024048805237 | test loss: 0.38438868522644043\n",
      "Epoch: 46 | loss: 0.5240622758865356 | test loss: 0.3841552734375\n",
      "Epoch: 46 | loss: 0.2623627781867981 | test loss: 0.3841330111026764\n",
      "Epoch: 46 | loss: 0.39516645669937134 | test loss: 0.3841308653354645\n",
      "Epoch: 46 | loss: 0.4825127422809601 | test loss: 0.38428404927253723\n",
      "Epoch: 46 | loss: 0.5082709193229675 | test loss: 0.38394519686698914\n",
      "Epoch: 46 | loss: 0.33428600430488586 | test loss: 0.38400399684906006\n",
      "Epoch: 46 | loss: 0.42863473296165466 | test loss: 0.38350898027420044\n",
      "Epoch: 46 | loss: 0.3806951344013214 | test loss: 0.3834240734577179\n",
      "Epoch: 46 | loss: 0.40378838777542114 | test loss: 0.38319939374923706\n",
      "Epoch: 46 | loss: 0.28075841069221497 | test loss: 0.3829410970211029\n",
      "Epoch: 46 | loss: 0.4412185549736023 | test loss: 0.3825188875198364\n",
      "Epoch: 46 | loss: 0.490705668926239 | test loss: 0.382537305355072\n",
      "Epoch: 46 | loss: 0.3975055515766144 | test loss: 0.38243693113327026\n",
      "Epoch: 46 | loss: 0.2818180024623871 | test loss: 0.3822747468948364\n",
      "Epoch: 46 | loss: 0.4605226218700409 | test loss: 0.3821205198764801\n",
      "Epoch: 46 | loss: 0.5387743711471558 | test loss: 0.3820074796676636\n",
      "Epoch: 46 | loss: 0.3216206729412079 | test loss: 0.38180986046791077\n",
      "Epoch: 46 | loss: 0.34613463282585144 | test loss: 0.38161608576774597\n",
      "Epoch: 46 | loss: 0.3997877240180969 | test loss: 0.3814566433429718\n",
      "Epoch: 46 | loss: 0.35241687297821045 | test loss: 0.38138720393180847\n",
      "Epoch: 46 | loss: 0.4718933403491974 | test loss: 0.3813561201095581\n",
      "Epoch: 46 | loss: 0.34205162525177 | test loss: 0.3815493583679199\n",
      "Epoch: 46 | loss: 0.22931097447872162 | test loss: 0.38138067722320557\n",
      "Epoch: 46 | loss: 0.1976252943277359 | test loss: 0.381252259016037\n",
      "Epoch: 46 | loss: 0.25506940484046936 | test loss: 0.38112694025039673\n",
      "Epoch: 46 | loss: 0.19683733582496643 | test loss: 0.3811008334159851\n",
      "Epoch: 46 | loss: 0.28965818881988525 | test loss: 0.3811359107494354\n",
      "Epoch: 47 | loss: 0.3596039116382599 | test loss: 0.3673399090766907\n",
      "Epoch: 47 | loss: 0.26833483576774597 | test loss: 0.36698001623153687\n",
      "Epoch: 47 | loss: 0.5924022197723389 | test loss: 0.3675093650817871\n",
      "Epoch: 47 | loss: 0.2428329885005951 | test loss: 0.3672218322753906\n",
      "Epoch: 47 | loss: 0.44134896993637085 | test loss: 0.36670657992362976\n",
      "Epoch: 47 | loss: 0.3565112352371216 | test loss: 0.36637789011001587\n",
      "Epoch: 47 | loss: 0.4139522314071655 | test loss: 0.366230845451355\n",
      "Epoch: 47 | loss: 0.3921588659286499 | test loss: 0.36583179235458374\n",
      "Epoch: 47 | loss: 0.4276442527770996 | test loss: 0.36562269926071167\n",
      "Epoch: 47 | loss: 0.34041929244995117 | test loss: 0.36541587114334106\n",
      "Epoch: 47 | loss: 0.35999780893325806 | test loss: 0.36546438932418823\n",
      "Epoch: 47 | loss: 0.3353649079799652 | test loss: 0.3652224540710449\n",
      "Epoch: 47 | loss: 0.36285144090652466 | test loss: 0.3649842143058777\n",
      "Epoch: 47 | loss: 0.23311185836791992 | test loss: 0.36501044034957886\n",
      "Epoch: 47 | loss: 0.5436509847640991 | test loss: 0.36513379216194153\n",
      "Epoch: 47 | loss: 0.3498165011405945 | test loss: 0.3647617697715759\n",
      "Epoch: 47 | loss: 0.2611173987388611 | test loss: 0.36477959156036377\n",
      "Epoch: 47 | loss: 0.5915467739105225 | test loss: 0.36444833874702454\n",
      "Epoch: 47 | loss: 0.24455784261226654 | test loss: 0.3642710745334625\n",
      "Epoch: 47 | loss: 0.47622737288475037 | test loss: 0.3641432821750641\n",
      "Epoch: 47 | loss: 0.3991251587867737 | test loss: 0.36412009596824646\n",
      "Epoch: 47 | loss: 0.26465028524398804 | test loss: 0.36427387595176697\n",
      "Epoch: 47 | loss: 0.4764120280742645 | test loss: 0.3640531897544861\n",
      "Epoch: 47 | loss: 0.4368818402290344 | test loss: 0.3637746274471283\n",
      "Epoch: 47 | loss: 0.35577499866485596 | test loss: 0.3636559844017029\n",
      "Epoch: 47 | loss: 0.38220301270484924 | test loss: 0.36329853534698486\n",
      "Epoch: 47 | loss: 0.25891363620758057 | test loss: 0.3632049560546875\n",
      "Epoch: 47 | loss: 0.23500697314739227 | test loss: 0.3632240295410156\n",
      "Epoch: 47 | loss: 0.4495026171207428 | test loss: 0.3634316623210907\n",
      "Epoch: 47 | loss: 0.2718219757080078 | test loss: 0.36334457993507385\n",
      "Epoch: 47 | loss: 0.4470362961292267 | test loss: 0.3630391061306\n",
      "Epoch: 47 | loss: 0.31373903155326843 | test loss: 0.3627550005912781\n",
      "Epoch: 47 | loss: 0.30352145433425903 | test loss: 0.36257731914520264\n",
      "Epoch: 48 | loss: 0.21553577482700348 | test loss: 0.3891400098800659\n",
      "Epoch: 48 | loss: 0.43292731046676636 | test loss: 0.3890135884284973\n",
      "Epoch: 48 | loss: 0.3503527045249939 | test loss: 0.38892662525177\n",
      "Epoch: 48 | loss: 0.36678236722946167 | test loss: 0.3891189992427826\n",
      "Epoch: 48 | loss: 0.3697682023048401 | test loss: 0.38858142495155334\n",
      "Epoch: 48 | loss: 0.29920870065689087 | test loss: 0.3884204924106598\n",
      "Epoch: 48 | loss: 0.39261192083358765 | test loss: 0.3883417248725891\n",
      "Epoch: 48 | loss: 0.49454471468925476 | test loss: 0.3882341682910919\n",
      "Epoch: 48 | loss: 0.5937063694000244 | test loss: 0.38816484808921814\n",
      "Epoch: 48 | loss: 0.40901556611061096 | test loss: 0.3875187337398529\n",
      "Epoch: 48 | loss: 0.2882973253726959 | test loss: 0.3874839246273041\n",
      "Epoch: 48 | loss: 0.2418598085641861 | test loss: 0.38722503185272217\n",
      "Epoch: 48 | loss: 0.5634171962738037 | test loss: 0.3875010907649994\n",
      "Epoch: 48 | loss: 0.4104483425617218 | test loss: 0.38715943694114685\n",
      "Epoch: 48 | loss: 0.3545997440814972 | test loss: 0.38739171624183655\n",
      "Epoch: 48 | loss: 0.25564590096473694 | test loss: 0.3874642252922058\n",
      "Epoch: 48 | loss: 0.186455637216568 | test loss: 0.3871956765651703\n",
      "Epoch: 48 | loss: 0.645055890083313 | test loss: 0.38749271631240845\n",
      "Epoch: 48 | loss: 0.2726956903934479 | test loss: 0.3874158561229706\n",
      "Epoch: 48 | loss: 0.4400445222854614 | test loss: 0.3876737356185913\n",
      "Epoch: 48 | loss: 0.2931995391845703 | test loss: 0.3875376880168915\n",
      "Epoch: 48 | loss: 0.3059733510017395 | test loss: 0.38757434487342834\n",
      "Epoch: 48 | loss: 0.4541441798210144 | test loss: 0.38801226019859314\n",
      "Epoch: 48 | loss: 0.4456520676612854 | test loss: 0.38783785700798035\n",
      "Epoch: 48 | loss: 0.33811911940574646 | test loss: 0.3874487280845642\n",
      "Epoch: 48 | loss: 0.4320206642150879 | test loss: 0.38667234778404236\n",
      "Epoch: 48 | loss: 0.41200584173202515 | test loss: 0.3867970407009125\n",
      "Epoch: 48 | loss: 0.24031668901443481 | test loss: 0.38632944226264954\n",
      "Epoch: 48 | loss: 0.22762681543827057 | test loss: 0.38611191511154175\n",
      "Epoch: 48 | loss: 0.3677405118942261 | test loss: 0.385711669921875\n",
      "Epoch: 48 | loss: 0.3703557252883911 | test loss: 0.3852372169494629\n",
      "Epoch: 48 | loss: 0.3742158114910126 | test loss: 0.3850111663341522\n",
      "Epoch: 48 | loss: 0.33150583505630493 | test loss: 0.3848317563533783\n",
      "Epoch: 49 | loss: 0.2905905544757843 | test loss: 0.37737029790878296\n",
      "Epoch: 49 | loss: 0.33222976326942444 | test loss: 0.37730053067207336\n",
      "Epoch: 49 | loss: 0.3360190689563751 | test loss: 0.37743762135505676\n",
      "Epoch: 49 | loss: 0.3424732983112335 | test loss: 0.3774695098400116\n",
      "Epoch: 49 | loss: 0.23727206885814667 | test loss: 0.37752994894981384\n",
      "Epoch: 49 | loss: 0.3196726441383362 | test loss: 0.3779066205024719\n",
      "Epoch: 49 | loss: 0.43795204162597656 | test loss: 0.37783780694007874\n",
      "Epoch: 49 | loss: 0.46190956234931946 | test loss: 0.37761279940605164\n",
      "Epoch: 49 | loss: 0.4543284773826599 | test loss: 0.3770798444747925\n",
      "Epoch: 49 | loss: 0.3758460283279419 | test loss: 0.3771691918373108\n",
      "Epoch: 49 | loss: 0.22753673791885376 | test loss: 0.37694597244262695\n",
      "Epoch: 49 | loss: 0.42177364230155945 | test loss: 0.37639519572257996\n",
      "Epoch: 49 | loss: 0.36774319410324097 | test loss: 0.37591472268104553\n",
      "Epoch: 49 | loss: 0.5335412621498108 | test loss: 0.3763241767883301\n",
      "Epoch: 49 | loss: 0.2560982406139374 | test loss: 0.37618309259414673\n",
      "Epoch: 49 | loss: 0.3122454881668091 | test loss: 0.3758675158023834\n",
      "Epoch: 49 | loss: 0.3385174572467804 | test loss: 0.37578821182250977\n",
      "Epoch: 49 | loss: 0.48574838042259216 | test loss: 0.37558552622795105\n",
      "Epoch: 49 | loss: 0.4338676333427429 | test loss: 0.3757244646549225\n",
      "Epoch: 49 | loss: 0.266025573015213 | test loss: 0.3756139874458313\n",
      "Epoch: 49 | loss: 0.3333774507045746 | test loss: 0.3754778206348419\n",
      "Epoch: 49 | loss: 0.48748013377189636 | test loss: 0.37550437450408936\n",
      "Epoch: 49 | loss: 0.4129640460014343 | test loss: 0.3753553628921509\n",
      "Epoch: 49 | loss: 0.40356388688087463 | test loss: 0.3751462399959564\n",
      "Epoch: 49 | loss: 0.3267698287963867 | test loss: 0.37505751848220825\n",
      "Epoch: 49 | loss: 0.3052877187728882 | test loss: 0.37489432096481323\n",
      "Epoch: 49 | loss: 0.32420170307159424 | test loss: 0.374599426984787\n",
      "Epoch: 49 | loss: 0.38668200373649597 | test loss: 0.37467923760414124\n",
      "Epoch: 49 | loss: 0.37866437435150146 | test loss: 0.37458282709121704\n",
      "Epoch: 49 | loss: 0.27213141322135925 | test loss: 0.3749408423900604\n",
      "Epoch: 49 | loss: 0.2139008790254593 | test loss: 0.3745327293872833\n",
      "Epoch: 49 | loss: 0.3376097083091736 | test loss: 0.3739820718765259\n",
      "Epoch: 49 | loss: 0.4598536491394043 | test loss: 0.37373924255371094\n",
      "Epoch: 50 | loss: 0.4313487708568573 | test loss: 0.39147478342056274\n",
      "Epoch: 50 | loss: 0.34558966755867004 | test loss: 0.39142879843711853\n",
      "Epoch: 50 | loss: 0.4380004107952118 | test loss: 0.39123255014419556\n",
      "Epoch: 50 | loss: 0.38191983103752136 | test loss: 0.3909805417060852\n",
      "Epoch: 50 | loss: 0.5208509564399719 | test loss: 0.39095866680145264\n",
      "Epoch: 50 | loss: 0.4169503152370453 | test loss: 0.3908306658267975\n",
      "Epoch: 50 | loss: 0.4226002097129822 | test loss: 0.3908693492412567\n",
      "Epoch: 50 | loss: 0.44852253794670105 | test loss: 0.39076852798461914\n",
      "Epoch: 50 | loss: 0.34047526121139526 | test loss: 0.39057308435440063\n",
      "Epoch: 50 | loss: 0.3040394186973572 | test loss: 0.39046499133110046\n",
      "Epoch: 50 | loss: 0.21645568311214447 | test loss: 0.390408992767334\n",
      "Epoch: 50 | loss: 0.48875540494918823 | test loss: 0.3904733955860138\n",
      "Epoch: 50 | loss: 0.35931238532066345 | test loss: 0.3903445303440094\n",
      "Epoch: 50 | loss: 0.6123520731925964 | test loss: 0.3904850482940674\n",
      "Epoch: 50 | loss: 0.23214110732078552 | test loss: 0.39022010564804077\n",
      "Epoch: 50 | loss: 0.4895777106285095 | test loss: 0.38999155163764954\n",
      "Epoch: 50 | loss: 0.4422444999217987 | test loss: 0.3899237811565399\n",
      "Epoch: 50 | loss: 0.26808157563209534 | test loss: 0.38979560136795044\n",
      "Epoch: 50 | loss: 0.3467492461204529 | test loss: 0.38966014981269836\n",
      "Epoch: 50 | loss: 0.2547183334827423 | test loss: 0.389602392911911\n",
      "Epoch: 50 | loss: 0.30268043279647827 | test loss: 0.38949111104011536\n",
      "Epoch: 50 | loss: 0.17673978209495544 | test loss: 0.38933002948760986\n",
      "Epoch: 50 | loss: 0.36255088448524475 | test loss: 0.3892105221748352\n",
      "Epoch: 50 | loss: 0.39143556356430054 | test loss: 0.3891546428203583\n",
      "Epoch: 50 | loss: 0.2023356556892395 | test loss: 0.38909149169921875\n",
      "Epoch: 50 | loss: 0.334300696849823 | test loss: 0.3888285756111145\n",
      "Epoch: 50 | loss: 0.36929649114608765 | test loss: 0.38870060443878174\n",
      "Epoch: 50 | loss: 0.3486698865890503 | test loss: 0.3887081444263458\n",
      "Epoch: 50 | loss: 0.3069230914115906 | test loss: 0.38865411281585693\n",
      "Epoch: 50 | loss: 0.28372007608413696 | test loss: 0.38856014609336853\n",
      "Epoch: 50 | loss: 0.4382780194282532 | test loss: 0.3884188234806061\n",
      "Epoch: 50 | loss: 0.23863792419433594 | test loss: 0.3883906900882721\n",
      "Epoch: 50 | loss: 0.2341615855693817 | test loss: 0.3882730305194855\n",
      "Epoch: 51 | loss: 0.50407475233078 | test loss: 0.3268376290798187\n",
      "Epoch: 51 | loss: 0.29578810930252075 | test loss: 0.3268585503101349\n",
      "Epoch: 51 | loss: 0.2831439673900604 | test loss: 0.3266071379184723\n",
      "Epoch: 51 | loss: 0.5993229150772095 | test loss: 0.326528936624527\n",
      "Epoch: 51 | loss: 0.45393139123916626 | test loss: 0.32647275924682617\n",
      "Epoch: 51 | loss: 0.26509734988212585 | test loss: 0.32645508646965027\n",
      "Epoch: 51 | loss: 0.5244057774543762 | test loss: 0.3266318440437317\n",
      "Epoch: 51 | loss: 0.23715871572494507 | test loss: 0.3265282213687897\n",
      "Epoch: 51 | loss: 0.2404433935880661 | test loss: 0.3262180685997009\n",
      "Epoch: 51 | loss: 0.32618948817253113 | test loss: 0.32607048749923706\n",
      "Epoch: 51 | loss: 0.25749915838241577 | test loss: 0.32581138610839844\n",
      "Epoch: 51 | loss: 0.26254603266716003 | test loss: 0.32571831345558167\n",
      "Epoch: 51 | loss: 0.3102019131183624 | test loss: 0.325687974691391\n",
      "Epoch: 51 | loss: 0.30740851163864136 | test loss: 0.3254755437374115\n",
      "Epoch: 51 | loss: 0.3214932978153229 | test loss: 0.32530707120895386\n",
      "Epoch: 51 | loss: 0.519122302532196 | test loss: 0.3255739212036133\n",
      "Epoch: 51 | loss: 0.24496906995773315 | test loss: 0.3251412808895111\n",
      "Epoch: 51 | loss: 0.43424320220947266 | test loss: 0.3251741826534271\n",
      "Epoch: 51 | loss: 0.38420480489730835 | test loss: 0.325091689825058\n",
      "Epoch: 51 | loss: 0.26715677976608276 | test loss: 0.3248400092124939\n",
      "Epoch: 51 | loss: 0.5115389227867126 | test loss: 0.3248984217643738\n",
      "Epoch: 51 | loss: 0.24440938234329224 | test loss: 0.32458800077438354\n",
      "Epoch: 51 | loss: 0.5356175899505615 | test loss: 0.32431986927986145\n",
      "Epoch: 51 | loss: 0.48160597681999207 | test loss: 0.3242640495300293\n",
      "Epoch: 51 | loss: 0.28975710272789 | test loss: 0.32408806681632996\n",
      "Epoch: 51 | loss: 0.3444851040840149 | test loss: 0.3240402042865753\n",
      "Epoch: 51 | loss: 0.2884562909603119 | test loss: 0.3238999843597412\n",
      "Epoch: 51 | loss: 0.17296095192432404 | test loss: 0.3238210380077362\n",
      "Epoch: 51 | loss: 0.3495299518108368 | test loss: 0.32368242740631104\n",
      "Epoch: 51 | loss: 0.3489789664745331 | test loss: 0.32346221804618835\n",
      "Epoch: 51 | loss: 0.31573373079299927 | test loss: 0.3233100473880768\n",
      "Epoch: 51 | loss: 0.2852349281311035 | test loss: 0.323105126619339\n",
      "Epoch: 51 | loss: 0.3139161765575409 | test loss: 0.32294052839279175\n",
      "Epoch: 52 | loss: 0.3298720717430115 | test loss: 0.3034043610095978\n",
      "Epoch: 52 | loss: 0.2388186901807785 | test loss: 0.3032839894294739\n",
      "Epoch: 52 | loss: 0.46782034635543823 | test loss: 0.3030650317668915\n",
      "Epoch: 52 | loss: 0.5404903888702393 | test loss: 0.3030056953430176\n",
      "Epoch: 52 | loss: 0.45979830622673035 | test loss: 0.30307865142822266\n",
      "Epoch: 52 | loss: 0.43988195061683655 | test loss: 0.30306604504585266\n",
      "Epoch: 52 | loss: 0.5036460757255554 | test loss: 0.30287954211235046\n",
      "Epoch: 52 | loss: 0.33646541833877563 | test loss: 0.3027196526527405\n",
      "Epoch: 52 | loss: 0.261956125497818 | test loss: 0.30244359374046326\n",
      "Epoch: 52 | loss: 0.23548190295696259 | test loss: 0.3023276627063751\n",
      "Epoch: 52 | loss: 0.2937917411327362 | test loss: 0.3023808002471924\n",
      "Epoch: 52 | loss: 0.2845383882522583 | test loss: 0.3022618889808655\n",
      "Epoch: 52 | loss: 0.2728186547756195 | test loss: 0.3021990954875946\n",
      "Epoch: 52 | loss: 0.3653567433357239 | test loss: 0.30194947123527527\n",
      "Epoch: 52 | loss: 0.5480678081512451 | test loss: 0.3020794987678528\n",
      "Epoch: 52 | loss: 0.3984907865524292 | test loss: 0.30173593759536743\n",
      "Epoch: 52 | loss: 0.226766437292099 | test loss: 0.3014978766441345\n",
      "Epoch: 52 | loss: 0.1761244833469391 | test loss: 0.30133184790611267\n",
      "Epoch: 52 | loss: 0.42686551809310913 | test loss: 0.30131202936172485\n",
      "Epoch: 52 | loss: 0.34511563181877136 | test loss: 0.3012520372867584\n",
      "Epoch: 52 | loss: 0.3765708804130554 | test loss: 0.30121421813964844\n",
      "Epoch: 52 | loss: 0.2666570246219635 | test loss: 0.30097106099128723\n",
      "Epoch: 52 | loss: 0.2290794551372528 | test loss: 0.3007744550704956\n",
      "Epoch: 52 | loss: 0.5527791380882263 | test loss: 0.30052000284194946\n",
      "Epoch: 52 | loss: 0.30001306533813477 | test loss: 0.30050793290138245\n",
      "Epoch: 52 | loss: 0.45427754521369934 | test loss: 0.3002910315990448\n",
      "Epoch: 52 | loss: 0.3887815475463867 | test loss: 0.30019262433052063\n",
      "Epoch: 52 | loss: 0.4815148115158081 | test loss: 0.30007147789001465\n",
      "Epoch: 52 | loss: 0.29530462622642517 | test loss: 0.30010032653808594\n",
      "Epoch: 52 | loss: 0.25731974840164185 | test loss: 0.2998224198818207\n",
      "Epoch: 52 | loss: 0.25897905230522156 | test loss: 0.29978200793266296\n",
      "Epoch: 52 | loss: 0.2409961074590683 | test loss: 0.29959458112716675\n",
      "Epoch: 52 | loss: 0.2980215847492218 | test loss: 0.29956936836242676\n",
      "Epoch: 53 | loss: 0.4381360113620758 | test loss: 0.34343165159225464\n",
      "Epoch: 53 | loss: 0.2535307705402374 | test loss: 0.3433864414691925\n",
      "Epoch: 53 | loss: 0.3778385818004608 | test loss: 0.3432014584541321\n",
      "Epoch: 53 | loss: 0.39647167921066284 | test loss: 0.3433115780353546\n",
      "Epoch: 53 | loss: 0.42197030782699585 | test loss: 0.34329643845558167\n",
      "Epoch: 53 | loss: 0.4314100444316864 | test loss: 0.34320250153541565\n",
      "Epoch: 53 | loss: 0.5633983612060547 | test loss: 0.343157559633255\n",
      "Epoch: 53 | loss: 0.25306349992752075 | test loss: 0.3430391848087311\n",
      "Epoch: 53 | loss: 0.38260334730148315 | test loss: 0.34291696548461914\n",
      "Epoch: 53 | loss: 0.4351869225502014 | test loss: 0.34275487065315247\n",
      "Epoch: 53 | loss: 0.4259587824344635 | test loss: 0.3425898551940918\n",
      "Epoch: 53 | loss: 0.3589394986629486 | test loss: 0.34251242876052856\n",
      "Epoch: 53 | loss: 0.33245474100112915 | test loss: 0.34249401092529297\n",
      "Epoch: 53 | loss: 0.13485011458396912 | test loss: 0.3423925042152405\n",
      "Epoch: 53 | loss: 0.4634232223033905 | test loss: 0.3424215018749237\n",
      "Epoch: 53 | loss: 0.20178763568401337 | test loss: 0.3423170745372772\n",
      "Epoch: 53 | loss: 0.26226022839546204 | test loss: 0.3424112796783447\n",
      "Epoch: 53 | loss: 0.19997018575668335 | test loss: 0.3422403633594513\n",
      "Epoch: 53 | loss: 0.5777305364608765 | test loss: 0.34219759702682495\n",
      "Epoch: 53 | loss: 0.1910696029663086 | test loss: 0.3420620858669281\n",
      "Epoch: 53 | loss: 0.33199065923690796 | test loss: 0.34195560216903687\n",
      "Epoch: 53 | loss: 0.22713536024093628 | test loss: 0.3418712019920349\n",
      "Epoch: 53 | loss: 0.2696300148963928 | test loss: 0.3417602479457855\n",
      "Epoch: 53 | loss: 0.6054617762565613 | test loss: 0.34168094396591187\n",
      "Epoch: 53 | loss: 0.3484676778316498 | test loss: 0.3416326642036438\n",
      "Epoch: 53 | loss: 0.30641916394233704 | test loss: 0.34162256121635437\n",
      "Epoch: 53 | loss: 0.23793068528175354 | test loss: 0.3414437770843506\n",
      "Epoch: 53 | loss: 0.27659448981285095 | test loss: 0.3413482904434204\n",
      "Epoch: 53 | loss: 0.301695317029953 | test loss: 0.341275691986084\n",
      "Epoch: 53 | loss: 0.14882256090641022 | test loss: 0.3411741554737091\n",
      "Epoch: 53 | loss: 0.5576063394546509 | test loss: 0.3409506380558014\n",
      "Epoch: 53 | loss: 0.3940185606479645 | test loss: 0.34095218777656555\n",
      "Epoch: 53 | loss: 0.42261067032814026 | test loss: 0.3408716320991516\n",
      "Epoch: 54 | loss: 0.4261026978492737 | test loss: 0.32235339283943176\n",
      "Epoch: 54 | loss: 0.46455463767051697 | test loss: 0.3222234845161438\n",
      "Epoch: 54 | loss: 0.2850989103317261 | test loss: 0.3220684826374054\n",
      "Epoch: 54 | loss: 0.2944165766239166 | test loss: 0.32204294204711914\n",
      "Epoch: 54 | loss: 0.320299357175827 | test loss: 0.3220495879650116\n",
      "Epoch: 54 | loss: 0.6507427096366882 | test loss: 0.32172977924346924\n",
      "Epoch: 54 | loss: 0.20514041185379028 | test loss: 0.3216318190097809\n",
      "Epoch: 54 | loss: 0.2783195376396179 | test loss: 0.3214806616306305\n",
      "Epoch: 54 | loss: 0.5517221689224243 | test loss: 0.32147547602653503\n",
      "Epoch: 54 | loss: 0.41415640711784363 | test loss: 0.32143667340278625\n",
      "Epoch: 54 | loss: 0.37596017122268677 | test loss: 0.3214079737663269\n",
      "Epoch: 54 | loss: 0.2670217454433441 | test loss: 0.321275532245636\n",
      "Epoch: 54 | loss: 0.3513510525226593 | test loss: 0.32113829255104065\n",
      "Epoch: 54 | loss: 0.33799681067466736 | test loss: 0.3211779296398163\n",
      "Epoch: 54 | loss: 0.27916640043258667 | test loss: 0.3210655748844147\n",
      "Epoch: 54 | loss: 0.329317569732666 | test loss: 0.3210405707359314\n",
      "Epoch: 54 | loss: 0.4832957088947296 | test loss: 0.3209781348705292\n",
      "Epoch: 54 | loss: 0.2003430277109146 | test loss: 0.3208553194999695\n",
      "Epoch: 54 | loss: 0.278871089220047 | test loss: 0.32074111700057983\n",
      "Epoch: 54 | loss: 0.29792365431785583 | test loss: 0.32057860493659973\n",
      "Epoch: 54 | loss: 0.242322638630867 | test loss: 0.32046520709991455\n",
      "Epoch: 54 | loss: 0.3468366861343384 | test loss: 0.3203805387020111\n",
      "Epoch: 54 | loss: 0.3825451135635376 | test loss: 0.32032859325408936\n",
      "Epoch: 54 | loss: 0.36160808801651 | test loss: 0.32024359703063965\n",
      "Epoch: 54 | loss: 0.49484050273895264 | test loss: 0.3202791213989258\n",
      "Epoch: 54 | loss: 0.3985268175601959 | test loss: 0.32026079297065735\n",
      "Epoch: 54 | loss: 0.21848231554031372 | test loss: 0.3202415108680725\n",
      "Epoch: 54 | loss: 0.25607752799987793 | test loss: 0.32021385431289673\n",
      "Epoch: 54 | loss: 0.17313794791698456 | test loss: 0.32013770937919617\n",
      "Epoch: 54 | loss: 0.46011438965797424 | test loss: 0.3201037645339966\n",
      "Epoch: 54 | loss: 0.2928619980812073 | test loss: 0.31994444131851196\n",
      "Epoch: 54 | loss: 0.36312559247016907 | test loss: 0.3200026750564575\n",
      "Epoch: 54 | loss: 0.2991616427898407 | test loss: 0.3199540972709656\n",
      "Epoch: 55 | loss: 0.4549570381641388 | test loss: 0.3237250745296478\n",
      "Epoch: 55 | loss: 0.34400269389152527 | test loss: 0.3235972225666046\n",
      "Epoch: 55 | loss: 0.33243006467819214 | test loss: 0.32360073924064636\n",
      "Epoch: 55 | loss: 0.3668050169944763 | test loss: 0.32359999418258667\n",
      "Epoch: 55 | loss: 0.31183940172195435 | test loss: 0.3235243856906891\n",
      "Epoch: 55 | loss: 0.1363474428653717 | test loss: 0.3234350085258484\n",
      "Epoch: 55 | loss: 0.32606640458106995 | test loss: 0.32319238781929016\n",
      "Epoch: 55 | loss: 0.2042703926563263 | test loss: 0.3230988681316376\n",
      "Epoch: 55 | loss: 0.30390119552612305 | test loss: 0.3229038119316101\n",
      "Epoch: 55 | loss: 0.47979414463043213 | test loss: 0.32297033071517944\n",
      "Epoch: 55 | loss: 0.24615921080112457 | test loss: 0.32293763756752014\n",
      "Epoch: 55 | loss: 0.288309782743454 | test loss: 0.32276710867881775\n",
      "Epoch: 55 | loss: 0.23222361505031586 | test loss: 0.32275763154029846\n",
      "Epoch: 55 | loss: 0.3233140707015991 | test loss: 0.32284075021743774\n",
      "Epoch: 55 | loss: 0.39460307359695435 | test loss: 0.32322487235069275\n",
      "Epoch: 55 | loss: 0.3717775046825409 | test loss: 0.32302501797676086\n",
      "Epoch: 55 | loss: 0.38997140526771545 | test loss: 0.32279840111732483\n",
      "Epoch: 55 | loss: 0.3057330548763275 | test loss: 0.32276350259780884\n",
      "Epoch: 55 | loss: 0.25279808044433594 | test loss: 0.32261064648628235\n",
      "Epoch: 55 | loss: 0.279823362827301 | test loss: 0.3226732611656189\n",
      "Epoch: 55 | loss: 0.23502357304096222 | test loss: 0.3225395977497101\n",
      "Epoch: 55 | loss: 0.26948079466819763 | test loss: 0.3223710060119629\n",
      "Epoch: 55 | loss: 0.594549834728241 | test loss: 0.3220844566822052\n",
      "Epoch: 55 | loss: 0.34475910663604736 | test loss: 0.3218991458415985\n",
      "Epoch: 55 | loss: 0.30369889736175537 | test loss: 0.32168230414390564\n",
      "Epoch: 55 | loss: 0.6559125781059265 | test loss: 0.32165753841400146\n",
      "Epoch: 55 | loss: 0.3606969714164734 | test loss: 0.32159101963043213\n",
      "Epoch: 55 | loss: 0.4326261281967163 | test loss: 0.3215324282646179\n",
      "Epoch: 55 | loss: 0.5889388918876648 | test loss: 0.32148197293281555\n",
      "Epoch: 55 | loss: 0.1711323857307434 | test loss: 0.3215586543083191\n",
      "Epoch: 55 | loss: 0.21237248182296753 | test loss: 0.32146185636520386\n",
      "Epoch: 55 | loss: 0.2683563530445099 | test loss: 0.3213280439376831\n",
      "Epoch: 55 | loss: 0.47974032163619995 | test loss: 0.3210127651691437\n",
      "Epoch: 56 | loss: 0.25859788060188293 | test loss: 0.34528976678848267\n",
      "Epoch: 56 | loss: 0.32404905557632446 | test loss: 0.34518733620643616\n",
      "Epoch: 56 | loss: 0.33422133326530457 | test loss: 0.34505361318588257\n",
      "Epoch: 56 | loss: 0.35488608479499817 | test loss: 0.3449135720729828\n",
      "Epoch: 56 | loss: 0.5694140195846558 | test loss: 0.3449229896068573\n",
      "Epoch: 56 | loss: 0.2675837278366089 | test loss: 0.3449712097644806\n",
      "Epoch: 56 | loss: 0.3963460624217987 | test loss: 0.3449564278125763\n",
      "Epoch: 56 | loss: 0.2885046899318695 | test loss: 0.3448702394962311\n",
      "Epoch: 56 | loss: 0.32094430923461914 | test loss: 0.3446352183818817\n",
      "Epoch: 56 | loss: 0.30741891264915466 | test loss: 0.34456726908683777\n",
      "Epoch: 56 | loss: 0.21423174440860748 | test loss: 0.34445512294769287\n",
      "Epoch: 56 | loss: 0.4880782663822174 | test loss: 0.3443354666233063\n",
      "Epoch: 56 | loss: 0.3109535276889801 | test loss: 0.3441886007785797\n",
      "Epoch: 56 | loss: 0.4634733200073242 | test loss: 0.3441721796989441\n",
      "Epoch: 56 | loss: 0.25615814328193665 | test loss: 0.34405213594436646\n",
      "Epoch: 56 | loss: 0.5513393878936768 | test loss: 0.34401026368141174\n",
      "Epoch: 56 | loss: 0.33456936478614807 | test loss: 0.34393084049224854\n",
      "Epoch: 56 | loss: 0.24306614696979523 | test loss: 0.3438195288181305\n",
      "Epoch: 56 | loss: 0.2550521492958069 | test loss: 0.3436991274356842\n",
      "Epoch: 56 | loss: 0.4521450400352478 | test loss: 0.3436124920845032\n",
      "Epoch: 56 | loss: 0.4182228744029999 | test loss: 0.34351250529289246\n",
      "Epoch: 56 | loss: 0.3150719404220581 | test loss: 0.3434118926525116\n",
      "Epoch: 56 | loss: 0.15914011001586914 | test loss: 0.343425452709198\n",
      "Epoch: 56 | loss: 0.3596131503582001 | test loss: 0.34335631132125854\n",
      "Epoch: 56 | loss: 0.24980440735816956 | test loss: 0.3433549702167511\n",
      "Epoch: 56 | loss: 0.3651224374771118 | test loss: 0.34319746494293213\n",
      "Epoch: 56 | loss: 0.22140057384967804 | test loss: 0.34307026863098145\n",
      "Epoch: 56 | loss: 0.39493367075920105 | test loss: 0.3429236114025116\n",
      "Epoch: 56 | loss: 0.41708406805992126 | test loss: 0.34294652938842773\n",
      "Epoch: 56 | loss: 0.3202061355113983 | test loss: 0.3429071605205536\n",
      "Epoch: 56 | loss: 0.27966201305389404 | test loss: 0.3428061306476593\n",
      "Epoch: 56 | loss: 0.3864392042160034 | test loss: 0.34276971220970154\n",
      "Epoch: 56 | loss: 0.2747423052787781 | test loss: 0.3426697850227356\n",
      "Epoch: 57 | loss: 0.3078376054763794 | test loss: 0.3544512391090393\n",
      "Epoch: 57 | loss: 0.28405240178108215 | test loss: 0.3543919026851654\n",
      "Epoch: 57 | loss: 0.4073183238506317 | test loss: 0.3543645739555359\n",
      "Epoch: 57 | loss: 0.33465954661369324 | test loss: 0.35433295369148254\n",
      "Epoch: 57 | loss: 0.18873146176338196 | test loss: 0.35433050990104675\n",
      "Epoch: 57 | loss: 0.36319389939308167 | test loss: 0.35435616970062256\n",
      "Epoch: 57 | loss: 0.42872676253318787 | test loss: 0.3541989326477051\n",
      "Epoch: 57 | loss: 0.43308407068252563 | test loss: 0.35419154167175293\n",
      "Epoch: 57 | loss: 0.562847375869751 | test loss: 0.3541603684425354\n",
      "Epoch: 57 | loss: 0.2778533399105072 | test loss: 0.35393983125686646\n",
      "Epoch: 57 | loss: 0.2565520107746124 | test loss: 0.35387474298477173\n",
      "Epoch: 57 | loss: 0.2710968852043152 | test loss: 0.35383251309394836\n",
      "Epoch: 57 | loss: 0.294158011674881 | test loss: 0.35377123951911926\n",
      "Epoch: 57 | loss: 0.43811270594596863 | test loss: 0.3535895347595215\n",
      "Epoch: 57 | loss: 0.20484106242656708 | test loss: 0.35349246859550476\n",
      "Epoch: 57 | loss: 0.28301379084587097 | test loss: 0.35318198800086975\n",
      "Epoch: 57 | loss: 0.19237135350704193 | test loss: 0.3531252443790436\n",
      "Epoch: 57 | loss: 0.26267874240875244 | test loss: 0.3530662953853607\n",
      "Epoch: 57 | loss: 0.45157158374786377 | test loss: 0.35293567180633545\n",
      "Epoch: 57 | loss: 0.2636815309524536 | test loss: 0.3528665006160736\n",
      "Epoch: 57 | loss: 0.21537114679813385 | test loss: 0.3528006374835968\n",
      "Epoch: 57 | loss: 0.47206059098243713 | test loss: 0.3528069257736206\n",
      "Epoch: 57 | loss: 0.2128903716802597 | test loss: 0.35274332761764526\n",
      "Epoch: 57 | loss: 0.3053819239139557 | test loss: 0.35269778966903687\n",
      "Epoch: 57 | loss: 0.41491788625717163 | test loss: 0.3526841402053833\n",
      "Epoch: 57 | loss: 0.2713032066822052 | test loss: 0.35257500410079956\n",
      "Epoch: 57 | loss: 0.27701523900032043 | test loss: 0.35250359773635864\n",
      "Epoch: 57 | loss: 0.3165016770362854 | test loss: 0.3524995446205139\n",
      "Epoch: 57 | loss: 0.456190288066864 | test loss: 0.35239529609680176\n",
      "Epoch: 57 | loss: 0.40237659215927124 | test loss: 0.35227179527282715\n",
      "Epoch: 57 | loss: 0.29243892431259155 | test loss: 0.35218551754951477\n",
      "Epoch: 57 | loss: 0.3947410583496094 | test loss: 0.3521087169647217\n",
      "Epoch: 57 | loss: 0.34123119711875916 | test loss: 0.3520044684410095\n",
      "Epoch: 58 | loss: 0.23227712512016296 | test loss: 0.2586697041988373\n",
      "Epoch: 58 | loss: 0.39799779653549194 | test loss: 0.2579587697982788\n",
      "Epoch: 58 | loss: 0.3842044770717621 | test loss: 0.2578243315219879\n",
      "Epoch: 58 | loss: 0.3882291615009308 | test loss: 0.25699901580810547\n",
      "Epoch: 58 | loss: 0.39128342270851135 | test loss: 0.25618472695350647\n",
      "Epoch: 58 | loss: 0.542550265789032 | test loss: 0.2572407126426697\n",
      "Epoch: 58 | loss: 0.22819852828979492 | test loss: 0.25729936361312866\n",
      "Epoch: 58 | loss: 0.26799872517585754 | test loss: 0.2572210729122162\n",
      "Epoch: 58 | loss: 0.30168628692626953 | test loss: 0.25764328241348267\n",
      "Epoch: 58 | loss: 0.26981034874916077 | test loss: 0.2577270269393921\n",
      "Epoch: 58 | loss: 0.3159480690956116 | test loss: 0.25781774520874023\n",
      "Epoch: 58 | loss: 0.29952672123908997 | test loss: 0.25772613286972046\n",
      "Epoch: 58 | loss: 0.2824936509132385 | test loss: 0.25756943225860596\n",
      "Epoch: 58 | loss: 0.21161866188049316 | test loss: 0.25743168592453003\n",
      "Epoch: 58 | loss: 0.3229638636112213 | test loss: 0.25790899991989136\n",
      "Epoch: 58 | loss: 0.36793872714042664 | test loss: 0.25777220726013184\n",
      "Epoch: 58 | loss: 0.286069393157959 | test loss: 0.25739914178848267\n",
      "Epoch: 58 | loss: 0.46847936511039734 | test loss: 0.2572191059589386\n",
      "Epoch: 58 | loss: 0.2470133900642395 | test loss: 0.25699615478515625\n",
      "Epoch: 58 | loss: 0.38221246004104614 | test loss: 0.257216215133667\n",
      "Epoch: 58 | loss: 0.36023035645484924 | test loss: 0.25725311040878296\n",
      "Epoch: 58 | loss: 0.1738198697566986 | test loss: 0.2574140429496765\n",
      "Epoch: 58 | loss: 0.32119518518447876 | test loss: 0.2572690546512604\n",
      "Epoch: 58 | loss: 0.4219786822795868 | test loss: 0.25696611404418945\n",
      "Epoch: 58 | loss: 0.2819420397281647 | test loss: 0.256724089384079\n",
      "Epoch: 58 | loss: 0.3229465186595917 | test loss: 0.256510853767395\n",
      "Epoch: 58 | loss: 0.5917350053787231 | test loss: 0.25695180892944336\n",
      "Epoch: 58 | loss: 0.18813472986221313 | test loss: 0.2572130560874939\n",
      "Epoch: 58 | loss: 0.3158092200756073 | test loss: 0.25682783126831055\n",
      "Epoch: 58 | loss: 0.35800662636756897 | test loss: 0.2571330666542053\n",
      "Epoch: 58 | loss: 0.3336634635925293 | test loss: 0.25634104013442993\n",
      "Epoch: 58 | loss: 0.2788875699043274 | test loss: 0.2560117840766907\n",
      "Epoch: 58 | loss: 0.3473455607891083 | test loss: 0.25624170899391174\n",
      "Epoch: 59 | loss: 0.19689355790615082 | test loss: 0.2846177816390991\n",
      "Epoch: 59 | loss: 0.23888806998729706 | test loss: 0.2846872806549072\n",
      "Epoch: 59 | loss: 0.2481403648853302 | test loss: 0.28465506434440613\n",
      "Epoch: 59 | loss: 0.4537658095359802 | test loss: 0.28511783480644226\n",
      "Epoch: 59 | loss: 0.1918698102235794 | test loss: 0.28506988286972046\n",
      "Epoch: 59 | loss: 0.3842085003852844 | test loss: 0.2849958837032318\n",
      "Epoch: 59 | loss: 0.33547845482826233 | test loss: 0.2847323417663574\n",
      "Epoch: 59 | loss: 0.23343238234519958 | test loss: 0.2848440706729889\n",
      "Epoch: 59 | loss: 0.3523460626602173 | test loss: 0.2848103642463684\n",
      "Epoch: 59 | loss: 0.39924076199531555 | test loss: 0.284483939409256\n",
      "Epoch: 59 | loss: 0.26799800992012024 | test loss: 0.2843017280101776\n",
      "Epoch: 59 | loss: 0.60416579246521 | test loss: 0.2839270234107971\n",
      "Epoch: 59 | loss: 0.35877808928489685 | test loss: 0.2835584878921509\n",
      "Epoch: 59 | loss: 0.2800946831703186 | test loss: 0.2833508849143982\n",
      "Epoch: 59 | loss: 0.1807977557182312 | test loss: 0.2830156087875366\n",
      "Epoch: 59 | loss: 0.3485642373561859 | test loss: 0.2830389738082886\n",
      "Epoch: 59 | loss: 0.5194891095161438 | test loss: 0.2829491198062897\n",
      "Epoch: 59 | loss: 0.23263010382652283 | test loss: 0.2826289236545563\n",
      "Epoch: 59 | loss: 0.2043299376964569 | test loss: 0.28252077102661133\n",
      "Epoch: 59 | loss: 0.33651286363601685 | test loss: 0.28234800696372986\n",
      "Epoch: 59 | loss: 0.30716437101364136 | test loss: 0.282314658164978\n",
      "Epoch: 59 | loss: 0.43888577818870544 | test loss: 0.28235355019569397\n",
      "Epoch: 59 | loss: 0.38500693440437317 | test loss: 0.2825249433517456\n",
      "Epoch: 59 | loss: 0.301718145608902 | test loss: 0.2825903296470642\n",
      "Epoch: 59 | loss: 0.4137524962425232 | test loss: 0.282352477312088\n",
      "Epoch: 59 | loss: 0.2316320240497589 | test loss: 0.2823486626148224\n",
      "Epoch: 59 | loss: 0.4554833173751831 | test loss: 0.2821347117424011\n",
      "Epoch: 59 | loss: 0.28558555245399475 | test loss: 0.2819088399410248\n",
      "Epoch: 59 | loss: 0.2850087881088257 | test loss: 0.28209829330444336\n",
      "Epoch: 59 | loss: 0.29244890809059143 | test loss: 0.28198134899139404\n",
      "Epoch: 59 | loss: 0.47236573696136475 | test loss: 0.2819754183292389\n",
      "Epoch: 59 | loss: 0.2815081775188446 | test loss: 0.28212130069732666\n",
      "Epoch: 59 | loss: 0.3318820297718048 | test loss: 0.28206026554107666\n",
      "Epoch: 60 | loss: 0.3703542947769165 | test loss: 0.2803603410720825\n",
      "Epoch: 60 | loss: 0.3037300407886505 | test loss: 0.27986690402030945\n",
      "Epoch: 60 | loss: 0.28503885865211487 | test loss: 0.28012722730636597\n",
      "Epoch: 60 | loss: 0.3244643211364746 | test loss: 0.28006505966186523\n",
      "Epoch: 60 | loss: 0.3824729323387146 | test loss: 0.2801038324832916\n",
      "Epoch: 60 | loss: 0.5061020851135254 | test loss: 0.28041982650756836\n",
      "Epoch: 60 | loss: 0.39988547563552856 | test loss: 0.2805999219417572\n",
      "Epoch: 60 | loss: 0.1971508413553238 | test loss: 0.2799772620201111\n",
      "Epoch: 60 | loss: 0.44457361102104187 | test loss: 0.27990254759788513\n",
      "Epoch: 60 | loss: 0.316581666469574 | test loss: 0.2801239490509033\n",
      "Epoch: 60 | loss: 0.1800517588853836 | test loss: 0.28026196360588074\n",
      "Epoch: 60 | loss: 0.26608937978744507 | test loss: 0.28057247400283813\n",
      "Epoch: 60 | loss: 0.3490372598171234 | test loss: 0.2795368432998657\n",
      "Epoch: 60 | loss: 0.586144208908081 | test loss: 0.27944517135620117\n",
      "Epoch: 60 | loss: 0.2662835419178009 | test loss: 0.27933329343795776\n",
      "Epoch: 60 | loss: 0.11806900054216385 | test loss: 0.27951541543006897\n",
      "Epoch: 60 | loss: 0.36777588725090027 | test loss: 0.27939966320991516\n",
      "Epoch: 60 | loss: 0.21028117835521698 | test loss: 0.2792005240917206\n",
      "Epoch: 60 | loss: 0.24051375687122345 | test loss: 0.2790376543998718\n",
      "Epoch: 60 | loss: 0.5634070634841919 | test loss: 0.278868705034256\n",
      "Epoch: 60 | loss: 0.4069056808948517 | test loss: 0.2793714106082916\n",
      "Epoch: 60 | loss: 0.3762388229370117 | test loss: 0.27966099977493286\n",
      "Epoch: 60 | loss: 0.235212042927742 | test loss: 0.28012561798095703\n",
      "Epoch: 60 | loss: 0.3465539216995239 | test loss: 0.28031066060066223\n",
      "Epoch: 60 | loss: 0.3481031358242035 | test loss: 0.2802846431732178\n",
      "Epoch: 60 | loss: 0.22131390869617462 | test loss: 0.28046509623527527\n",
      "Epoch: 60 | loss: 0.4567374289035797 | test loss: 0.28046780824661255\n",
      "Epoch: 60 | loss: 0.36182498931884766 | test loss: 0.2806175649166107\n",
      "Epoch: 60 | loss: 0.32016319036483765 | test loss: 0.2804872393608093\n",
      "Epoch: 60 | loss: 0.26759037375450134 | test loss: 0.28021833300590515\n",
      "Epoch: 60 | loss: 0.4067634642124176 | test loss: 0.2796523869037628\n",
      "Epoch: 60 | loss: 0.13300636410713196 | test loss: 0.2794068157672882\n",
      "Epoch: 60 | loss: 0.3175528347492218 | test loss: 0.279078871011734\n",
      "Epoch: 61 | loss: 0.22887013852596283 | test loss: 0.32089003920555115\n",
      "Epoch: 61 | loss: 0.2653883695602417 | test loss: 0.32082366943359375\n",
      "Epoch: 61 | loss: 0.4966426193714142 | test loss: 0.3205986022949219\n",
      "Epoch: 61 | loss: 0.26948603987693787 | test loss: 0.32005172967910767\n",
      "Epoch: 61 | loss: 0.2936018109321594 | test loss: 0.320020854473114\n",
      "Epoch: 61 | loss: 0.48090511560440063 | test loss: 0.320023775100708\n",
      "Epoch: 61 | loss: 0.4352434575557709 | test loss: 0.32034438848495483\n",
      "Epoch: 61 | loss: 0.2412298172712326 | test loss: 0.3202768862247467\n",
      "Epoch: 61 | loss: 0.5472100973129272 | test loss: 0.32094356417655945\n",
      "Epoch: 61 | loss: 0.24922695755958557 | test loss: 0.3201873302459717\n",
      "Epoch: 61 | loss: 0.3301672637462616 | test loss: 0.3197460174560547\n",
      "Epoch: 61 | loss: 0.30974265933036804 | test loss: 0.31967592239379883\n",
      "Epoch: 61 | loss: 0.2859618365764618 | test loss: 0.3193667531013489\n",
      "Epoch: 61 | loss: 0.2386331856250763 | test loss: 0.3194144666194916\n",
      "Epoch: 61 | loss: 0.3164910078048706 | test loss: 0.3191465735435486\n",
      "Epoch: 61 | loss: 0.1989174783229828 | test loss: 0.3190627992153168\n",
      "Epoch: 61 | loss: 0.30809295177459717 | test loss: 0.31902772188186646\n",
      "Epoch: 61 | loss: 0.23720680177211761 | test loss: 0.31910836696624756\n",
      "Epoch: 61 | loss: 0.3713308572769165 | test loss: 0.3195929527282715\n",
      "Epoch: 61 | loss: 0.37640029191970825 | test loss: 0.3188522458076477\n",
      "Epoch: 61 | loss: 0.5869593024253845 | test loss: 0.319100946187973\n",
      "Epoch: 61 | loss: 0.18001794815063477 | test loss: 0.3190353214740753\n",
      "Epoch: 61 | loss: 0.43198201060295105 | test loss: 0.31879475712776184\n",
      "Epoch: 61 | loss: 0.3887570798397064 | test loss: 0.31913021206855774\n",
      "Epoch: 61 | loss: 0.2744806408882141 | test loss: 0.3192676901817322\n",
      "Epoch: 61 | loss: 0.24222072958946228 | test loss: 0.31920135021209717\n",
      "Epoch: 61 | loss: 0.4202292859554291 | test loss: 0.3193563222885132\n",
      "Epoch: 61 | loss: 0.5203981399536133 | test loss: 0.3198998272418976\n",
      "Epoch: 61 | loss: 0.24300609529018402 | test loss: 0.3198050558567047\n",
      "Epoch: 61 | loss: 0.325320303440094 | test loss: 0.3192668557167053\n",
      "Epoch: 61 | loss: 0.18680939078330994 | test loss: 0.31909072399139404\n",
      "Epoch: 61 | loss: 0.3389558494091034 | test loss: 0.31912678480148315\n",
      "Epoch: 61 | loss: 0.25924497842788696 | test loss: 0.3189665973186493\n",
      "Epoch: 62 | loss: 0.29760533571243286 | test loss: 0.3240487575531006\n",
      "Epoch: 62 | loss: 0.27085626125335693 | test loss: 0.32393917441368103\n",
      "Epoch: 62 | loss: 0.19059576094150543 | test loss: 0.3238700032234192\n",
      "Epoch: 62 | loss: 0.6761664152145386 | test loss: 0.3238220512866974\n",
      "Epoch: 62 | loss: 0.2612735629081726 | test loss: 0.3237866163253784\n",
      "Epoch: 62 | loss: 0.2433241456747055 | test loss: 0.323737770318985\n",
      "Epoch: 62 | loss: 0.4711710810661316 | test loss: 0.32377687096595764\n",
      "Epoch: 62 | loss: 0.2667299211025238 | test loss: 0.3237174451351166\n",
      "Epoch: 62 | loss: 0.31751272082328796 | test loss: 0.3236079514026642\n",
      "Epoch: 62 | loss: 0.20114591717720032 | test loss: 0.32351353764533997\n",
      "Epoch: 62 | loss: 0.26885268092155457 | test loss: 0.3234477937221527\n",
      "Epoch: 62 | loss: 0.37509873509407043 | test loss: 0.32335641980171204\n",
      "Epoch: 62 | loss: 0.3704204559326172 | test loss: 0.3233070373535156\n",
      "Epoch: 62 | loss: 0.16722798347473145 | test loss: 0.32321786880493164\n",
      "Epoch: 62 | loss: 0.33242982625961304 | test loss: 0.3231296241283417\n",
      "Epoch: 62 | loss: 0.5992268323898315 | test loss: 0.3230491876602173\n",
      "Epoch: 62 | loss: 0.29900750517845154 | test loss: 0.32302001118659973\n",
      "Epoch: 62 | loss: 0.34531980752944946 | test loss: 0.32299792766571045\n",
      "Epoch: 62 | loss: 0.2250789850950241 | test loss: 0.32293346524238586\n",
      "Epoch: 62 | loss: 0.4083106815814972 | test loss: 0.3228304088115692\n",
      "Epoch: 62 | loss: 0.29244011640548706 | test loss: 0.322795033454895\n",
      "Epoch: 62 | loss: 0.4625338912010193 | test loss: 0.32277005910873413\n",
      "Epoch: 62 | loss: 0.327293336391449 | test loss: 0.322729229927063\n",
      "Epoch: 62 | loss: 0.3171294331550598 | test loss: 0.3226485550403595\n",
      "Epoch: 62 | loss: 0.33669817447662354 | test loss: 0.32271718978881836\n",
      "Epoch: 62 | loss: 0.27382344007492065 | test loss: 0.3227148950099945\n",
      "Epoch: 62 | loss: 0.29652538895606995 | test loss: 0.32257384061813354\n",
      "Epoch: 62 | loss: 0.2091851532459259 | test loss: 0.32248374819755554\n",
      "Epoch: 62 | loss: 0.41324394941329956 | test loss: 0.322341650724411\n",
      "Epoch: 62 | loss: 0.13778619468212128 | test loss: 0.32227009534835815\n",
      "Epoch: 62 | loss: 0.16341574490070343 | test loss: 0.322249174118042\n",
      "Epoch: 62 | loss: 0.35117805004119873 | test loss: 0.3221638798713684\n",
      "Epoch: 62 | loss: 0.4615406394004822 | test loss: 0.3221309781074524\n",
      "Epoch: 63 | loss: 0.2445049285888672 | test loss: 0.32366180419921875\n",
      "Epoch: 63 | loss: 0.2676859498023987 | test loss: 0.3235311210155487\n",
      "Epoch: 63 | loss: 0.25264114141464233 | test loss: 0.32349714636802673\n",
      "Epoch: 63 | loss: 0.26064813137054443 | test loss: 0.3234286904335022\n",
      "Epoch: 63 | loss: 0.283579021692276 | test loss: 0.3233586847782135\n",
      "Epoch: 63 | loss: 0.4234250485897064 | test loss: 0.32340630888938904\n",
      "Epoch: 63 | loss: 0.3539884686470032 | test loss: 0.32356324791908264\n",
      "Epoch: 63 | loss: 0.4878520369529724 | test loss: 0.32351019978523254\n",
      "Epoch: 63 | loss: 0.2810678482055664 | test loss: 0.32322534918785095\n",
      "Epoch: 63 | loss: 0.32442331314086914 | test loss: 0.32312631607055664\n",
      "Epoch: 63 | loss: 0.4145541489124298 | test loss: 0.3229539096355438\n",
      "Epoch: 63 | loss: 0.5361756682395935 | test loss: 0.3228699266910553\n",
      "Epoch: 63 | loss: 0.07793112099170685 | test loss: 0.3228452801704407\n",
      "Epoch: 63 | loss: 0.21255846321582794 | test loss: 0.32265934348106384\n",
      "Epoch: 63 | loss: 0.38212713599205017 | test loss: 0.32273390889167786\n",
      "Epoch: 63 | loss: 0.4356975555419922 | test loss: 0.3227537274360657\n",
      "Epoch: 63 | loss: 0.20757372677326202 | test loss: 0.32271087169647217\n",
      "Epoch: 63 | loss: 0.3129900097846985 | test loss: 0.3225392699241638\n",
      "Epoch: 63 | loss: 0.38096463680267334 | test loss: 0.3225170969963074\n",
      "Epoch: 63 | loss: 0.17716637253761292 | test loss: 0.3225347399711609\n",
      "Epoch: 63 | loss: 0.2730429470539093 | test loss: 0.32260504364967346\n",
      "Epoch: 63 | loss: 0.44886314868927 | test loss: 0.322247713804245\n",
      "Epoch: 63 | loss: 0.27859872579574585 | test loss: 0.322214812040329\n",
      "Epoch: 63 | loss: 0.21587347984313965 | test loss: 0.3222913146018982\n",
      "Epoch: 63 | loss: 0.5493342280387878 | test loss: 0.32221484184265137\n",
      "Epoch: 63 | loss: 0.2931836247444153 | test loss: 0.32259976863861084\n",
      "Epoch: 63 | loss: 0.3266250789165497 | test loss: 0.3221830725669861\n",
      "Epoch: 63 | loss: 0.38655391335487366 | test loss: 0.3221239149570465\n",
      "Epoch: 63 | loss: 0.3463490605354309 | test loss: 0.32200130820274353\n",
      "Epoch: 63 | loss: 0.19387395679950714 | test loss: 0.321914404630661\n",
      "Epoch: 63 | loss: 0.4012009799480438 | test loss: 0.32175326347351074\n",
      "Epoch: 63 | loss: 0.2673391103744507 | test loss: 0.321692556142807\n",
      "Epoch: 63 | loss: 0.29453906416893005 | test loss: 0.32147154211997986\n",
      "Epoch: 64 | loss: 0.3927113115787506 | test loss: 0.33083441853523254\n",
      "Epoch: 64 | loss: 0.28476089239120483 | test loss: 0.3307996392250061\n",
      "Epoch: 64 | loss: 0.21598359942436218 | test loss: 0.3308093249797821\n",
      "Epoch: 64 | loss: 0.3297549784183502 | test loss: 0.33069130778312683\n",
      "Epoch: 64 | loss: 0.33048197627067566 | test loss: 0.33064955472946167\n",
      "Epoch: 64 | loss: 0.20964846014976501 | test loss: 0.3306049108505249\n",
      "Epoch: 64 | loss: 0.38211897015571594 | test loss: 0.33051761984825134\n",
      "Epoch: 64 | loss: 0.26587578654289246 | test loss: 0.3304622769355774\n",
      "Epoch: 64 | loss: 0.24840472638607025 | test loss: 0.33040252327919006\n",
      "Epoch: 64 | loss: 0.3118203282356262 | test loss: 0.33037522435188293\n",
      "Epoch: 64 | loss: 0.42741858959198 | test loss: 0.33036917448043823\n",
      "Epoch: 64 | loss: 0.3308046758174896 | test loss: 0.33034420013427734\n",
      "Epoch: 64 | loss: 0.14837852120399475 | test loss: 0.33027759194374084\n",
      "Epoch: 64 | loss: 0.7097568511962891 | test loss: 0.33015114068984985\n",
      "Epoch: 64 | loss: 0.49497759342193604 | test loss: 0.330086886882782\n",
      "Epoch: 64 | loss: 0.13114053010940552 | test loss: 0.3300055265426636\n",
      "Epoch: 64 | loss: 0.2170601487159729 | test loss: 0.32998067140579224\n",
      "Epoch: 64 | loss: 0.3781481683254242 | test loss: 0.32991859316825867\n",
      "Epoch: 64 | loss: 0.33555543422698975 | test loss: 0.32985880970954895\n",
      "Epoch: 64 | loss: 0.3332696259021759 | test loss: 0.32974642515182495\n",
      "Epoch: 64 | loss: 0.33629316091537476 | test loss: 0.3297288119792938\n",
      "Epoch: 64 | loss: 0.38613998889923096 | test loss: 0.32968467473983765\n",
      "Epoch: 64 | loss: 0.43277791142463684 | test loss: 0.32970014214515686\n",
      "Epoch: 64 | loss: 0.34397372603416443 | test loss: 0.32964932918548584\n",
      "Epoch: 64 | loss: 0.2603844404220581 | test loss: 0.3296350836753845\n",
      "Epoch: 64 | loss: 0.3577941060066223 | test loss: 0.3296031653881073\n",
      "Epoch: 64 | loss: 0.23791153728961945 | test loss: 0.3294374346733093\n",
      "Epoch: 64 | loss: 0.27826187014579773 | test loss: 0.32935306429862976\n",
      "Epoch: 64 | loss: 0.22500485181808472 | test loss: 0.3293191194534302\n",
      "Epoch: 64 | loss: 0.20161281526088715 | test loss: 0.32929903268814087\n",
      "Epoch: 64 | loss: 0.3238151967525482 | test loss: 0.329255074262619\n",
      "Epoch: 64 | loss: 0.26976028084754944 | test loss: 0.3291560411453247\n",
      "Epoch: 64 | loss: 0.39037588238716125 | test loss: 0.32902586460113525\n",
      "Epoch: 65 | loss: 0.2770913243293762 | test loss: 0.25558024644851685\n",
      "Epoch: 65 | loss: 0.24651703238487244 | test loss: 0.2555064857006073\n",
      "Epoch: 65 | loss: 0.12038394808769226 | test loss: 0.2554452121257782\n",
      "Epoch: 65 | loss: 0.5852500796318054 | test loss: 0.25570520758628845\n",
      "Epoch: 65 | loss: 0.2803001403808594 | test loss: 0.25555649399757385\n",
      "Epoch: 65 | loss: 0.3146663010120392 | test loss: 0.25535154342651367\n",
      "Epoch: 65 | loss: 0.2146417796611786 | test loss: 0.2554740607738495\n",
      "Epoch: 65 | loss: 0.3923361003398895 | test loss: 0.2553258538246155\n",
      "Epoch: 65 | loss: 0.28906872868537903 | test loss: 0.2552112638950348\n",
      "Epoch: 65 | loss: 0.4276180863380432 | test loss: 0.25496822595596313\n",
      "Epoch: 65 | loss: 0.32110846042633057 | test loss: 0.25493893027305603\n",
      "Epoch: 65 | loss: 0.4919320046901703 | test loss: 0.25488603115081787\n",
      "Epoch: 65 | loss: 0.4695713222026825 | test loss: 0.25513511896133423\n",
      "Epoch: 65 | loss: 0.21046927571296692 | test loss: 0.2551232576370239\n",
      "Epoch: 65 | loss: 0.34148797392845154 | test loss: 0.254961758852005\n",
      "Epoch: 65 | loss: 0.32351693511009216 | test loss: 0.25472500920295715\n",
      "Epoch: 65 | loss: 0.3169754147529602 | test loss: 0.2546634376049042\n",
      "Epoch: 65 | loss: 0.1286536604166031 | test loss: 0.2545848786830902\n",
      "Epoch: 65 | loss: 0.0785854309797287 | test loss: 0.2545261085033417\n",
      "Epoch: 65 | loss: 0.11111743003129959 | test loss: 0.25440895557403564\n",
      "Epoch: 65 | loss: 0.1615424007177353 | test loss: 0.25435560941696167\n",
      "Epoch: 65 | loss: 0.2336934506893158 | test loss: 0.25413694977760315\n",
      "Epoch: 65 | loss: 0.35653114318847656 | test loss: 0.2540808916091919\n",
      "Epoch: 65 | loss: 0.47820138931274414 | test loss: 0.25407326221466064\n",
      "Epoch: 65 | loss: 0.304913729429245 | test loss: 0.2540552318096161\n",
      "Epoch: 65 | loss: 0.38054829835891724 | test loss: 0.2539771497249603\n",
      "Epoch: 65 | loss: 0.31090256571769714 | test loss: 0.25388267636299133\n",
      "Epoch: 65 | loss: 0.3772232234477997 | test loss: 0.2538963556289673\n",
      "Epoch: 65 | loss: 0.24984590709209442 | test loss: 0.2537994086742401\n",
      "Epoch: 65 | loss: 0.34540852904319763 | test loss: 0.253799170255661\n",
      "Epoch: 65 | loss: 0.27817341685295105 | test loss: 0.25373056530952454\n",
      "Epoch: 65 | loss: 0.46067753434181213 | test loss: 0.2535581588745117\n",
      "Epoch: 65 | loss: 0.5066330432891846 | test loss: 0.25358885526657104\n",
      "Epoch: 66 | loss: 0.3365488350391388 | test loss: 0.2621949017047882\n",
      "Epoch: 66 | loss: 0.1359359472990036 | test loss: 0.2627786695957184\n",
      "Epoch: 66 | loss: 0.3267921507358551 | test loss: 0.26219096779823303\n",
      "Epoch: 66 | loss: 0.36409372091293335 | test loss: 0.2609989643096924\n",
      "Epoch: 66 | loss: 0.3405406177043915 | test loss: 0.2616768181324005\n",
      "Epoch: 66 | loss: 0.32498714327812195 | test loss: 0.260327011346817\n",
      "Epoch: 66 | loss: 0.14624875783920288 | test loss: 0.26016247272491455\n",
      "Epoch: 66 | loss: 0.2746431231498718 | test loss: 0.2592727243900299\n",
      "Epoch: 66 | loss: 0.24474631249904633 | test loss: 0.26027360558509827\n",
      "Epoch: 66 | loss: 0.3111400008201599 | test loss: 0.26006200909614563\n",
      "Epoch: 66 | loss: 0.41288474202156067 | test loss: 0.2605520486831665\n",
      "Epoch: 66 | loss: 0.33365774154663086 | test loss: 0.2602400779724121\n",
      "Epoch: 66 | loss: 0.14407846331596375 | test loss: 0.26015704870224\n",
      "Epoch: 66 | loss: 0.43047916889190674 | test loss: 0.25926658511161804\n",
      "Epoch: 66 | loss: 0.40276816487312317 | test loss: 0.26046013832092285\n",
      "Epoch: 66 | loss: 0.3250160813331604 | test loss: 0.25993379950523376\n",
      "Epoch: 66 | loss: 0.3780039846897125 | test loss: 0.2595365047454834\n",
      "Epoch: 66 | loss: 0.264007031917572 | test loss: 0.2599126696586609\n",
      "Epoch: 66 | loss: 0.40144646167755127 | test loss: 0.25833237171173096\n",
      "Epoch: 66 | loss: 0.24905677139759064 | test loss: 0.2578578591346741\n",
      "Epoch: 66 | loss: 0.22029079496860504 | test loss: 0.2577921748161316\n",
      "Epoch: 66 | loss: 0.1357869654893875 | test loss: 0.2580040395259857\n",
      "Epoch: 66 | loss: 0.1258114129304886 | test loss: 0.2585880756378174\n",
      "Epoch: 66 | loss: 0.675027072429657 | test loss: 0.2579822540283203\n",
      "Epoch: 66 | loss: 0.19952236115932465 | test loss: 0.2575089633464813\n",
      "Epoch: 66 | loss: 0.37856432795524597 | test loss: 0.2590410113334656\n",
      "Epoch: 66 | loss: 0.5062817335128784 | test loss: 0.25867438316345215\n",
      "Epoch: 66 | loss: 0.1964629739522934 | test loss: 0.25842955708503723\n",
      "Epoch: 66 | loss: 0.1903824806213379 | test loss: 0.25949206948280334\n",
      "Epoch: 66 | loss: 0.28029730916023254 | test loss: 0.2592536211013794\n",
      "Epoch: 66 | loss: 0.6057053804397583 | test loss: 0.26069310307502747\n",
      "Epoch: 66 | loss: 0.37105926871299744 | test loss: 0.2621955871582031\n",
      "Epoch: 66 | loss: 0.4038366973400116 | test loss: 0.26186317205429077\n",
      "Epoch: 67 | loss: 0.2880456745624542 | test loss: 0.32746511697769165\n",
      "Epoch: 67 | loss: 0.4232184588909149 | test loss: 0.32760387659072876\n",
      "Epoch: 67 | loss: 0.38055503368377686 | test loss: 0.3276011347770691\n",
      "Epoch: 67 | loss: 0.22399339079856873 | test loss: 0.3275572955608368\n",
      "Epoch: 67 | loss: 0.24053677916526794 | test loss: 0.3275374472141266\n",
      "Epoch: 67 | loss: 0.3724193572998047 | test loss: 0.32746219635009766\n",
      "Epoch: 67 | loss: 0.2912822365760803 | test loss: 0.32751286029815674\n",
      "Epoch: 67 | loss: 0.26478248834609985 | test loss: 0.3276917636394501\n",
      "Epoch: 67 | loss: 0.3538984954357147 | test loss: 0.3275618553161621\n",
      "Epoch: 67 | loss: 0.23273560404777527 | test loss: 0.32759806513786316\n",
      "Epoch: 67 | loss: 0.21551990509033203 | test loss: 0.3273715674877167\n",
      "Epoch: 67 | loss: 0.29832470417022705 | test loss: 0.32695943117141724\n",
      "Epoch: 67 | loss: 0.16202133893966675 | test loss: 0.32683682441711426\n",
      "Epoch: 67 | loss: 0.2948407828807831 | test loss: 0.3266567587852478\n",
      "Epoch: 67 | loss: 0.2738536596298218 | test loss: 0.32665759325027466\n",
      "Epoch: 67 | loss: 0.28258374333381653 | test loss: 0.3264651596546173\n",
      "Epoch: 67 | loss: 0.22351589798927307 | test loss: 0.32633060216903687\n",
      "Epoch: 67 | loss: 0.5470896363258362 | test loss: 0.3262588381767273\n",
      "Epoch: 67 | loss: 0.37711480259895325 | test loss: 0.3261989653110504\n",
      "Epoch: 67 | loss: 0.3098122477531433 | test loss: 0.3262474536895752\n",
      "Epoch: 67 | loss: 0.4905046820640564 | test loss: 0.32639238238334656\n",
      "Epoch: 67 | loss: 0.16445399820804596 | test loss: 0.32632631063461304\n",
      "Epoch: 67 | loss: 0.309916228055954 | test loss: 0.3261847198009491\n",
      "Epoch: 67 | loss: 0.4091755747795105 | test loss: 0.3262694180011749\n",
      "Epoch: 67 | loss: 0.5842878222465515 | test loss: 0.3264016807079315\n",
      "Epoch: 67 | loss: 0.1886945515871048 | test loss: 0.3261364698410034\n",
      "Epoch: 67 | loss: 0.40533432364463806 | test loss: 0.3261604309082031\n",
      "Epoch: 67 | loss: 0.2816682457923889 | test loss: 0.3261042535305023\n",
      "Epoch: 67 | loss: 0.22642458975315094 | test loss: 0.3260824680328369\n",
      "Epoch: 67 | loss: 0.2749207615852356 | test loss: 0.32614386081695557\n",
      "Epoch: 67 | loss: 0.28252747654914856 | test loss: 0.3261926472187042\n",
      "Epoch: 67 | loss: 0.24216175079345703 | test loss: 0.3261665999889374\n",
      "Epoch: 67 | loss: 0.27113091945648193 | test loss: 0.3260161280632019\n",
      "Epoch: 68 | loss: 0.46661338210105896 | test loss: 0.2967572808265686\n",
      "Epoch: 68 | loss: 0.3372837007045746 | test loss: 0.29669469594955444\n",
      "Epoch: 68 | loss: 0.5231373906135559 | test loss: 0.29663437604904175\n",
      "Epoch: 68 | loss: 0.516987681388855 | test loss: 0.2966172695159912\n",
      "Epoch: 68 | loss: 0.19467894732952118 | test loss: 0.2966340482234955\n",
      "Epoch: 68 | loss: 0.31321728229522705 | test loss: 0.29650118947029114\n",
      "Epoch: 68 | loss: 0.17040769755840302 | test loss: 0.2965419888496399\n",
      "Epoch: 68 | loss: 0.403251588344574 | test loss: 0.29628074169158936\n",
      "Epoch: 68 | loss: 0.3787754774093628 | test loss: 0.2961462438106537\n",
      "Epoch: 68 | loss: 0.19195736944675446 | test loss: 0.2960285544395447\n",
      "Epoch: 68 | loss: 0.30793496966362 | test loss: 0.2960297465324402\n",
      "Epoch: 68 | loss: 0.3693161904811859 | test loss: 0.2960977852344513\n",
      "Epoch: 68 | loss: 0.26252081990242004 | test loss: 0.2959577143192291\n",
      "Epoch: 68 | loss: 0.21966443955898285 | test loss: 0.29582563042640686\n",
      "Epoch: 68 | loss: 0.17544081807136536 | test loss: 0.2958170473575592\n",
      "Epoch: 68 | loss: 0.1391461342573166 | test loss: 0.2958664000034332\n",
      "Epoch: 68 | loss: 0.237422913312912 | test loss: 0.29572615027427673\n",
      "Epoch: 68 | loss: 0.174141988158226 | test loss: 0.2955986261367798\n",
      "Epoch: 68 | loss: 0.3232971131801605 | test loss: 0.29557129740715027\n",
      "Epoch: 68 | loss: 0.4452536404132843 | test loss: 0.29551056027412415\n",
      "Epoch: 68 | loss: 0.30226707458496094 | test loss: 0.2953481376171112\n",
      "Epoch: 68 | loss: 0.27316778898239136 | test loss: 0.2953198552131653\n",
      "Epoch: 68 | loss: 0.40385761857032776 | test loss: 0.2953016757965088\n",
      "Epoch: 68 | loss: 0.5464431047439575 | test loss: 0.2952895164489746\n",
      "Epoch: 68 | loss: 0.20759524405002594 | test loss: 0.29516923427581787\n",
      "Epoch: 68 | loss: 0.3006291389465332 | test loss: 0.29506397247314453\n",
      "Epoch: 68 | loss: 0.2775025963783264 | test loss: 0.2950412631034851\n",
      "Epoch: 68 | loss: 0.35381031036376953 | test loss: 0.2949919104576111\n",
      "Epoch: 68 | loss: 0.16811473667621613 | test loss: 0.29495948553085327\n",
      "Epoch: 68 | loss: 0.24755659699440002 | test loss: 0.2949366569519043\n",
      "Epoch: 68 | loss: 0.36932137608528137 | test loss: 0.29490306973457336\n",
      "Epoch: 68 | loss: 0.3806297481060028 | test loss: 0.294890820980072\n",
      "Epoch: 68 | loss: 0.22173888981342316 | test loss: 0.2948470711708069\n",
      "Epoch: 69 | loss: 0.36427247524261475 | test loss: 0.32851260900497437\n",
      "Epoch: 69 | loss: 0.19678239524364471 | test loss: 0.32847169041633606\n",
      "Epoch: 69 | loss: 0.4314786493778229 | test loss: 0.3279859721660614\n",
      "Epoch: 69 | loss: 0.26673781871795654 | test loss: 0.32789433002471924\n",
      "Epoch: 69 | loss: 0.19198577105998993 | test loss: 0.3277219235897064\n",
      "Epoch: 69 | loss: 0.3158966898918152 | test loss: 0.3276061713695526\n",
      "Epoch: 69 | loss: 0.47281795740127563 | test loss: 0.32770389318466187\n",
      "Epoch: 69 | loss: 0.2230236679315567 | test loss: 0.3276972770690918\n",
      "Epoch: 69 | loss: 0.28207114338874817 | test loss: 0.3277440369129181\n",
      "Epoch: 69 | loss: 0.27884340286254883 | test loss: 0.3274781405925751\n",
      "Epoch: 69 | loss: 0.40068843960762024 | test loss: 0.32724839448928833\n",
      "Epoch: 69 | loss: 0.280112087726593 | test loss: 0.3273908495903015\n",
      "Epoch: 69 | loss: 0.2790650427341461 | test loss: 0.32749465107917786\n",
      "Epoch: 69 | loss: 0.43625229597091675 | test loss: 0.32722997665405273\n",
      "Epoch: 69 | loss: 0.30459004640579224 | test loss: 0.3273218274116516\n",
      "Epoch: 69 | loss: 0.48435094952583313 | test loss: 0.32780057191848755\n",
      "Epoch: 69 | loss: 0.12248650938272476 | test loss: 0.3277130424976349\n",
      "Epoch: 69 | loss: 0.2512272894382477 | test loss: 0.3278808295726776\n",
      "Epoch: 69 | loss: 0.39037832617759705 | test loss: 0.3276613652706146\n",
      "Epoch: 69 | loss: 0.36356407403945923 | test loss: 0.327808678150177\n",
      "Epoch: 69 | loss: 0.2622319161891937 | test loss: 0.3274691700935364\n",
      "Epoch: 69 | loss: 0.34468623995780945 | test loss: 0.32776573300361633\n",
      "Epoch: 69 | loss: 0.2417932152748108 | test loss: 0.32808154821395874\n",
      "Epoch: 69 | loss: 0.15686091780662537 | test loss: 0.327878475189209\n",
      "Epoch: 69 | loss: 0.2481316477060318 | test loss: 0.32753244042396545\n",
      "Epoch: 69 | loss: 0.4307873547077179 | test loss: 0.3273620903491974\n",
      "Epoch: 69 | loss: 0.4581557512283325 | test loss: 0.32713401317596436\n",
      "Epoch: 69 | loss: 0.23111577332019806 | test loss: 0.326962947845459\n",
      "Epoch: 69 | loss: 0.18790915608406067 | test loss: 0.3268561065196991\n",
      "Epoch: 69 | loss: 0.28391319513320923 | test loss: 0.3267149329185486\n",
      "Epoch: 69 | loss: 0.3380437195301056 | test loss: 0.3267292082309723\n",
      "Epoch: 69 | loss: 0.360506147146225 | test loss: 0.3269253969192505\n",
      "Epoch: 69 | loss: 0.35102397203445435 | test loss: 0.3269728720188141\n",
      "Epoch: 70 | loss: 0.4073971211910248 | test loss: 0.2590577006340027\n",
      "Epoch: 70 | loss: 0.18823537230491638 | test loss: 0.2589079439640045\n",
      "Epoch: 70 | loss: 0.2396077811717987 | test loss: 0.2589104473590851\n",
      "Epoch: 70 | loss: 0.42168667912483215 | test loss: 0.25890037417411804\n",
      "Epoch: 70 | loss: 0.10006897151470184 | test loss: 0.25888484716415405\n",
      "Epoch: 70 | loss: 0.3799602687358856 | test loss: 0.25893718004226685\n",
      "Epoch: 70 | loss: 0.15555119514465332 | test loss: 0.25891736149787903\n",
      "Epoch: 70 | loss: 0.24823042750358582 | test loss: 0.25876545906066895\n",
      "Epoch: 70 | loss: 0.29418620467185974 | test loss: 0.25890374183654785\n",
      "Epoch: 70 | loss: 0.19648219645023346 | test loss: 0.2588851749897003\n",
      "Epoch: 70 | loss: 0.21502096951007843 | test loss: 0.25894442200660706\n",
      "Epoch: 70 | loss: 0.3234016001224518 | test loss: 0.2589002251625061\n",
      "Epoch: 70 | loss: 0.2669174373149872 | test loss: 0.25883668661117554\n",
      "Epoch: 70 | loss: 0.2759056091308594 | test loss: 0.2589915692806244\n",
      "Epoch: 70 | loss: 0.47804397344589233 | test loss: 0.2591002881526947\n",
      "Epoch: 70 | loss: 0.5552711486816406 | test loss: 0.2588061988353729\n",
      "Epoch: 70 | loss: 0.20839323103427887 | test loss: 0.25868111848831177\n",
      "Epoch: 70 | loss: 0.3211265802383423 | test loss: 0.2584935426712036\n",
      "Epoch: 70 | loss: 0.34668388962745667 | test loss: 0.25848206877708435\n",
      "Epoch: 70 | loss: 0.25456950068473816 | test loss: 0.25867101550102234\n",
      "Epoch: 70 | loss: 0.4452783167362213 | test loss: 0.2584210932254791\n",
      "Epoch: 70 | loss: 0.21367748081684113 | test loss: 0.25829017162323\n",
      "Epoch: 70 | loss: 0.1365862637758255 | test loss: 0.25814011693000793\n",
      "Epoch: 70 | loss: 0.24827159941196442 | test loss: 0.2580053508281708\n",
      "Epoch: 70 | loss: 0.2864221930503845 | test loss: 0.2578154504299164\n",
      "Epoch: 70 | loss: 0.5317580699920654 | test loss: 0.25770726799964905\n",
      "Epoch: 70 | loss: 0.432699978351593 | test loss: 0.2575843632221222\n",
      "Epoch: 70 | loss: 0.3986016511917114 | test loss: 0.2575911283493042\n",
      "Epoch: 70 | loss: 0.29460829496383667 | test loss: 0.25759872794151306\n",
      "Epoch: 70 | loss: 0.11870729923248291 | test loss: 0.25747939944267273\n",
      "Epoch: 70 | loss: 0.3345150053501129 | test loss: 0.2574860453605652\n",
      "Epoch: 70 | loss: 0.49135521054267883 | test loss: 0.2574523091316223\n",
      "Epoch: 70 | loss: 0.30622681975364685 | test loss: 0.2573186755180359\n",
      "Epoch: 71 | loss: 0.31268057227134705 | test loss: 0.26456040143966675\n",
      "Epoch: 71 | loss: 0.34075215458869934 | test loss: 0.2645640969276428\n",
      "Epoch: 71 | loss: 0.34535324573516846 | test loss: 0.2645191550254822\n",
      "Epoch: 71 | loss: 0.14037340879440308 | test loss: 0.264395534992218\n",
      "Epoch: 71 | loss: 0.30861231684684753 | test loss: 0.2644839286804199\n",
      "Epoch: 71 | loss: 0.38522669672966003 | test loss: 0.2644851505756378\n",
      "Epoch: 71 | loss: 0.31779083609580994 | test loss: 0.26437392830848694\n",
      "Epoch: 71 | loss: 0.25363263487815857 | test loss: 0.2642796039581299\n",
      "Epoch: 71 | loss: 0.43133288621902466 | test loss: 0.2642289102077484\n",
      "Epoch: 71 | loss: 0.23973847925662994 | test loss: 0.2642326056957245\n",
      "Epoch: 71 | loss: 0.12819026410579681 | test loss: 0.26413533091545105\n",
      "Epoch: 71 | loss: 0.12982097268104553 | test loss: 0.2640511393547058\n",
      "Epoch: 71 | loss: 0.17765095829963684 | test loss: 0.2639616131782532\n",
      "Epoch: 71 | loss: 0.2200549989938736 | test loss: 0.2638896107673645\n",
      "Epoch: 71 | loss: 0.13571861386299133 | test loss: 0.2638327181339264\n",
      "Epoch: 71 | loss: 0.34189602732658386 | test loss: 0.2638019025325775\n",
      "Epoch: 71 | loss: 0.1311509758234024 | test loss: 0.2636953890323639\n",
      "Epoch: 71 | loss: 0.3991985321044922 | test loss: 0.2636439800262451\n",
      "Epoch: 71 | loss: 0.1868889480829239 | test loss: 0.26364386081695557\n",
      "Epoch: 71 | loss: 0.40768370032310486 | test loss: 0.2636542320251465\n",
      "Epoch: 71 | loss: 0.2691417634487152 | test loss: 0.2635520398616791\n",
      "Epoch: 71 | loss: 0.41217055916786194 | test loss: 0.2634711265563965\n",
      "Epoch: 71 | loss: 0.2911987006664276 | test loss: 0.263399600982666\n",
      "Epoch: 71 | loss: 0.28062188625335693 | test loss: 0.2633509337902069\n",
      "Epoch: 71 | loss: 0.4038807451725006 | test loss: 0.26332730054855347\n",
      "Epoch: 71 | loss: 0.3490263521671295 | test loss: 0.2633161246776581\n",
      "Epoch: 71 | loss: 0.2402375340461731 | test loss: 0.26323267817497253\n",
      "Epoch: 71 | loss: 0.3756856918334961 | test loss: 0.263261079788208\n",
      "Epoch: 71 | loss: 0.27503177523612976 | test loss: 0.26328322291374207\n",
      "Epoch: 71 | loss: 0.13465666770935059 | test loss: 0.26324471831321716\n",
      "Epoch: 71 | loss: 0.35702064633369446 | test loss: 0.2632277011871338\n",
      "Epoch: 71 | loss: 0.301000714302063 | test loss: 0.2631816565990448\n",
      "Epoch: 71 | loss: 0.7303292155265808 | test loss: 0.26317399740219116\n",
      "Epoch: 72 | loss: 0.40658459067344666 | test loss: 0.28609499335289\n",
      "Epoch: 72 | loss: 0.2531684935092926 | test loss: 0.2858554422855377\n",
      "Epoch: 72 | loss: 0.32500946521759033 | test loss: 0.2859748899936676\n",
      "Epoch: 72 | loss: 0.37171968817710876 | test loss: 0.28589481115341187\n",
      "Epoch: 72 | loss: 0.26780569553375244 | test loss: 0.2858036756515503\n",
      "Epoch: 72 | loss: 0.449937641620636 | test loss: 0.2858789563179016\n",
      "Epoch: 72 | loss: 0.22353948652744293 | test loss: 0.2858140170574188\n",
      "Epoch: 72 | loss: 0.24545377492904663 | test loss: 0.2856757342815399\n",
      "Epoch: 72 | loss: 0.29142698645591736 | test loss: 0.28565508127212524\n",
      "Epoch: 72 | loss: 0.20021972060203552 | test loss: 0.285659521818161\n",
      "Epoch: 72 | loss: 0.25408273935317993 | test loss: 0.28541824221611023\n",
      "Epoch: 72 | loss: 0.3039645552635193 | test loss: 0.2853586673736572\n",
      "Epoch: 72 | loss: 0.2509693503379822 | test loss: 0.2853337526321411\n",
      "Epoch: 72 | loss: 0.23314489424228668 | test loss: 0.28521740436553955\n",
      "Epoch: 72 | loss: 0.36028680205345154 | test loss: 0.2852228581905365\n",
      "Epoch: 72 | loss: 0.34305694699287415 | test loss: 0.2851288914680481\n",
      "Epoch: 72 | loss: 0.4464109539985657 | test loss: 0.28522375226020813\n",
      "Epoch: 72 | loss: 0.34121784567832947 | test loss: 0.28504347801208496\n",
      "Epoch: 72 | loss: 0.2532455027103424 | test loss: 0.2850865423679352\n",
      "Epoch: 72 | loss: 0.32518088817596436 | test loss: 0.28510183095932007\n",
      "Epoch: 72 | loss: 0.4050050377845764 | test loss: 0.2847982347011566\n",
      "Epoch: 72 | loss: 0.16047759354114532 | test loss: 0.2845962345600128\n",
      "Epoch: 72 | loss: 0.4008151888847351 | test loss: 0.28462812304496765\n",
      "Epoch: 72 | loss: 0.24096368253231049 | test loss: 0.28456178307533264\n",
      "Epoch: 72 | loss: 0.3215685784816742 | test loss: 0.28477078676223755\n",
      "Epoch: 72 | loss: 0.10938822478055954 | test loss: 0.2846032977104187\n",
      "Epoch: 72 | loss: 0.2761863172054291 | test loss: 0.2845876216888428\n",
      "Epoch: 72 | loss: 0.192914679646492 | test loss: 0.28449809551239014\n",
      "Epoch: 72 | loss: 0.45429515838623047 | test loss: 0.2844974994659424\n",
      "Epoch: 72 | loss: 0.2651236355304718 | test loss: 0.28462910652160645\n",
      "Epoch: 72 | loss: 0.3117605149745941 | test loss: 0.28447338938713074\n",
      "Epoch: 72 | loss: 0.2784891724586487 | test loss: 0.2842097878456116\n",
      "Epoch: 72 | loss: 0.26794514060020447 | test loss: 0.2842230498790741\n",
      "Epoch: 73 | loss: 0.3034253418445587 | test loss: 0.252319872379303\n",
      "Epoch: 73 | loss: 0.31485193967819214 | test loss: 0.25236785411834717\n",
      "Epoch: 73 | loss: 0.26795998215675354 | test loss: 0.25236427783966064\n",
      "Epoch: 73 | loss: 0.3258711099624634 | test loss: 0.2521304190158844\n",
      "Epoch: 73 | loss: 0.2792414128780365 | test loss: 0.2521277070045471\n",
      "Epoch: 73 | loss: 0.3938750922679901 | test loss: 0.25198978185653687\n",
      "Epoch: 73 | loss: 0.20408885180950165 | test loss: 0.2519640326499939\n",
      "Epoch: 73 | loss: 0.3202202022075653 | test loss: 0.25160685181617737\n",
      "Epoch: 73 | loss: 0.37054890394210815 | test loss: 0.25159648060798645\n",
      "Epoch: 73 | loss: 0.2825569808483124 | test loss: 0.25141391158103943\n",
      "Epoch: 73 | loss: 0.2599671483039856 | test loss: 0.2514340579509735\n",
      "Epoch: 73 | loss: 0.470400869846344 | test loss: 0.2513630986213684\n",
      "Epoch: 73 | loss: 0.23089930415153503 | test loss: 0.25124603509902954\n",
      "Epoch: 73 | loss: 0.24643148481845856 | test loss: 0.25108715891838074\n",
      "Epoch: 73 | loss: 0.2733171880245209 | test loss: 0.2509790062904358\n",
      "Epoch: 73 | loss: 0.13664719462394714 | test loss: 0.25082793831825256\n",
      "Epoch: 73 | loss: 0.24226222932338715 | test loss: 0.25086456537246704\n",
      "Epoch: 73 | loss: 0.2645651698112488 | test loss: 0.25085896253585815\n",
      "Epoch: 73 | loss: 0.3497413396835327 | test loss: 0.25089049339294434\n",
      "Epoch: 73 | loss: 0.15427951514720917 | test loss: 0.2507999539375305\n",
      "Epoch: 73 | loss: 0.3380548059940338 | test loss: 0.25075122714042664\n",
      "Epoch: 73 | loss: 0.549260675907135 | test loss: 0.250927597284317\n",
      "Epoch: 73 | loss: 0.33819714188575745 | test loss: 0.2509118318557739\n",
      "Epoch: 73 | loss: 0.37727344036102295 | test loss: 0.2509128451347351\n",
      "Epoch: 73 | loss: 0.4055545926094055 | test loss: 0.25080397725105286\n",
      "Epoch: 73 | loss: 0.34302252531051636 | test loss: 0.25089794397354126\n",
      "Epoch: 73 | loss: 0.31942835450172424 | test loss: 0.25103235244750977\n",
      "Epoch: 73 | loss: 0.1775091141462326 | test loss: 0.2509289085865021\n",
      "Epoch: 73 | loss: 0.2568075954914093 | test loss: 0.2508479952812195\n",
      "Epoch: 73 | loss: 0.4284113347530365 | test loss: 0.25093576312065125\n",
      "Epoch: 73 | loss: 0.2621867060661316 | test loss: 0.2511056363582611\n",
      "Epoch: 73 | loss: 0.14006979763507843 | test loss: 0.2508902847766876\n",
      "Epoch: 73 | loss: 0.17457395792007446 | test loss: 0.25092118978500366\n",
      "Epoch: 74 | loss: 0.3709009289741516 | test loss: 0.21389473974704742\n",
      "Epoch: 74 | loss: 0.15075601637363434 | test loss: 0.21400955319404602\n",
      "Epoch: 74 | loss: 0.3735341429710388 | test loss: 0.2140909880399704\n",
      "Epoch: 74 | loss: 0.297483891248703 | test loss: 0.2148517668247223\n",
      "Epoch: 74 | loss: 0.18839673697948456 | test loss: 0.2152959406375885\n",
      "Epoch: 74 | loss: 0.196459099650383 | test loss: 0.21430423855781555\n",
      "Epoch: 74 | loss: 0.35173171758651733 | test loss: 0.21494744718074799\n",
      "Epoch: 74 | loss: 0.42293795943260193 | test loss: 0.214640274643898\n",
      "Epoch: 74 | loss: 0.2371675670146942 | test loss: 0.21394184231758118\n",
      "Epoch: 74 | loss: 0.5417439937591553 | test loss: 0.21460983157157898\n",
      "Epoch: 74 | loss: 0.4152759611606598 | test loss: 0.21439366042613983\n",
      "Epoch: 74 | loss: 0.23061776161193848 | test loss: 0.21440060436725616\n",
      "Epoch: 74 | loss: 0.11447873711585999 | test loss: 0.2136749029159546\n",
      "Epoch: 74 | loss: 0.25767987966537476 | test loss: 0.2135312408208847\n",
      "Epoch: 74 | loss: 0.231067955493927 | test loss: 0.21325717866420746\n",
      "Epoch: 74 | loss: 0.3281949460506439 | test loss: 0.21331150829792023\n",
      "Epoch: 74 | loss: 0.2273479551076889 | test loss: 0.21266022324562073\n",
      "Epoch: 74 | loss: 0.2772265374660492 | test loss: 0.21159298717975616\n",
      "Epoch: 74 | loss: 0.24558602273464203 | test loss: 0.211173415184021\n",
      "Epoch: 74 | loss: 0.3789866864681244 | test loss: 0.21181192994117737\n",
      "Epoch: 74 | loss: 0.21700330078601837 | test loss: 0.21127445995807648\n",
      "Epoch: 74 | loss: 0.20599274337291718 | test loss: 0.21106211841106415\n",
      "Epoch: 74 | loss: 0.13873834908008575 | test loss: 0.21094870567321777\n",
      "Epoch: 74 | loss: 0.2961444556713104 | test loss: 0.21104249358177185\n",
      "Epoch: 74 | loss: 0.3411843180656433 | test loss: 0.21031998097896576\n",
      "Epoch: 74 | loss: 0.283993661403656 | test loss: 0.20973823964595795\n",
      "Epoch: 74 | loss: 0.3968796730041504 | test loss: 0.20960776507854462\n",
      "Epoch: 74 | loss: 0.5117080211639404 | test loss: 0.21052928268909454\n",
      "Epoch: 74 | loss: 0.22635436058044434 | test loss: 0.2104302942752838\n",
      "Epoch: 74 | loss: 0.2722659707069397 | test loss: 0.21084269881248474\n",
      "Epoch: 74 | loss: 0.7027605772018433 | test loss: 0.21186529099941254\n",
      "Epoch: 74 | loss: 0.23940107226371765 | test loss: 0.2113294154405594\n",
      "Epoch: 74 | loss: 0.09343782812356949 | test loss: 0.21116717159748077\n",
      "Epoch: 75 | loss: 0.348293274641037 | test loss: 0.30877283215522766\n",
      "Epoch: 75 | loss: 0.4031471014022827 | test loss: 0.3094494640827179\n",
      "Epoch: 75 | loss: 0.21046821773052216 | test loss: 0.3094056248664856\n",
      "Epoch: 75 | loss: 0.1894211620092392 | test loss: 0.30940520763397217\n",
      "Epoch: 75 | loss: 0.42975756525993347 | test loss: 0.3087344169616699\n",
      "Epoch: 75 | loss: 0.33631497621536255 | test loss: 0.30850693583488464\n",
      "Epoch: 75 | loss: 0.38797464966773987 | test loss: 0.30882957577705383\n",
      "Epoch: 75 | loss: 0.2853049635887146 | test loss: 0.30869704484939575\n",
      "Epoch: 75 | loss: 0.13797727227210999 | test loss: 0.30826449394226074\n",
      "Epoch: 75 | loss: 0.46410343050956726 | test loss: 0.30820342898368835\n",
      "Epoch: 75 | loss: 0.2744326591491699 | test loss: 0.307967871427536\n",
      "Epoch: 75 | loss: 0.3534747362136841 | test loss: 0.30797675251960754\n",
      "Epoch: 75 | loss: 0.3005434274673462 | test loss: 0.30809611082077026\n",
      "Epoch: 75 | loss: 0.32959526777267456 | test loss: 0.307886004447937\n",
      "Epoch: 75 | loss: 0.16180796921253204 | test loss: 0.30791860818862915\n",
      "Epoch: 75 | loss: 0.22779105603694916 | test loss: 0.30809128284454346\n",
      "Epoch: 75 | loss: 0.3498102128505707 | test loss: 0.3082225024700165\n",
      "Epoch: 75 | loss: 0.1888314187526703 | test loss: 0.30787017941474915\n",
      "Epoch: 75 | loss: 0.3467153310775757 | test loss: 0.3081066608428955\n",
      "Epoch: 75 | loss: 0.2035248577594757 | test loss: 0.307677686214447\n",
      "Epoch: 75 | loss: 0.24707572162151337 | test loss: 0.3072293996810913\n",
      "Epoch: 75 | loss: 0.22913433611392975 | test loss: 0.3073125183582306\n",
      "Epoch: 75 | loss: 0.2469608336687088 | test loss: 0.30747371912002563\n",
      "Epoch: 75 | loss: 0.31154727935791016 | test loss: 0.30803006887435913\n",
      "Epoch: 75 | loss: 0.3233448266983032 | test loss: 0.3082597851753235\n",
      "Epoch: 75 | loss: 0.28726375102996826 | test loss: 0.30809730291366577\n",
      "Epoch: 75 | loss: 0.30175700783729553 | test loss: 0.30843108892440796\n",
      "Epoch: 75 | loss: 0.2783392071723938 | test loss: 0.3078312277793884\n",
      "Epoch: 75 | loss: 0.33765798807144165 | test loss: 0.30798983573913574\n",
      "Epoch: 75 | loss: 0.3079957365989685 | test loss: 0.30778926610946655\n",
      "Epoch: 75 | loss: 0.3108619153499603 | test loss: 0.30777591466903687\n",
      "Epoch: 75 | loss: 0.30887600779533386 | test loss: 0.30720576643943787\n",
      "Epoch: 75 | loss: 0.4271215796470642 | test loss: 0.3077216148376465\n",
      "Epoch: 76 | loss: 0.25083962082862854 | test loss: 0.2759612202644348\n",
      "Epoch: 76 | loss: 0.27706557512283325 | test loss: 0.27584338188171387\n",
      "Epoch: 76 | loss: 0.12644173204898834 | test loss: 0.2752567529678345\n",
      "Epoch: 76 | loss: 0.3408200442790985 | test loss: 0.27615585923194885\n",
      "Epoch: 76 | loss: 0.2464723289012909 | test loss: 0.2764655351638794\n",
      "Epoch: 76 | loss: 0.535314679145813 | test loss: 0.27597472071647644\n",
      "Epoch: 76 | loss: 0.29341748356819153 | test loss: 0.2755953371524811\n",
      "Epoch: 76 | loss: 0.1556490957736969 | test loss: 0.27549946308135986\n",
      "Epoch: 76 | loss: 0.20457805693149567 | test loss: 0.27537113428115845\n",
      "Epoch: 76 | loss: 0.32765331864356995 | test loss: 0.27460339665412903\n",
      "Epoch: 76 | loss: 0.167873352766037 | test loss: 0.2745504081249237\n",
      "Epoch: 76 | loss: 0.594430148601532 | test loss: 0.27522608637809753\n",
      "Epoch: 76 | loss: 0.3127622604370117 | test loss: 0.2753414511680603\n",
      "Epoch: 76 | loss: 0.3539380431175232 | test loss: 0.27509304881095886\n",
      "Epoch: 76 | loss: 0.20242153108119965 | test loss: 0.2754591703414917\n",
      "Epoch: 76 | loss: 0.307560533285141 | test loss: 0.27478283643722534\n",
      "Epoch: 76 | loss: 0.48377546668052673 | test loss: 0.2731710374355316\n",
      "Epoch: 76 | loss: 0.27901896834373474 | test loss: 0.2736879289150238\n",
      "Epoch: 76 | loss: 0.3200579583644867 | test loss: 0.27413102984428406\n",
      "Epoch: 76 | loss: 0.2697027325630188 | test loss: 0.2739734351634979\n",
      "Epoch: 76 | loss: 0.36261579394340515 | test loss: 0.2745051085948944\n",
      "Epoch: 76 | loss: 0.36639299988746643 | test loss: 0.27511367201805115\n",
      "Epoch: 76 | loss: 0.17559127509593964 | test loss: 0.2748847007751465\n",
      "Epoch: 76 | loss: 0.17285579442977905 | test loss: 0.2740165591239929\n",
      "Epoch: 76 | loss: 0.40016475319862366 | test loss: 0.27427560091018677\n",
      "Epoch: 76 | loss: 0.43986427783966064 | test loss: 0.27470195293426514\n",
      "Epoch: 76 | loss: 0.21982212364673615 | test loss: 0.2744668424129486\n",
      "Epoch: 76 | loss: 0.16388429701328278 | test loss: 0.2740902304649353\n",
      "Epoch: 76 | loss: 0.48417967557907104 | test loss: 0.27432966232299805\n",
      "Epoch: 76 | loss: 0.28587740659713745 | test loss: 0.2751171588897705\n",
      "Epoch: 76 | loss: 0.2186499983072281 | test loss: 0.2747325003147125\n",
      "Epoch: 76 | loss: 0.3111485540866852 | test loss: 0.2756776511669159\n",
      "Epoch: 76 | loss: 0.15732304751873016 | test loss: 0.27558809518814087\n",
      "Epoch: 77 | loss: 0.1614299714565277 | test loss: 0.2771811783313751\n",
      "Epoch: 77 | loss: 0.1961662918329239 | test loss: 0.27698132395744324\n",
      "Epoch: 77 | loss: 0.18610164523124695 | test loss: 0.2768356204032898\n",
      "Epoch: 77 | loss: 0.466240257024765 | test loss: 0.2770407795906067\n",
      "Epoch: 77 | loss: 0.22453972697257996 | test loss: 0.27697473764419556\n",
      "Epoch: 77 | loss: 0.42311960458755493 | test loss: 0.2767976224422455\n",
      "Epoch: 77 | loss: 0.20138303935527802 | test loss: 0.27675822377204895\n",
      "Epoch: 77 | loss: 0.20052622258663177 | test loss: 0.2766420841217041\n",
      "Epoch: 77 | loss: 0.145953968167305 | test loss: 0.27658525109291077\n",
      "Epoch: 77 | loss: 0.43510812520980835 | test loss: 0.2766333818435669\n",
      "Epoch: 77 | loss: 0.3707425892353058 | test loss: 0.2766048014163971\n",
      "Epoch: 77 | loss: 0.40162089467048645 | test loss: 0.2768758237361908\n",
      "Epoch: 77 | loss: 0.22781695425510406 | test loss: 0.27691447734832764\n",
      "Epoch: 77 | loss: 0.20748670399188995 | test loss: 0.27680858969688416\n",
      "Epoch: 77 | loss: 0.16626892983913422 | test loss: 0.27691102027893066\n",
      "Epoch: 77 | loss: 0.43700477480888367 | test loss: 0.2770460247993469\n",
      "Epoch: 77 | loss: 0.5157142281532288 | test loss: 0.27721020579338074\n",
      "Epoch: 77 | loss: 0.4373170733451843 | test loss: 0.2773347496986389\n",
      "Epoch: 77 | loss: 0.3690858483314514 | test loss: 0.27754443883895874\n",
      "Epoch: 77 | loss: 0.10201109200716019 | test loss: 0.2776073217391968\n",
      "Epoch: 77 | loss: 0.2310379594564438 | test loss: 0.2769002318382263\n",
      "Epoch: 77 | loss: 0.2753434479236603 | test loss: 0.2768297493457794\n",
      "Epoch: 77 | loss: 0.20235757529735565 | test loss: 0.2766329348087311\n",
      "Epoch: 77 | loss: 0.14978228509426117 | test loss: 0.2765892446041107\n",
      "Epoch: 77 | loss: 0.29982390999794006 | test loss: 0.2765701115131378\n",
      "Epoch: 77 | loss: 0.3097369968891144 | test loss: 0.27662694454193115\n",
      "Epoch: 77 | loss: 0.20381760597229004 | test loss: 0.2763044536113739\n",
      "Epoch: 77 | loss: 0.42336660623550415 | test loss: 0.27573758363723755\n",
      "Epoch: 77 | loss: 0.17199823260307312 | test loss: 0.2758886516094208\n",
      "Epoch: 77 | loss: 0.3163507282733917 | test loss: 0.2760528028011322\n",
      "Epoch: 77 | loss: 0.29188933968544006 | test loss: 0.2760396897792816\n",
      "Epoch: 77 | loss: 0.40765073895454407 | test loss: 0.275480717420578\n",
      "Epoch: 77 | loss: 0.35871976613998413 | test loss: 0.275208055973053\n",
      "Epoch: 78 | loss: 0.38896694779396057 | test loss: 0.21961970627307892\n",
      "Epoch: 78 | loss: 0.46824657917022705 | test loss: 0.21998441219329834\n",
      "Epoch: 78 | loss: 0.29616355895996094 | test loss: 0.21987976133823395\n",
      "Epoch: 78 | loss: 0.21854491531848907 | test loss: 0.21978409588336945\n",
      "Epoch: 78 | loss: 0.15690717101097107 | test loss: 0.21973171830177307\n",
      "Epoch: 78 | loss: 0.4278901517391205 | test loss: 0.21960067749023438\n",
      "Epoch: 78 | loss: 0.1450047791004181 | test loss: 0.21958445012569427\n",
      "Epoch: 78 | loss: 0.30783167481422424 | test loss: 0.21948055922985077\n",
      "Epoch: 78 | loss: 0.32662275433540344 | test loss: 0.2196681946516037\n",
      "Epoch: 78 | loss: 0.3593820631504059 | test loss: 0.21958079934120178\n",
      "Epoch: 78 | loss: 0.38994157314300537 | test loss: 0.2197926789522171\n",
      "Epoch: 78 | loss: 0.4312289357185364 | test loss: 0.2194744050502777\n",
      "Epoch: 78 | loss: 0.258266806602478 | test loss: 0.21949557960033417\n",
      "Epoch: 78 | loss: 0.42808398604393005 | test loss: 0.21946215629577637\n",
      "Epoch: 78 | loss: 0.3785973787307739 | test loss: 0.2194412499666214\n",
      "Epoch: 78 | loss: 0.1923971325159073 | test loss: 0.21944528818130493\n",
      "Epoch: 78 | loss: 0.3223792314529419 | test loss: 0.21935433149337769\n",
      "Epoch: 78 | loss: 0.16064141690731049 | test loss: 0.21922306716442108\n",
      "Epoch: 78 | loss: 0.160021111369133 | test loss: 0.21914592385292053\n",
      "Epoch: 78 | loss: 0.2304057478904724 | test loss: 0.21902410686016083\n",
      "Epoch: 78 | loss: 0.49785956740379333 | test loss: 0.2191055715084076\n",
      "Epoch: 78 | loss: 0.26375728845596313 | test loss: 0.21890243887901306\n",
      "Epoch: 78 | loss: 0.19059047102928162 | test loss: 0.21888919174671173\n",
      "Epoch: 78 | loss: 0.29850488901138306 | test loss: 0.21875430643558502\n",
      "Epoch: 78 | loss: 0.18301311135292053 | test loss: 0.21866410970687866\n",
      "Epoch: 78 | loss: 0.29620295763015747 | test loss: 0.21859343349933624\n",
      "Epoch: 78 | loss: 0.1437634527683258 | test loss: 0.21851609647274017\n",
      "Epoch: 78 | loss: 0.3888779878616333 | test loss: 0.21846069395542145\n",
      "Epoch: 78 | loss: 0.14456631243228912 | test loss: 0.2183653563261032\n",
      "Epoch: 78 | loss: 0.20477445423603058 | test loss: 0.21830809116363525\n",
      "Epoch: 78 | loss: 0.4176187515258789 | test loss: 0.21835456788539886\n",
      "Epoch: 78 | loss: 0.18296939134597778 | test loss: 0.218251034617424\n",
      "Epoch: 78 | loss: 0.31167981028556824 | test loss: 0.21822930872440338\n",
      "Epoch: 79 | loss: 0.15471330285072327 | test loss: 0.24448920786380768\n",
      "Epoch: 79 | loss: 0.212759330868721 | test loss: 0.24443520605564117\n",
      "Epoch: 79 | loss: 0.19457022845745087 | test loss: 0.24426135420799255\n",
      "Epoch: 79 | loss: 0.12853258848190308 | test loss: 0.2443174421787262\n",
      "Epoch: 79 | loss: 0.20695899426937103 | test loss: 0.24411362409591675\n",
      "Epoch: 79 | loss: 0.2636939287185669 | test loss: 0.2440117746591568\n",
      "Epoch: 79 | loss: 0.31774887442588806 | test loss: 0.24422425031661987\n",
      "Epoch: 79 | loss: 0.27252689003944397 | test loss: 0.2439570128917694\n",
      "Epoch: 79 | loss: 0.2647615969181061 | test loss: 0.2437915802001953\n",
      "Epoch: 79 | loss: 0.42661353945732117 | test loss: 0.2437802404165268\n",
      "Epoch: 79 | loss: 0.17232118546962738 | test loss: 0.24367906153202057\n",
      "Epoch: 79 | loss: 0.4383070170879364 | test loss: 0.24370087683200836\n",
      "Epoch: 79 | loss: 0.5015340447425842 | test loss: 0.24380382895469666\n",
      "Epoch: 79 | loss: 0.3919937312602997 | test loss: 0.24374902248382568\n",
      "Epoch: 79 | loss: 0.2047962099313736 | test loss: 0.2435891032218933\n",
      "Epoch: 79 | loss: 0.20194269716739655 | test loss: 0.24359048902988434\n",
      "Epoch: 79 | loss: 0.2626003921031952 | test loss: 0.24359342455863953\n",
      "Epoch: 79 | loss: 0.215037003159523 | test loss: 0.24337567389011383\n",
      "Epoch: 79 | loss: 0.26472318172454834 | test loss: 0.2432958334684372\n",
      "Epoch: 79 | loss: 0.2185637503862381 | test loss: 0.24328534305095673\n",
      "Epoch: 79 | loss: 0.3585725724697113 | test loss: 0.24340343475341797\n",
      "Epoch: 79 | loss: 0.4409131705760956 | test loss: 0.24328970909118652\n",
      "Epoch: 79 | loss: 0.22421008348464966 | test loss: 0.24345901608467102\n",
      "Epoch: 79 | loss: 0.33717092871665955 | test loss: 0.24350623786449432\n",
      "Epoch: 79 | loss: 0.38026490807533264 | test loss: 0.24340809881687164\n",
      "Epoch: 79 | loss: 0.2354438304901123 | test loss: 0.24342049658298492\n",
      "Epoch: 79 | loss: 0.11970975250005722 | test loss: 0.24323923885822296\n",
      "Epoch: 79 | loss: 0.38418594002723694 | test loss: 0.24322380125522614\n",
      "Epoch: 79 | loss: 0.29843366146087646 | test loss: 0.2432689666748047\n",
      "Epoch: 79 | loss: 0.3650847375392914 | test loss: 0.24338296055793762\n",
      "Epoch: 79 | loss: 0.1900235265493393 | test loss: 0.2433909773826599\n",
      "Epoch: 79 | loss: 0.350826621055603 | test loss: 0.24337302148342133\n",
      "Epoch: 79 | loss: 0.2920055389404297 | test loss: 0.24328702688217163\n",
      "Epoch: 80 | loss: 0.2325069159269333 | test loss: 0.24313712120056152\n",
      "Epoch: 80 | loss: 0.37368127703666687 | test loss: 0.24313026666641235\n",
      "Epoch: 80 | loss: 0.37513870000839233 | test loss: 0.2434156835079193\n",
      "Epoch: 80 | loss: 0.2942748963832855 | test loss: 0.24344579875469208\n",
      "Epoch: 80 | loss: 0.3153991103172302 | test loss: 0.24356906116008759\n",
      "Epoch: 80 | loss: 0.49418866634368896 | test loss: 0.2430344521999359\n",
      "Epoch: 80 | loss: 0.14024503529071808 | test loss: 0.2431422770023346\n",
      "Epoch: 80 | loss: 0.1406266689300537 | test loss: 0.24311870336532593\n",
      "Epoch: 80 | loss: 0.39130645990371704 | test loss: 0.24318630993366241\n",
      "Epoch: 80 | loss: 0.27303174138069153 | test loss: 0.24341155588626862\n",
      "Epoch: 80 | loss: 0.32387062907218933 | test loss: 0.24342180788516998\n",
      "Epoch: 80 | loss: 0.3680451214313507 | test loss: 0.2436012327671051\n",
      "Epoch: 80 | loss: 0.4627113938331604 | test loss: 0.24330449104309082\n",
      "Epoch: 80 | loss: 0.10949799418449402 | test loss: 0.243015855550766\n",
      "Epoch: 80 | loss: 0.3218492567539215 | test loss: 0.24297642707824707\n",
      "Epoch: 80 | loss: 0.0787554681301117 | test loss: 0.24287836253643036\n",
      "Epoch: 80 | loss: 0.26107650995254517 | test loss: 0.24263398349285126\n",
      "Epoch: 80 | loss: 0.21501478552818298 | test loss: 0.24238556623458862\n",
      "Epoch: 80 | loss: 0.12993352115154266 | test loss: 0.24216486513614655\n",
      "Epoch: 80 | loss: 0.2793879210948944 | test loss: 0.24227432906627655\n",
      "Epoch: 80 | loss: 0.2583702504634857 | test loss: 0.24221554398536682\n",
      "Epoch: 80 | loss: 0.28044530749320984 | test loss: 0.2420036941766739\n",
      "Epoch: 80 | loss: 0.3065345585346222 | test loss: 0.2419707477092743\n",
      "Epoch: 80 | loss: 0.5166229605674744 | test loss: 0.2419702708721161\n",
      "Epoch: 80 | loss: 0.41646575927734375 | test loss: 0.24183021485805511\n",
      "Epoch: 80 | loss: 0.21251140534877777 | test loss: 0.24167071282863617\n",
      "Epoch: 80 | loss: 0.2747420370578766 | test loss: 0.24163256585597992\n",
      "Epoch: 80 | loss: 0.1290382742881775 | test loss: 0.24147219955921173\n",
      "Epoch: 80 | loss: 0.31648343801498413 | test loss: 0.24140775203704834\n",
      "Epoch: 80 | loss: 0.4258979856967926 | test loss: 0.24127312004566193\n",
      "Epoch: 80 | loss: 0.3165348470211029 | test loss: 0.24116265773773193\n",
      "Epoch: 80 | loss: 0.20118556916713715 | test loss: 0.241245836019516\n",
      "Epoch: 80 | loss: 0.2937316596508026 | test loss: 0.2413294017314911\n",
      "Epoch: 81 | loss: 0.09938964992761612 | test loss: 0.28461334109306335\n",
      "Epoch: 81 | loss: 0.5080472230911255 | test loss: 0.28561973571777344\n",
      "Epoch: 81 | loss: 0.35907256603240967 | test loss: 0.28512105345726013\n",
      "Epoch: 81 | loss: 0.25968918204307556 | test loss: 0.28563249111175537\n",
      "Epoch: 81 | loss: 0.18176889419555664 | test loss: 0.28551802039146423\n",
      "Epoch: 81 | loss: 0.29921630024909973 | test loss: 0.2858338952064514\n",
      "Epoch: 81 | loss: 0.23441414535045624 | test loss: 0.28627973794937134\n",
      "Epoch: 81 | loss: 0.29361629486083984 | test loss: 0.28618180751800537\n",
      "Epoch: 81 | loss: 0.25002139806747437 | test loss: 0.28654834628105164\n",
      "Epoch: 81 | loss: 0.36157962679862976 | test loss: 0.2860696017742157\n",
      "Epoch: 81 | loss: 0.37689441442489624 | test loss: 0.2859839200973511\n",
      "Epoch: 81 | loss: 0.27749890089035034 | test loss: 0.2855771481990814\n",
      "Epoch: 81 | loss: 0.31149420142173767 | test loss: 0.2850722074508667\n",
      "Epoch: 81 | loss: 0.3832246661186218 | test loss: 0.28496673703193665\n",
      "Epoch: 81 | loss: 0.13868917524814606 | test loss: 0.28469547629356384\n",
      "Epoch: 81 | loss: 0.3156048357486725 | test loss: 0.2849005460739136\n",
      "Epoch: 81 | loss: 0.306039959192276 | test loss: 0.2852306365966797\n",
      "Epoch: 81 | loss: 0.2794744372367859 | test loss: 0.2856772541999817\n",
      "Epoch: 81 | loss: 0.32244089245796204 | test loss: 0.2856399416923523\n",
      "Epoch: 81 | loss: 0.2386895716190338 | test loss: 0.2854183316230774\n",
      "Epoch: 81 | loss: 0.2156767100095749 | test loss: 0.28492170572280884\n",
      "Epoch: 81 | loss: 0.3988267183303833 | test loss: 0.28443238139152527\n",
      "Epoch: 81 | loss: 0.27217230200767517 | test loss: 0.2845195233821869\n",
      "Epoch: 81 | loss: 0.37932541966438293 | test loss: 0.2839456796646118\n",
      "Epoch: 81 | loss: 0.38804712891578674 | test loss: 0.28378826379776\n",
      "Epoch: 81 | loss: 0.17664538323879242 | test loss: 0.2836369276046753\n",
      "Epoch: 81 | loss: 0.4037184715270996 | test loss: 0.282909095287323\n",
      "Epoch: 81 | loss: 0.3695841431617737 | test loss: 0.2826857268810272\n",
      "Epoch: 81 | loss: 0.27095526456832886 | test loss: 0.28255754709243774\n",
      "Epoch: 81 | loss: 0.17913226783275604 | test loss: 0.2822469472885132\n",
      "Epoch: 81 | loss: 0.1054050624370575 | test loss: 0.2823273241519928\n",
      "Epoch: 81 | loss: 0.12645691633224487 | test loss: 0.2825177013874054\n",
      "Epoch: 81 | loss: 0.22429008781909943 | test loss: 0.28284627199172974\n",
      "Epoch: 82 | loss: 0.17782361805438995 | test loss: 0.3086993396282196\n",
      "Epoch: 82 | loss: 0.39124590158462524 | test loss: 0.3082692325115204\n",
      "Epoch: 82 | loss: 0.1919737160205841 | test loss: 0.30875372886657715\n",
      "Epoch: 82 | loss: 0.3557215631008148 | test loss: 0.30925944447517395\n",
      "Epoch: 82 | loss: 0.3447186350822449 | test loss: 0.3088175356388092\n",
      "Epoch: 82 | loss: 0.18383553624153137 | test loss: 0.3086611032485962\n",
      "Epoch: 82 | loss: 0.34189045429229736 | test loss: 0.30832526087760925\n",
      "Epoch: 82 | loss: 0.42150747776031494 | test loss: 0.3088647127151489\n",
      "Epoch: 82 | loss: 0.36880719661712646 | test loss: 0.30933162569999695\n",
      "Epoch: 82 | loss: 0.14038614928722382 | test loss: 0.3090459108352661\n",
      "Epoch: 82 | loss: 0.29944708943367004 | test loss: 0.3089769780635834\n",
      "Epoch: 82 | loss: 0.16014380753040314 | test loss: 0.3092234432697296\n",
      "Epoch: 82 | loss: 0.23472706973552704 | test loss: 0.30908867716789246\n",
      "Epoch: 82 | loss: 0.2289954125881195 | test loss: 0.3092954754829407\n",
      "Epoch: 82 | loss: 0.22900086641311646 | test loss: 0.3090868890285492\n",
      "Epoch: 82 | loss: 0.3583540916442871 | test loss: 0.30943238735198975\n",
      "Epoch: 82 | loss: 0.19090454280376434 | test loss: 0.30969399213790894\n",
      "Epoch: 82 | loss: 0.2853039801120758 | test loss: 0.3093573749065399\n",
      "Epoch: 82 | loss: 0.537928581237793 | test loss: 0.3090776801109314\n",
      "Epoch: 82 | loss: 0.2090705782175064 | test loss: 0.3094336688518524\n",
      "Epoch: 82 | loss: 0.24157288670539856 | test loss: 0.30901429057121277\n",
      "Epoch: 82 | loss: 0.28850868344306946 | test loss: 0.3092091381549835\n",
      "Epoch: 82 | loss: 0.4657237231731415 | test loss: 0.3093928396701813\n",
      "Epoch: 82 | loss: 0.24511398375034332 | test loss: 0.3091543912887573\n",
      "Epoch: 82 | loss: 0.21938511729240417 | test loss: 0.3086075782775879\n",
      "Epoch: 82 | loss: 0.175358384847641 | test loss: 0.3086050748825073\n",
      "Epoch: 82 | loss: 0.20148715376853943 | test loss: 0.30868446826934814\n",
      "Epoch: 82 | loss: 0.07945051044225693 | test loss: 0.30844053626060486\n",
      "Epoch: 82 | loss: 0.2903832495212555 | test loss: 0.3083011209964752\n",
      "Epoch: 82 | loss: 0.47097650170326233 | test loss: 0.3082673251628876\n",
      "Epoch: 82 | loss: 0.3716621398925781 | test loss: 0.307833194732666\n",
      "Epoch: 82 | loss: 0.45420777797698975 | test loss: 0.3080295920372009\n",
      "Epoch: 82 | loss: 0.2796389162540436 | test loss: 0.30762580037117004\n",
      "Epoch: 83 | loss: 0.15243831276893616 | test loss: 0.23210985958576202\n",
      "Epoch: 83 | loss: 0.16981810331344604 | test loss: 0.2319537252187729\n",
      "Epoch: 83 | loss: 0.38641059398651123 | test loss: 0.23200111091136932\n",
      "Epoch: 83 | loss: 0.40272989869117737 | test loss: 0.23225373029708862\n",
      "Epoch: 83 | loss: 0.24524568021297455 | test loss: 0.23195090889930725\n",
      "Epoch: 83 | loss: 0.48356759548187256 | test loss: 0.23240572214126587\n",
      "Epoch: 83 | loss: 0.24848222732543945 | test loss: 0.23226714134216309\n",
      "Epoch: 83 | loss: 0.391024649143219 | test loss: 0.23220248520374298\n",
      "Epoch: 83 | loss: 0.22597388923168182 | test loss: 0.2322767674922943\n",
      "Epoch: 83 | loss: 0.18244554102420807 | test loss: 0.23220032453536987\n",
      "Epoch: 83 | loss: 0.41541787981987 | test loss: 0.23224517703056335\n",
      "Epoch: 83 | loss: 0.22521615028381348 | test loss: 0.23206651210784912\n",
      "Epoch: 83 | loss: 0.2414497435092926 | test loss: 0.2319352924823761\n",
      "Epoch: 83 | loss: 0.27357038855552673 | test loss: 0.2318839579820633\n",
      "Epoch: 83 | loss: 0.43606308102607727 | test loss: 0.23237061500549316\n",
      "Epoch: 83 | loss: 0.17176981270313263 | test loss: 0.23210881650447845\n",
      "Epoch: 83 | loss: 0.1683596521615982 | test loss: 0.23220780491828918\n",
      "Epoch: 83 | loss: 0.13789302110671997 | test loss: 0.23175153136253357\n",
      "Epoch: 83 | loss: 0.3277296721935272 | test loss: 0.23133191466331482\n",
      "Epoch: 83 | loss: 0.30375975370407104 | test loss: 0.2314307689666748\n",
      "Epoch: 83 | loss: 0.1177019402384758 | test loss: 0.23117610812187195\n",
      "Epoch: 83 | loss: 0.4468277096748352 | test loss: 0.23125135898590088\n",
      "Epoch: 83 | loss: 0.4752776026725769 | test loss: 0.23150162398815155\n",
      "Epoch: 83 | loss: 0.4256359040737152 | test loss: 0.23183871805667877\n",
      "Epoch: 83 | loss: 0.19209930300712585 | test loss: 0.23127950727939606\n",
      "Epoch: 83 | loss: 0.25856488943099976 | test loss: 0.2314273566007614\n",
      "Epoch: 83 | loss: 0.3689327538013458 | test loss: 0.23093709349632263\n",
      "Epoch: 83 | loss: 0.3499242067337036 | test loss: 0.2307761311531067\n",
      "Epoch: 83 | loss: 0.23334234952926636 | test loss: 0.23038658499717712\n",
      "Epoch: 83 | loss: 0.1733652949333191 | test loss: 0.2303740382194519\n",
      "Epoch: 83 | loss: 0.16866081953048706 | test loss: 0.23037871718406677\n",
      "Epoch: 83 | loss: 0.19291703402996063 | test loss: 0.2302575409412384\n",
      "Epoch: 83 | loss: 0.40656325221061707 | test loss: 0.23038095235824585\n",
      "Epoch: 84 | loss: 0.3304031193256378 | test loss: 0.3077760338783264\n",
      "Epoch: 84 | loss: 0.32349351048469543 | test loss: 0.3077673316001892\n",
      "Epoch: 84 | loss: 0.29864832758903503 | test loss: 0.30752673745155334\n",
      "Epoch: 84 | loss: 0.17403405904769897 | test loss: 0.3077155649662018\n",
      "Epoch: 84 | loss: 0.15503425896167755 | test loss: 0.3077508211135864\n",
      "Epoch: 84 | loss: 0.2847324013710022 | test loss: 0.3075638711452484\n",
      "Epoch: 84 | loss: 0.38032975792884827 | test loss: 0.3074953854084015\n",
      "Epoch: 84 | loss: 0.3263394236564636 | test loss: 0.30746522545814514\n",
      "Epoch: 84 | loss: 0.11946436017751694 | test loss: 0.30745115876197815\n",
      "Epoch: 84 | loss: 0.1971116065979004 | test loss: 0.30753734707832336\n",
      "Epoch: 84 | loss: 0.21121971309185028 | test loss: 0.30744802951812744\n",
      "Epoch: 84 | loss: 0.27139943838119507 | test loss: 0.3075233995914459\n",
      "Epoch: 84 | loss: 0.4463386833667755 | test loss: 0.3074892461299896\n",
      "Epoch: 84 | loss: 0.2671574652194977 | test loss: 0.3072989284992218\n",
      "Epoch: 84 | loss: 0.3784884810447693 | test loss: 0.307689368724823\n",
      "Epoch: 84 | loss: 0.2375541627407074 | test loss: 0.3074951171875\n",
      "Epoch: 84 | loss: 0.15935444831848145 | test loss: 0.307467520236969\n",
      "Epoch: 84 | loss: 0.2766188979148865 | test loss: 0.307536244392395\n",
      "Epoch: 84 | loss: 0.460355669260025 | test loss: 0.3072208762168884\n",
      "Epoch: 84 | loss: 0.2783581018447876 | test loss: 0.3071354031562805\n",
      "Epoch: 84 | loss: 0.3913482129573822 | test loss: 0.306915819644928\n",
      "Epoch: 84 | loss: 0.22942061722278595 | test loss: 0.3068822920322418\n",
      "Epoch: 84 | loss: 0.2797754406929016 | test loss: 0.30683988332748413\n",
      "Epoch: 84 | loss: 0.3231757581233978 | test loss: 0.30678239464759827\n",
      "Epoch: 84 | loss: 0.20976562798023224 | test loss: 0.3068751394748688\n",
      "Epoch: 84 | loss: 0.22554439306259155 | test loss: 0.3070237934589386\n",
      "Epoch: 84 | loss: 0.29212072491645813 | test loss: 0.3071278929710388\n",
      "Epoch: 84 | loss: 0.3064088225364685 | test loss: 0.3072007894515991\n",
      "Epoch: 84 | loss: 0.23945800960063934 | test loss: 0.3070155382156372\n",
      "Epoch: 84 | loss: 0.2681976854801178 | test loss: 0.30685216188430786\n",
      "Epoch: 84 | loss: 0.16903218626976013 | test loss: 0.3069125711917877\n",
      "Epoch: 84 | loss: 0.5174866914749146 | test loss: 0.3067950904369354\n",
      "Epoch: 84 | loss: 0.30127575993537903 | test loss: 0.306734174489975\n",
      "Epoch: 85 | loss: 0.28053271770477295 | test loss: 0.2442353069782257\n",
      "Epoch: 85 | loss: 0.5535607933998108 | test loss: 0.24435925483703613\n",
      "Epoch: 85 | loss: 0.319830983877182 | test loss: 0.24459366500377655\n",
      "Epoch: 85 | loss: 0.14644651114940643 | test loss: 0.24462297558784485\n",
      "Epoch: 85 | loss: 0.3279983103275299 | test loss: 0.24465620517730713\n",
      "Epoch: 85 | loss: 0.3615717589855194 | test loss: 0.2445245087146759\n",
      "Epoch: 85 | loss: 0.6022011637687683 | test loss: 0.24503004550933838\n",
      "Epoch: 85 | loss: 0.375043123960495 | test loss: 0.24517597258090973\n",
      "Epoch: 85 | loss: 0.2709568440914154 | test loss: 0.24509641528129578\n",
      "Epoch: 85 | loss: 0.17163392901420593 | test loss: 0.24519643187522888\n",
      "Epoch: 85 | loss: 0.2225533276796341 | test loss: 0.24524593353271484\n",
      "Epoch: 85 | loss: 0.08940763026475906 | test loss: 0.2453169971704483\n",
      "Epoch: 85 | loss: 0.2523565888404846 | test loss: 0.2456270158290863\n",
      "Epoch: 85 | loss: 0.4014752209186554 | test loss: 0.24534131586551666\n",
      "Epoch: 85 | loss: 0.2099352777004242 | test loss: 0.24566514790058136\n",
      "Epoch: 85 | loss: 0.14589916169643402 | test loss: 0.24485084414482117\n",
      "Epoch: 85 | loss: 0.25584495067596436 | test loss: 0.24483942985534668\n",
      "Epoch: 85 | loss: 0.10826098173856735 | test loss: 0.24432195723056793\n",
      "Epoch: 85 | loss: 0.15057779848575592 | test loss: 0.24439089000225067\n",
      "Epoch: 85 | loss: 0.3821851313114166 | test loss: 0.24433442950248718\n",
      "Epoch: 85 | loss: 0.38428252935409546 | test loss: 0.24347415566444397\n",
      "Epoch: 85 | loss: 0.2009250968694687 | test loss: 0.24333930015563965\n",
      "Epoch: 85 | loss: 0.4653657078742981 | test loss: 0.24337729811668396\n",
      "Epoch: 85 | loss: 0.20827606320381165 | test loss: 0.24342359602451324\n",
      "Epoch: 85 | loss: 0.23408737778663635 | test loss: 0.24373570084571838\n",
      "Epoch: 85 | loss: 0.40567195415496826 | test loss: 0.24364149570465088\n",
      "Epoch: 85 | loss: 0.31118714809417725 | test loss: 0.24402283132076263\n",
      "Epoch: 85 | loss: 0.32471519708633423 | test loss: 0.24330495297908783\n",
      "Epoch: 85 | loss: 0.2040577232837677 | test loss: 0.24320004880428314\n",
      "Epoch: 85 | loss: 0.28010767698287964 | test loss: 0.24349533021450043\n",
      "Epoch: 85 | loss: 0.2639724016189575 | test loss: 0.2432612031698227\n",
      "Epoch: 85 | loss: 0.2556360363960266 | test loss: 0.24310854077339172\n",
      "Epoch: 85 | loss: 0.204081192612648 | test loss: 0.2430776208639145\n",
      "Epoch: 86 | loss: 0.23989151418209076 | test loss: 0.2547554671764374\n",
      "Epoch: 86 | loss: 0.26398512721061707 | test loss: 0.25470462441444397\n",
      "Epoch: 86 | loss: 0.2932437062263489 | test loss: 0.25464946031570435\n",
      "Epoch: 86 | loss: 0.33953866362571716 | test loss: 0.2546493709087372\n",
      "Epoch: 86 | loss: 0.22223041951656342 | test loss: 0.2546561062335968\n",
      "Epoch: 86 | loss: 0.2676139771938324 | test loss: 0.2546030282974243\n",
      "Epoch: 86 | loss: 0.2038159817457199 | test loss: 0.25452089309692383\n",
      "Epoch: 86 | loss: 0.3736764192581177 | test loss: 0.25457966327667236\n",
      "Epoch: 86 | loss: 0.11445116251707077 | test loss: 0.25446271896362305\n",
      "Epoch: 86 | loss: 0.30348750948905945 | test loss: 0.25453999638557434\n",
      "Epoch: 86 | loss: 0.20864970982074738 | test loss: 0.2544684112071991\n",
      "Epoch: 86 | loss: 0.2984771728515625 | test loss: 0.2543686032295227\n",
      "Epoch: 86 | loss: 0.1856468766927719 | test loss: 0.2545011639595032\n",
      "Epoch: 86 | loss: 0.20264510810375214 | test loss: 0.25445201992988586\n",
      "Epoch: 86 | loss: 0.7045474648475647 | test loss: 0.2544527053833008\n",
      "Epoch: 86 | loss: 0.2940978705883026 | test loss: 0.2544291615486145\n",
      "Epoch: 86 | loss: 0.3401756286621094 | test loss: 0.2544170022010803\n",
      "Epoch: 86 | loss: 0.5694553852081299 | test loss: 0.254453182220459\n",
      "Epoch: 86 | loss: 0.55936199426651 | test loss: 0.2546611428260803\n",
      "Epoch: 86 | loss: 0.2020948976278305 | test loss: 0.25456979870796204\n",
      "Epoch: 86 | loss: 0.3403393626213074 | test loss: 0.25470831990242004\n",
      "Epoch: 86 | loss: 0.21682144701480865 | test loss: 0.25457218289375305\n",
      "Epoch: 86 | loss: 0.19468308985233307 | test loss: 0.2545090615749359\n",
      "Epoch: 86 | loss: 0.1130577102303505 | test loss: 0.25444573163986206\n",
      "Epoch: 86 | loss: 0.34675076603889465 | test loss: 0.25437110662460327\n",
      "Epoch: 86 | loss: 0.39185985922813416 | test loss: 0.2542828321456909\n",
      "Epoch: 86 | loss: 0.19877108931541443 | test loss: 0.2542848289012909\n",
      "Epoch: 86 | loss: 0.35543182492256165 | test loss: 0.25421664118766785\n",
      "Epoch: 86 | loss: 0.21347084641456604 | test loss: 0.2541840672492981\n",
      "Epoch: 86 | loss: 0.10285767912864685 | test loss: 0.25408586859703064\n",
      "Epoch: 86 | loss: 0.14986729621887207 | test loss: 0.25404977798461914\n",
      "Epoch: 86 | loss: 0.26083749532699585 | test loss: 0.253935307264328\n",
      "Epoch: 86 | loss: 0.2136709988117218 | test loss: 0.2538406252861023\n",
      "Epoch: 87 | loss: 0.13165615499019623 | test loss: 0.24916629493236542\n",
      "Epoch: 87 | loss: 0.09769558906555176 | test loss: 0.2491331696510315\n",
      "Epoch: 87 | loss: 0.32937461137771606 | test loss: 0.24927115440368652\n",
      "Epoch: 87 | loss: 0.4940643310546875 | test loss: 0.24907644093036652\n",
      "Epoch: 87 | loss: 0.2323482483625412 | test loss: 0.24893952906131744\n",
      "Epoch: 87 | loss: 0.2643047273159027 | test loss: 0.2489199936389923\n",
      "Epoch: 87 | loss: 0.27055442333221436 | test loss: 0.24892768263816833\n",
      "Epoch: 87 | loss: 0.31609103083610535 | test loss: 0.24872353672981262\n",
      "Epoch: 87 | loss: 0.5469022989273071 | test loss: 0.24898001551628113\n",
      "Epoch: 87 | loss: 0.11016625910997391 | test loss: 0.2490285485982895\n",
      "Epoch: 87 | loss: 0.20307059586048126 | test loss: 0.2489631026983261\n",
      "Epoch: 87 | loss: 0.4111899137496948 | test loss: 0.2486182600259781\n",
      "Epoch: 87 | loss: 0.3376389443874359 | test loss: 0.2487829029560089\n",
      "Epoch: 87 | loss: 0.4215761721134186 | test loss: 0.24839848279953003\n",
      "Epoch: 87 | loss: 0.20137229561805725 | test loss: 0.2482912689447403\n",
      "Epoch: 87 | loss: 0.3394913971424103 | test loss: 0.24822673201560974\n",
      "Epoch: 87 | loss: 0.2789779007434845 | test loss: 0.24824360013008118\n",
      "Epoch: 87 | loss: 0.2558955252170563 | test loss: 0.2482197880744934\n",
      "Epoch: 87 | loss: 0.19659274816513062 | test loss: 0.2483697235584259\n",
      "Epoch: 87 | loss: 0.4320372939109802 | test loss: 0.2484336793422699\n",
      "Epoch: 87 | loss: 0.23499253392219543 | test loss: 0.2484363317489624\n",
      "Epoch: 87 | loss: 0.2969937026500702 | test loss: 0.24827732145786285\n",
      "Epoch: 87 | loss: 0.29551756381988525 | test loss: 0.2481040060520172\n",
      "Epoch: 87 | loss: 0.3310542702674866 | test loss: 0.2481233775615692\n",
      "Epoch: 87 | loss: 0.24665473401546478 | test loss: 0.2481401264667511\n",
      "Epoch: 87 | loss: 0.2973308563232422 | test loss: 0.24805191159248352\n",
      "Epoch: 87 | loss: 0.22657890617847443 | test loss: 0.24811185896396637\n",
      "Epoch: 87 | loss: 0.3817857801914215 | test loss: 0.24830742180347443\n",
      "Epoch: 87 | loss: 0.22945454716682434 | test loss: 0.24834543466567993\n",
      "Epoch: 87 | loss: 0.0965341329574585 | test loss: 0.24840088188648224\n",
      "Epoch: 87 | loss: 0.18900784850120544 | test loss: 0.24822460114955902\n",
      "Epoch: 87 | loss: 0.27039000391960144 | test loss: 0.2481182962656021\n",
      "Epoch: 87 | loss: 0.2927805185317993 | test loss: 0.24785201251506805\n",
      "Epoch: 88 | loss: 0.2440594583749771 | test loss: 0.3030758202075958\n",
      "Epoch: 88 | loss: 0.1663067787885666 | test loss: 0.3032788932323456\n",
      "Epoch: 88 | loss: 0.25482049584388733 | test loss: 0.3030020594596863\n",
      "Epoch: 88 | loss: 0.22987397015094757 | test loss: 0.3028872311115265\n",
      "Epoch: 88 | loss: 0.13179120421409607 | test loss: 0.3028668761253357\n",
      "Epoch: 88 | loss: 0.30070236325263977 | test loss: 0.30316877365112305\n",
      "Epoch: 88 | loss: 0.3661350905895233 | test loss: 0.3029005229473114\n",
      "Epoch: 88 | loss: 0.343814879655838 | test loss: 0.3024296164512634\n",
      "Epoch: 88 | loss: 0.3869911730289459 | test loss: 0.302360475063324\n",
      "Epoch: 88 | loss: 0.16496212780475616 | test loss: 0.30253899097442627\n",
      "Epoch: 88 | loss: 0.2084372192621231 | test loss: 0.30218228697776794\n",
      "Epoch: 88 | loss: 0.2632734477519989 | test loss: 0.3022780418395996\n",
      "Epoch: 88 | loss: 0.34543874859809875 | test loss: 0.302491158246994\n",
      "Epoch: 88 | loss: 0.28838834166526794 | test loss: 0.3024485409259796\n",
      "Epoch: 88 | loss: 0.22898150980472565 | test loss: 0.30184587836265564\n",
      "Epoch: 88 | loss: 0.18936896324157715 | test loss: 0.30215296149253845\n",
      "Epoch: 88 | loss: 0.6326825618743896 | test loss: 0.3023151755332947\n",
      "Epoch: 88 | loss: 0.341217577457428 | test loss: 0.30238911509513855\n",
      "Epoch: 88 | loss: 0.21444980800151825 | test loss: 0.3015492260456085\n",
      "Epoch: 88 | loss: 0.13487644493579865 | test loss: 0.30111783742904663\n",
      "Epoch: 88 | loss: 0.3447452485561371 | test loss: 0.3014141321182251\n",
      "Epoch: 88 | loss: 0.2811603546142578 | test loss: 0.3014536201953888\n",
      "Epoch: 88 | loss: 0.20290611684322357 | test loss: 0.3013492226600647\n",
      "Epoch: 88 | loss: 0.3411199450492859 | test loss: 0.30173271894454956\n",
      "Epoch: 88 | loss: 0.12322470545768738 | test loss: 0.3017226457595825\n",
      "Epoch: 88 | loss: 0.4162927269935608 | test loss: 0.30236852169036865\n",
      "Epoch: 88 | loss: 0.08672139793634415 | test loss: 0.3021247088909149\n",
      "Epoch: 88 | loss: 0.39931970834732056 | test loss: 0.30241307616233826\n",
      "Epoch: 88 | loss: 0.5863962769508362 | test loss: 0.3016619086265564\n",
      "Epoch: 88 | loss: 0.2128152698278427 | test loss: 0.3019227683544159\n",
      "Epoch: 88 | loss: 0.10553218424320221 | test loss: 0.3019481301307678\n",
      "Epoch: 88 | loss: 0.326335072517395 | test loss: 0.3017616868019104\n",
      "Epoch: 88 | loss: 0.1589137613773346 | test loss: 0.3015686273574829\n",
      "Epoch: 89 | loss: 0.11184271425008774 | test loss: 0.2430809885263443\n",
      "Epoch: 89 | loss: 0.20896537601947784 | test loss: 0.24289999902248383\n",
      "Epoch: 89 | loss: 0.48875731229782104 | test loss: 0.2432059794664383\n",
      "Epoch: 89 | loss: 0.18033644556999207 | test loss: 0.24340477585792542\n",
      "Epoch: 89 | loss: 0.3339187502861023 | test loss: 0.2433486133813858\n",
      "Epoch: 89 | loss: 0.3045007884502411 | test loss: 0.24347521364688873\n",
      "Epoch: 89 | loss: 0.26637712121009827 | test loss: 0.24310584366321564\n",
      "Epoch: 89 | loss: 0.20228269696235657 | test loss: 0.24267548322677612\n",
      "Epoch: 89 | loss: 0.3270358443260193 | test loss: 0.24252891540527344\n",
      "Epoch: 89 | loss: 0.2904234826564789 | test loss: 0.24266982078552246\n",
      "Epoch: 89 | loss: 0.27457764744758606 | test loss: 0.24240703880786896\n",
      "Epoch: 89 | loss: 0.15598440170288086 | test loss: 0.2427518218755722\n",
      "Epoch: 89 | loss: 0.48539823293685913 | test loss: 0.2422534078359604\n",
      "Epoch: 89 | loss: 0.14277993142604828 | test loss: 0.24240238964557648\n",
      "Epoch: 89 | loss: 0.19119597971439362 | test loss: 0.2424340397119522\n",
      "Epoch: 89 | loss: 0.2539467215538025 | test loss: 0.24294109642505646\n",
      "Epoch: 89 | loss: 0.21264290809631348 | test loss: 0.24246957898139954\n",
      "Epoch: 89 | loss: 0.29674533009529114 | test loss: 0.24219021201133728\n",
      "Epoch: 89 | loss: 0.411151260137558 | test loss: 0.2427116185426712\n",
      "Epoch: 89 | loss: 0.17969201505184174 | test loss: 0.2427046298980713\n",
      "Epoch: 89 | loss: 0.2873075306415558 | test loss: 0.2428625375032425\n",
      "Epoch: 89 | loss: 0.0972389206290245 | test loss: 0.24289341270923615\n",
      "Epoch: 89 | loss: 0.24727723002433777 | test loss: 0.24283219873905182\n",
      "Epoch: 89 | loss: 0.36143919825553894 | test loss: 0.24352288246154785\n",
      "Epoch: 89 | loss: 0.4934656023979187 | test loss: 0.24303321540355682\n",
      "Epoch: 89 | loss: 0.4416444003582001 | test loss: 0.24259980022907257\n",
      "Epoch: 89 | loss: 0.07499448955059052 | test loss: 0.24263116717338562\n",
      "Epoch: 89 | loss: 0.3451084792613983 | test loss: 0.24276478588581085\n",
      "Epoch: 89 | loss: 0.3065181076526642 | test loss: 0.2420177310705185\n",
      "Epoch: 89 | loss: 0.3840065598487854 | test loss: 0.24193651974201202\n",
      "Epoch: 89 | loss: 0.3255707323551178 | test loss: 0.2417319416999817\n",
      "Epoch: 89 | loss: 0.2583252489566803 | test loss: 0.2415119856595993\n",
      "Epoch: 89 | loss: 0.24093063175678253 | test loss: 0.24148330092430115\n",
      "Epoch: 90 | loss: 0.30513089895248413 | test loss: 0.23788626492023468\n",
      "Epoch: 90 | loss: 0.18981078267097473 | test loss: 0.23756296932697296\n",
      "Epoch: 90 | loss: 0.18919682502746582 | test loss: 0.23730015754699707\n",
      "Epoch: 90 | loss: 0.44326555728912354 | test loss: 0.23753312230110168\n",
      "Epoch: 90 | loss: 0.22363482415676117 | test loss: 0.23754297196865082\n",
      "Epoch: 90 | loss: 0.17326617240905762 | test loss: 0.2373698502779007\n",
      "Epoch: 90 | loss: 0.39705702662467957 | test loss: 0.23762506246566772\n",
      "Epoch: 90 | loss: 0.24329303205013275 | test loss: 0.23718978464603424\n",
      "Epoch: 90 | loss: 0.29235854744911194 | test loss: 0.23719897866249084\n",
      "Epoch: 90 | loss: 0.08746585249900818 | test loss: 0.23717905580997467\n",
      "Epoch: 90 | loss: 0.41181209683418274 | test loss: 0.23712168633937836\n",
      "Epoch: 90 | loss: 0.22477789223194122 | test loss: 0.23711907863616943\n",
      "Epoch: 90 | loss: 0.2634834945201874 | test loss: 0.23672953248023987\n",
      "Epoch: 90 | loss: 0.4682343304157257 | test loss: 0.23733630776405334\n",
      "Epoch: 90 | loss: 0.29439201951026917 | test loss: 0.23705919086933136\n",
      "Epoch: 90 | loss: 0.2824914753437042 | test loss: 0.23711569607257843\n",
      "Epoch: 90 | loss: 0.2062908560037613 | test loss: 0.2365940362215042\n",
      "Epoch: 90 | loss: 0.24337822198867798 | test loss: 0.23661240935325623\n",
      "Epoch: 90 | loss: 0.09420279413461685 | test loss: 0.23642832040786743\n",
      "Epoch: 90 | loss: 0.2766116261482239 | test loss: 0.2366674691438675\n",
      "Epoch: 90 | loss: 0.2908606231212616 | test loss: 0.23656223714351654\n",
      "Epoch: 90 | loss: 0.22127367556095123 | test loss: 0.23674973845481873\n",
      "Epoch: 90 | loss: 0.16073672473430634 | test loss: 0.23650391399860382\n",
      "Epoch: 90 | loss: 0.1675591617822647 | test loss: 0.236591637134552\n",
      "Epoch: 90 | loss: 0.2636766731739044 | test loss: 0.23655574023723602\n",
      "Epoch: 90 | loss: 0.36768579483032227 | test loss: 0.23639227449893951\n",
      "Epoch: 90 | loss: 0.2688450217247009 | test loss: 0.236697256565094\n",
      "Epoch: 90 | loss: 0.4043138921260834 | test loss: 0.23708559572696686\n",
      "Epoch: 90 | loss: 0.266269326210022 | test loss: 0.23757539689540863\n",
      "Epoch: 90 | loss: 0.45925772190093994 | test loss: 0.23849588632583618\n",
      "Epoch: 90 | loss: 0.2510199248790741 | test loss: 0.23800398409366608\n",
      "Epoch: 90 | loss: 0.357289582490921 | test loss: 0.2375718206167221\n",
      "Epoch: 90 | loss: 0.20988032221794128 | test loss: 0.2373676300048828\n",
      "Epoch: 91 | loss: 0.2329997718334198 | test loss: 0.20561298727989197\n",
      "Epoch: 91 | loss: 0.2469952553510666 | test loss: 0.20566120743751526\n",
      "Epoch: 91 | loss: 0.290831983089447 | test loss: 0.20563332736492157\n",
      "Epoch: 91 | loss: 0.23591379821300507 | test loss: 0.20518793165683746\n",
      "Epoch: 91 | loss: 0.47049665451049805 | test loss: 0.20336373150348663\n",
      "Epoch: 91 | loss: 0.3222636878490448 | test loss: 0.20446686446666718\n",
      "Epoch: 91 | loss: 0.3701060712337494 | test loss: 0.20276302099227905\n",
      "Epoch: 91 | loss: 0.3389473557472229 | test loss: 0.20333294570446014\n",
      "Epoch: 91 | loss: 0.4142760932445526 | test loss: 0.20378869771957397\n",
      "Epoch: 91 | loss: 0.2600274980068207 | test loss: 0.20340034365653992\n",
      "Epoch: 91 | loss: 0.2827237844467163 | test loss: 0.20465394854545593\n",
      "Epoch: 91 | loss: 0.2504245936870575 | test loss: 0.20498999953269958\n",
      "Epoch: 91 | loss: 0.49320438504219055 | test loss: 0.20460674166679382\n",
      "Epoch: 91 | loss: 0.2437307983636856 | test loss: 0.20505458116531372\n",
      "Epoch: 91 | loss: 0.30097389221191406 | test loss: 0.20522812008857727\n",
      "Epoch: 91 | loss: 0.1987091451883316 | test loss: 0.20554938912391663\n",
      "Epoch: 91 | loss: 0.3196236491203308 | test loss: 0.20636875927448273\n",
      "Epoch: 91 | loss: 0.27260273694992065 | test loss: 0.20800349116325378\n",
      "Epoch: 91 | loss: 0.3419376313686371 | test loss: 0.20803722739219666\n",
      "Epoch: 91 | loss: 0.28134283423423767 | test loss: 0.20804725587368011\n",
      "Epoch: 91 | loss: 0.15378284454345703 | test loss: 0.20822912454605103\n",
      "Epoch: 91 | loss: 0.16903671622276306 | test loss: 0.20738868415355682\n",
      "Epoch: 91 | loss: 0.1793813854455948 | test loss: 0.2069983333349228\n",
      "Epoch: 91 | loss: 0.25068968534469604 | test loss: 0.20610885322093964\n",
      "Epoch: 91 | loss: 0.1387099325656891 | test loss: 0.2059272825717926\n",
      "Epoch: 91 | loss: 0.26378530263900757 | test loss: 0.20488478243350983\n",
      "Epoch: 91 | loss: 0.09799527376890182 | test loss: 0.2052381932735443\n",
      "Epoch: 91 | loss: 0.34901612997055054 | test loss: 0.20528185367584229\n",
      "Epoch: 91 | loss: 0.29877015948295593 | test loss: 0.20482508838176727\n",
      "Epoch: 91 | loss: 0.2530507743358612 | test loss: 0.2036462426185608\n",
      "Epoch: 91 | loss: 0.20637936890125275 | test loss: 0.2028731256723404\n",
      "Epoch: 91 | loss: 0.2440490424633026 | test loss: 0.20333877205848694\n",
      "Epoch: 91 | loss: 0.19633932411670685 | test loss: 0.20425370335578918\n",
      "Epoch: 92 | loss: 0.20916125178337097 | test loss: 0.3004377484321594\n",
      "Epoch: 92 | loss: 0.2526482939720154 | test loss: 0.3003675639629364\n",
      "Epoch: 92 | loss: 0.11431695520877838 | test loss: 0.30049341917037964\n",
      "Epoch: 92 | loss: 0.4287776052951813 | test loss: 0.30026817321777344\n",
      "Epoch: 92 | loss: 0.1925826519727707 | test loss: 0.299957275390625\n",
      "Epoch: 92 | loss: 0.18984752893447876 | test loss: 0.2999503016471863\n",
      "Epoch: 92 | loss: 0.20410805940628052 | test loss: 0.29970839619636536\n",
      "Epoch: 92 | loss: 0.241006538271904 | test loss: 0.29950955510139465\n",
      "Epoch: 92 | loss: 0.13061614334583282 | test loss: 0.29949918389320374\n",
      "Epoch: 92 | loss: 0.17404238879680634 | test loss: 0.2995437979698181\n",
      "Epoch: 92 | loss: 0.22892522811889648 | test loss: 0.299307256937027\n",
      "Epoch: 92 | loss: 0.47418421506881714 | test loss: 0.29981017112731934\n",
      "Epoch: 92 | loss: 0.26050224900245667 | test loss: 0.29987573623657227\n",
      "Epoch: 92 | loss: 0.20129483938217163 | test loss: 0.3002423644065857\n",
      "Epoch: 92 | loss: 0.1669221967458725 | test loss: 0.3002748489379883\n",
      "Epoch: 92 | loss: 0.1366889476776123 | test loss: 0.30059653520584106\n",
      "Epoch: 92 | loss: 0.27119675278663635 | test loss: 0.30052754282951355\n",
      "Epoch: 92 | loss: 0.49030447006225586 | test loss: 0.3007768988609314\n",
      "Epoch: 92 | loss: 0.17739237844944 | test loss: 0.30050358176231384\n",
      "Epoch: 92 | loss: 0.21253246068954468 | test loss: 0.3003884553909302\n",
      "Epoch: 92 | loss: 0.39677315950393677 | test loss: 0.300591379404068\n",
      "Epoch: 92 | loss: 0.17683789134025574 | test loss: 0.30051055550575256\n",
      "Epoch: 92 | loss: 0.2130909413099289 | test loss: 0.30095648765563965\n",
      "Epoch: 92 | loss: 0.3552040159702301 | test loss: 0.3005176782608032\n",
      "Epoch: 92 | loss: 0.14743287861347198 | test loss: 0.29995396733283997\n",
      "Epoch: 92 | loss: 0.32826048135757446 | test loss: 0.2999376654624939\n",
      "Epoch: 92 | loss: 0.25965797901153564 | test loss: 0.29963305592536926\n",
      "Epoch: 92 | loss: 0.4032885730266571 | test loss: 0.29957661032676697\n",
      "Epoch: 92 | loss: 0.32451093196868896 | test loss: 0.2995479702949524\n",
      "Epoch: 92 | loss: 0.6616630554199219 | test loss: 0.29911136627197266\n",
      "Epoch: 92 | loss: 0.43351200222969055 | test loss: 0.2990626394748688\n",
      "Epoch: 92 | loss: 0.22912852466106415 | test loss: 0.2990737855434418\n",
      "Epoch: 92 | loss: 0.2671589255332947 | test loss: 0.29935306310653687\n",
      "Epoch: 93 | loss: 0.21162323653697968 | test loss: 0.28893470764160156\n",
      "Epoch: 93 | loss: 0.2307361215353012 | test loss: 0.28820326924324036\n",
      "Epoch: 93 | loss: 0.21250416338443756 | test loss: 0.2883157432079315\n",
      "Epoch: 93 | loss: 0.17577876150608063 | test loss: 0.28854137659072876\n",
      "Epoch: 93 | loss: 0.24336527287960052 | test loss: 0.28894150257110596\n",
      "Epoch: 93 | loss: 0.25827255845069885 | test loss: 0.288624107837677\n",
      "Epoch: 93 | loss: 0.3323221504688263 | test loss: 0.2883574664592743\n",
      "Epoch: 93 | loss: 0.3177211582660675 | test loss: 0.2878914475440979\n",
      "Epoch: 93 | loss: 0.17632539570331573 | test loss: 0.28747737407684326\n",
      "Epoch: 93 | loss: 0.31081679463386536 | test loss: 0.28762125968933105\n",
      "Epoch: 93 | loss: 0.09382130205631256 | test loss: 0.28738725185394287\n",
      "Epoch: 93 | loss: 0.29593318700790405 | test loss: 0.28732872009277344\n",
      "Epoch: 93 | loss: 0.4226025938987732 | test loss: 0.28717413544654846\n",
      "Epoch: 93 | loss: 0.3990447223186493 | test loss: 0.2870636284351349\n",
      "Epoch: 93 | loss: 0.15426667034626007 | test loss: 0.2871328294277191\n",
      "Epoch: 93 | loss: 0.2837486267089844 | test loss: 0.2872872054576874\n",
      "Epoch: 93 | loss: 0.22289827466011047 | test loss: 0.2870037853717804\n",
      "Epoch: 93 | loss: 0.40302082896232605 | test loss: 0.2873247265815735\n",
      "Epoch: 93 | loss: 0.15940351784229279 | test loss: 0.2872326672077179\n",
      "Epoch: 93 | loss: 0.18293394148349762 | test loss: 0.2873082160949707\n",
      "Epoch: 93 | loss: 0.39480310678482056 | test loss: 0.2872481346130371\n",
      "Epoch: 93 | loss: 0.2971828281879425 | test loss: 0.28749439120292664\n",
      "Epoch: 93 | loss: 0.24729731678962708 | test loss: 0.28752246499061584\n",
      "Epoch: 93 | loss: 0.4175371527671814 | test loss: 0.28733986616134644\n",
      "Epoch: 93 | loss: 0.23245325684547424 | test loss: 0.28730371594429016\n",
      "Epoch: 93 | loss: 0.12862400710582733 | test loss: 0.2872137427330017\n",
      "Epoch: 93 | loss: 0.3878656327724457 | test loss: 0.2879888117313385\n",
      "Epoch: 93 | loss: 0.4498349726200104 | test loss: 0.2884678840637207\n",
      "Epoch: 93 | loss: 0.22654278576374054 | test loss: 0.28880539536476135\n",
      "Epoch: 93 | loss: 0.27885523438453674 | test loss: 0.28840741515159607\n",
      "Epoch: 93 | loss: 0.19046474993228912 | test loss: 0.28826653957366943\n",
      "Epoch: 93 | loss: 0.20959140360355377 | test loss: 0.28816914558410645\n",
      "Epoch: 93 | loss: 0.292455792427063 | test loss: 0.2883096933364868\n",
      "Epoch: 94 | loss: 0.21415108442306519 | test loss: 0.3047651946544647\n",
      "Epoch: 94 | loss: 0.301479697227478 | test loss: 0.304735392332077\n",
      "Epoch: 94 | loss: 0.13607801496982574 | test loss: 0.3045998811721802\n",
      "Epoch: 94 | loss: 0.5187434554100037 | test loss: 0.3045727610588074\n",
      "Epoch: 94 | loss: 0.21020165085792542 | test loss: 0.3045177161693573\n",
      "Epoch: 94 | loss: 0.2993162274360657 | test loss: 0.30461937189102173\n",
      "Epoch: 94 | loss: 0.21325384080410004 | test loss: 0.30469810962677\n",
      "Epoch: 94 | loss: 0.17184562981128693 | test loss: 0.30473852157592773\n",
      "Epoch: 94 | loss: 0.21600069105625153 | test loss: 0.30471035838127136\n",
      "Epoch: 94 | loss: 0.5113846659660339 | test loss: 0.3047867715358734\n",
      "Epoch: 94 | loss: 0.35855332016944885 | test loss: 0.30407798290252686\n",
      "Epoch: 94 | loss: 0.29993903636932373 | test loss: 0.30423232913017273\n",
      "Epoch: 94 | loss: 0.14528942108154297 | test loss: 0.3038862645626068\n",
      "Epoch: 94 | loss: 0.19426852464675903 | test loss: 0.30352964997291565\n",
      "Epoch: 94 | loss: 0.29512956738471985 | test loss: 0.30332356691360474\n",
      "Epoch: 94 | loss: 0.2690023183822632 | test loss: 0.3033963143825531\n",
      "Epoch: 94 | loss: 0.4421119689941406 | test loss: 0.30341774225234985\n",
      "Epoch: 94 | loss: 0.31532278656959534 | test loss: 0.3036257326602936\n",
      "Epoch: 94 | loss: 0.1603868156671524 | test loss: 0.3036350905895233\n",
      "Epoch: 94 | loss: 0.3605058789253235 | test loss: 0.3035765290260315\n",
      "Epoch: 94 | loss: 0.191020205616951 | test loss: 0.30361637473106384\n",
      "Epoch: 94 | loss: 0.1058184802532196 | test loss: 0.30357468128204346\n",
      "Epoch: 94 | loss: 0.3465126156806946 | test loss: 0.30364176630973816\n",
      "Epoch: 94 | loss: 0.07734674960374832 | test loss: 0.303557813167572\n",
      "Epoch: 94 | loss: 0.3991933763027191 | test loss: 0.30339014530181885\n",
      "Epoch: 94 | loss: 0.11428090184926987 | test loss: 0.3033120036125183\n",
      "Epoch: 94 | loss: 0.42282983660697937 | test loss: 0.3033958971500397\n",
      "Epoch: 94 | loss: 0.12838870286941528 | test loss: 0.303357869386673\n",
      "Epoch: 94 | loss: 0.4218846559524536 | test loss: 0.3037177324295044\n",
      "Epoch: 94 | loss: 0.22429941594600677 | test loss: 0.303923636674881\n",
      "Epoch: 94 | loss: 0.1659223884344101 | test loss: 0.30393949151039124\n",
      "Epoch: 94 | loss: 0.2068997472524643 | test loss: 0.3040948212146759\n",
      "Epoch: 94 | loss: 0.430349200963974 | test loss: 0.303773432970047\n",
      "Epoch: 95 | loss: 0.21863597631454468 | test loss: 0.23495815694332123\n",
      "Epoch: 95 | loss: 0.16749432682991028 | test loss: 0.2347741425037384\n",
      "Epoch: 95 | loss: 0.2847486436367035 | test loss: 0.2353026121854782\n",
      "Epoch: 95 | loss: 0.2931063771247864 | test loss: 0.2354062795639038\n",
      "Epoch: 95 | loss: 0.25228482484817505 | test loss: 0.23571732640266418\n",
      "Epoch: 95 | loss: 0.23546035587787628 | test loss: 0.23545420169830322\n",
      "Epoch: 95 | loss: 0.17631500959396362 | test loss: 0.2349718064069748\n",
      "Epoch: 95 | loss: 0.22690556943416595 | test loss: 0.23525959253311157\n",
      "Epoch: 95 | loss: 0.10194285959005356 | test loss: 0.23521091043949127\n",
      "Epoch: 95 | loss: 0.32857048511505127 | test loss: 0.23586459457874298\n",
      "Epoch: 95 | loss: 0.2452278435230255 | test loss: 0.23556514084339142\n",
      "Epoch: 95 | loss: 0.3574378192424774 | test loss: 0.2349313497543335\n",
      "Epoch: 95 | loss: 0.4265140891075134 | test loss: 0.235282301902771\n",
      "Epoch: 95 | loss: 0.19228439033031464 | test loss: 0.23531311750411987\n",
      "Epoch: 95 | loss: 0.32139259576797485 | test loss: 0.2348729521036148\n",
      "Epoch: 95 | loss: 0.2174186408519745 | test loss: 0.23494094610214233\n",
      "Epoch: 95 | loss: 0.3170933425426483 | test loss: 0.23468804359436035\n",
      "Epoch: 95 | loss: 0.3027133047580719 | test loss: 0.23447959125041962\n",
      "Epoch: 95 | loss: 0.28361400961875916 | test loss: 0.2343548685312271\n",
      "Epoch: 95 | loss: 0.17698262631893158 | test loss: 0.2343977838754654\n",
      "Epoch: 95 | loss: 0.24926267564296722 | test loss: 0.23460403084754944\n",
      "Epoch: 95 | loss: 0.35839900374412537 | test loss: 0.23424576222896576\n",
      "Epoch: 95 | loss: 0.15933911502361298 | test loss: 0.23418325185775757\n",
      "Epoch: 95 | loss: 0.46093717217445374 | test loss: 0.23369979858398438\n",
      "Epoch: 95 | loss: 0.37804216146469116 | test loss: 0.23354990780353546\n",
      "Epoch: 95 | loss: 0.1941038817167282 | test loss: 0.23368625342845917\n",
      "Epoch: 95 | loss: 0.1185169592499733 | test loss: 0.23356111347675323\n",
      "Epoch: 95 | loss: 0.2046770453453064 | test loss: 0.23334939777851105\n",
      "Epoch: 95 | loss: 0.260517954826355 | test loss: 0.23362842202186584\n",
      "Epoch: 95 | loss: 0.17536525428295135 | test loss: 0.23369865119457245\n",
      "Epoch: 95 | loss: 0.2892683744430542 | test loss: 0.23409196734428406\n",
      "Epoch: 95 | loss: 0.31581205129623413 | test loss: 0.23371848464012146\n",
      "Epoch: 95 | loss: 0.42651093006134033 | test loss: 0.23372937738895416\n",
      "Epoch: 96 | loss: 0.21489746868610382 | test loss: 0.2856902778148651\n",
      "Epoch: 96 | loss: 0.3062945008277893 | test loss: 0.2855972349643707\n",
      "Epoch: 96 | loss: 0.250659316778183 | test loss: 0.285718709230423\n",
      "Epoch: 96 | loss: 0.3559236228466034 | test loss: 0.2855706512928009\n",
      "Epoch: 96 | loss: 0.32494303584098816 | test loss: 0.2858203947544098\n",
      "Epoch: 96 | loss: 0.3499906659126282 | test loss: 0.2859317660331726\n",
      "Epoch: 96 | loss: 0.21016760170459747 | test loss: 0.2858715355396271\n",
      "Epoch: 96 | loss: 0.2069205343723297 | test loss: 0.2859412729740143\n",
      "Epoch: 96 | loss: 0.10047852247953415 | test loss: 0.285605251789093\n",
      "Epoch: 96 | loss: 0.388284832239151 | test loss: 0.2856003940105438\n",
      "Epoch: 96 | loss: 0.38152503967285156 | test loss: 0.2858654856681824\n",
      "Epoch: 96 | loss: 0.33551594614982605 | test loss: 0.2860402762889862\n",
      "Epoch: 96 | loss: 0.17927010357379913 | test loss: 0.2859666049480438\n",
      "Epoch: 96 | loss: 0.24646501243114471 | test loss: 0.2855961322784424\n",
      "Epoch: 96 | loss: 0.4344530701637268 | test loss: 0.2852787375450134\n",
      "Epoch: 96 | loss: 0.20983262360095978 | test loss: 0.28531762957572937\n",
      "Epoch: 96 | loss: 0.43357715010643005 | test loss: 0.2850871980190277\n",
      "Epoch: 96 | loss: 0.25389236211776733 | test loss: 0.28526270389556885\n",
      "Epoch: 96 | loss: 0.28869467973709106 | test loss: 0.28535279631614685\n",
      "Epoch: 96 | loss: 0.23292967677116394 | test loss: 0.2852855324745178\n",
      "Epoch: 96 | loss: 0.1972365528345108 | test loss: 0.2852310538291931\n",
      "Epoch: 96 | loss: 0.21053075790405273 | test loss: 0.2852768301963806\n",
      "Epoch: 96 | loss: 0.26712262630462646 | test loss: 0.28527361154556274\n",
      "Epoch: 96 | loss: 0.1817093938589096 | test loss: 0.2854103446006775\n",
      "Epoch: 96 | loss: 0.25592026114463806 | test loss: 0.2853966951370239\n",
      "Epoch: 96 | loss: 0.21526210010051727 | test loss: 0.28551560640335083\n",
      "Epoch: 96 | loss: 0.4984474778175354 | test loss: 0.28520667552948\n",
      "Epoch: 96 | loss: 0.22543224692344666 | test loss: 0.2851741909980774\n",
      "Epoch: 96 | loss: 0.2013453096151352 | test loss: 0.2851943373680115\n",
      "Epoch: 96 | loss: 0.1471523493528366 | test loss: 0.2849879860877991\n",
      "Epoch: 96 | loss: 0.2501238286495209 | test loss: 0.28527015447616577\n",
      "Epoch: 96 | loss: 0.29051223397254944 | test loss: 0.28520411252975464\n",
      "Epoch: 96 | loss: 0.13694600760936737 | test loss: 0.28533369302749634\n",
      "Epoch: 97 | loss: 0.4217245280742645 | test loss: 0.22554120421409607\n",
      "Epoch: 97 | loss: 0.28663507103919983 | test loss: 0.22531545162200928\n",
      "Epoch: 97 | loss: 0.5102844834327698 | test loss: 0.22511833906173706\n",
      "Epoch: 97 | loss: 0.1083875522017479 | test loss: 0.2251693308353424\n",
      "Epoch: 97 | loss: 0.1632104516029358 | test loss: 0.2252430021762848\n",
      "Epoch: 97 | loss: 0.31658878922462463 | test loss: 0.2249438464641571\n",
      "Epoch: 97 | loss: 0.22570280730724335 | test loss: 0.2249901294708252\n",
      "Epoch: 97 | loss: 0.22378993034362793 | test loss: 0.2250184565782547\n",
      "Epoch: 97 | loss: 0.32628771662712097 | test loss: 0.2253955900669098\n",
      "Epoch: 97 | loss: 0.28328990936279297 | test loss: 0.22553704679012299\n",
      "Epoch: 97 | loss: 0.3496043086051941 | test loss: 0.22544485330581665\n",
      "Epoch: 97 | loss: 0.1490933895111084 | test loss: 0.22499240934848785\n",
      "Epoch: 97 | loss: 0.16155479848384857 | test loss: 0.22477176785469055\n",
      "Epoch: 97 | loss: 0.35593611001968384 | test loss: 0.22457724809646606\n",
      "Epoch: 97 | loss: 0.25291773676872253 | test loss: 0.224771186709404\n",
      "Epoch: 97 | loss: 0.1049058735370636 | test loss: 0.22460924088954926\n",
      "Epoch: 97 | loss: 0.18552780151367188 | test loss: 0.22441090643405914\n",
      "Epoch: 97 | loss: 0.25268295407295227 | test loss: 0.22427889704704285\n",
      "Epoch: 97 | loss: 0.22757868468761444 | test loss: 0.2244272232055664\n",
      "Epoch: 97 | loss: 0.2160964161157608 | test loss: 0.22440558671951294\n",
      "Epoch: 97 | loss: 0.36419442296028137 | test loss: 0.22423745691776276\n",
      "Epoch: 97 | loss: 0.3060916066169739 | test loss: 0.22412757575511932\n",
      "Epoch: 97 | loss: 0.2607927620410919 | test loss: 0.22449719905853271\n",
      "Epoch: 97 | loss: 0.3584505021572113 | test loss: 0.22425776720046997\n",
      "Epoch: 97 | loss: 0.08668320626020432 | test loss: 0.22421099245548248\n",
      "Epoch: 97 | loss: 0.23575852811336517 | test loss: 0.22407934069633484\n",
      "Epoch: 97 | loss: 0.3071000277996063 | test loss: 0.2240191549062729\n",
      "Epoch: 97 | loss: 0.29129788279533386 | test loss: 0.22405098378658295\n",
      "Epoch: 97 | loss: 0.4029448330402374 | test loss: 0.22456279397010803\n",
      "Epoch: 97 | loss: 0.3458881974220276 | test loss: 0.22426019608974457\n",
      "Epoch: 97 | loss: 0.17274682223796844 | test loss: 0.224190816283226\n",
      "Epoch: 97 | loss: 0.08623389154672623 | test loss: 0.22393065690994263\n",
      "Epoch: 97 | loss: 0.36201149225234985 | test loss: 0.22399792075157166\n",
      "Epoch: 98 | loss: 0.29241207242012024 | test loss: 0.2698829174041748\n",
      "Epoch: 98 | loss: 0.14282825589179993 | test loss: 0.2699587941169739\n",
      "Epoch: 98 | loss: 0.12022149562835693 | test loss: 0.26996931433677673\n",
      "Epoch: 98 | loss: 0.36614635586738586 | test loss: 0.26988083124160767\n",
      "Epoch: 98 | loss: 0.2536376416683197 | test loss: 0.26988860964775085\n",
      "Epoch: 98 | loss: 0.29704537987709045 | test loss: 0.26993146538734436\n",
      "Epoch: 98 | loss: 0.18742364645004272 | test loss: 0.26993584632873535\n",
      "Epoch: 98 | loss: 0.40708017349243164 | test loss: 0.2698657214641571\n",
      "Epoch: 98 | loss: 0.3238387405872345 | test loss: 0.26996535062789917\n",
      "Epoch: 98 | loss: 0.24008551239967346 | test loss: 0.26994234323501587\n",
      "Epoch: 98 | loss: 0.13897980749607086 | test loss: 0.2698984444141388\n",
      "Epoch: 98 | loss: 0.12100175023078918 | test loss: 0.26985058188438416\n",
      "Epoch: 98 | loss: 0.2008376568555832 | test loss: 0.2700188159942627\n",
      "Epoch: 98 | loss: 0.46723997592926025 | test loss: 0.27000126242637634\n",
      "Epoch: 98 | loss: 0.23099638521671295 | test loss: 0.27010291814804077\n",
      "Epoch: 98 | loss: 0.40230533480644226 | test loss: 0.2701109051704407\n",
      "Epoch: 98 | loss: 0.16458316147327423 | test loss: 0.2698870897293091\n",
      "Epoch: 98 | loss: 0.278734028339386 | test loss: 0.2698080837726593\n",
      "Epoch: 98 | loss: 0.3607310354709625 | test loss: 0.2696337401866913\n",
      "Epoch: 98 | loss: 0.4014098346233368 | test loss: 0.2696276009082794\n",
      "Epoch: 98 | loss: 0.19697335362434387 | test loss: 0.2695758640766144\n",
      "Epoch: 98 | loss: 0.17939530313014984 | test loss: 0.269466370344162\n",
      "Epoch: 98 | loss: 0.30941736698150635 | test loss: 0.2693435549736023\n",
      "Epoch: 98 | loss: 0.41434383392333984 | test loss: 0.26927444338798523\n",
      "Epoch: 98 | loss: 0.31307104229927063 | test loss: 0.2692544758319855\n",
      "Epoch: 98 | loss: 0.3915853500366211 | test loss: 0.26926565170288086\n",
      "Epoch: 98 | loss: 0.17078110575675964 | test loss: 0.26922160387039185\n",
      "Epoch: 98 | loss: 0.22647221386432648 | test loss: 0.26922330260276794\n",
      "Epoch: 98 | loss: 0.0971001535654068 | test loss: 0.2691681683063507\n",
      "Epoch: 98 | loss: 0.2295333296060562 | test loss: 0.2690829038619995\n",
      "Epoch: 98 | loss: 0.21750524640083313 | test loss: 0.2690414488315582\n",
      "Epoch: 98 | loss: 0.2333088517189026 | test loss: 0.2690163254737854\n",
      "Epoch: 98 | loss: 0.2730674743652344 | test loss: 0.2690122723579407\n",
      "Epoch: 99 | loss: 0.28332725167274475 | test loss: 0.2446819543838501\n",
      "Epoch: 99 | loss: 0.24055951833724976 | test loss: 0.24494799971580505\n",
      "Epoch: 99 | loss: 0.2285504788160324 | test loss: 0.24494566023349762\n",
      "Epoch: 99 | loss: 0.3690322935581207 | test loss: 0.24506868422031403\n",
      "Epoch: 99 | loss: 0.16754694283008575 | test loss: 0.2447800487279892\n",
      "Epoch: 99 | loss: 0.3184092938899994 | test loss: 0.2447531521320343\n",
      "Epoch: 99 | loss: 0.14356595277786255 | test loss: 0.24469681084156036\n",
      "Epoch: 99 | loss: 0.21943600475788116 | test loss: 0.24485528469085693\n",
      "Epoch: 99 | loss: 0.3605572283267975 | test loss: 0.2449813187122345\n",
      "Epoch: 99 | loss: 0.35444262623786926 | test loss: 0.24503974616527557\n",
      "Epoch: 99 | loss: 0.19177038967609406 | test loss: 0.24531887471675873\n",
      "Epoch: 99 | loss: 0.29800519347190857 | test loss: 0.24489356577396393\n",
      "Epoch: 99 | loss: 0.2304392158985138 | test loss: 0.2448483109474182\n",
      "Epoch: 99 | loss: 0.2907315790653229 | test loss: 0.2447684109210968\n",
      "Epoch: 99 | loss: 0.4218587279319763 | test loss: 0.2443070262670517\n",
      "Epoch: 99 | loss: 0.34203818440437317 | test loss: 0.24413494765758514\n",
      "Epoch: 99 | loss: 0.15377037227153778 | test loss: 0.2439139038324356\n",
      "Epoch: 99 | loss: 0.365066796541214 | test loss: 0.24374142289161682\n",
      "Epoch: 99 | loss: 0.21326468884944916 | test loss: 0.24371960759162903\n",
      "Epoch: 99 | loss: 0.20584967732429504 | test loss: 0.24350306391716003\n",
      "Epoch: 99 | loss: 0.3048063814640045 | test loss: 0.24316762387752533\n",
      "Epoch: 99 | loss: 0.12238728255033493 | test loss: 0.24314060807228088\n",
      "Epoch: 99 | loss: 0.20205183327198029 | test loss: 0.24312230944633484\n",
      "Epoch: 99 | loss: 0.19341988861560822 | test loss: 0.24325083196163177\n",
      "Epoch: 99 | loss: 0.4238903820514679 | test loss: 0.24330978095531464\n",
      "Epoch: 99 | loss: 0.30193912982940674 | test loss: 0.24341082572937012\n",
      "Epoch: 99 | loss: 0.11559326946735382 | test loss: 0.24335967004299164\n",
      "Epoch: 99 | loss: 0.29555845260620117 | test loss: 0.24339376389980316\n",
      "Epoch: 99 | loss: 0.25436556339263916 | test loss: 0.2435898631811142\n",
      "Epoch: 99 | loss: 0.24711495637893677 | test loss: 0.24325570464134216\n",
      "Epoch: 99 | loss: 0.1787392497062683 | test loss: 0.24338199198246002\n",
      "Epoch: 99 | loss: 0.33249276876449585 | test loss: 0.2433968484401703\n",
      "Epoch: 99 | loss: 0.24325861036777496 | test loss: 0.2432088851928711\n",
      "Epoch: 100 | loss: 0.17600245773792267 | test loss: 0.28690019249916077\n",
      "Epoch: 100 | loss: 0.36084306240081787 | test loss: 0.2870265245437622\n",
      "Epoch: 100 | loss: 0.11607466638088226 | test loss: 0.287077933549881\n",
      "Epoch: 100 | loss: 0.3675168454647064 | test loss: 0.2871541976928711\n",
      "Epoch: 100 | loss: 0.3933914005756378 | test loss: 0.2873461842536926\n",
      "Epoch: 100 | loss: 0.2939351797103882 | test loss: 0.28722521662712097\n",
      "Epoch: 100 | loss: 0.24477487802505493 | test loss: 0.287441223859787\n",
      "Epoch: 100 | loss: 0.18636904656887054 | test loss: 0.2871813178062439\n",
      "Epoch: 100 | loss: 0.0973651334643364 | test loss: 0.28717178106307983\n",
      "Epoch: 100 | loss: 0.2580179274082184 | test loss: 0.28716808557510376\n",
      "Epoch: 100 | loss: 0.2892337739467621 | test loss: 0.28726229071617126\n",
      "Epoch: 100 | loss: 0.2530793249607086 | test loss: 0.2873993217945099\n",
      "Epoch: 100 | loss: 0.12148399651050568 | test loss: 0.2875390350818634\n",
      "Epoch: 100 | loss: 0.23530413210391998 | test loss: 0.287534236907959\n",
      "Epoch: 100 | loss: 0.19769902527332306 | test loss: 0.28731611371040344\n",
      "Epoch: 100 | loss: 0.3971279263496399 | test loss: 0.287408709526062\n",
      "Epoch: 100 | loss: 0.3228950798511505 | test loss: 0.2874111831188202\n",
      "Epoch: 100 | loss: 0.14216260612010956 | test loss: 0.2870364785194397\n",
      "Epoch: 100 | loss: 0.3164321482181549 | test loss: 0.2870579957962036\n",
      "Epoch: 100 | loss: 0.3924962878227234 | test loss: 0.2869432866573334\n",
      "Epoch: 100 | loss: 0.4165028929710388 | test loss: 0.2870570719242096\n",
      "Epoch: 100 | loss: 0.3456161618232727 | test loss: 0.28653016686439514\n",
      "Epoch: 100 | loss: 0.08184968680143356 | test loss: 0.2865546643733978\n",
      "Epoch: 100 | loss: 0.13899311423301697 | test loss: 0.2864099144935608\n",
      "Epoch: 100 | loss: 0.1572982221841812 | test loss: 0.2865332067012787\n",
      "Epoch: 100 | loss: 0.236604705452919 | test loss: 0.28688061237335205\n",
      "Epoch: 100 | loss: 0.1550329178571701 | test loss: 0.28686580061912537\n",
      "Epoch: 100 | loss: 0.21650977432727814 | test loss: 0.2868640124797821\n",
      "Epoch: 100 | loss: 0.37108853459358215 | test loss: 0.28652656078338623\n",
      "Epoch: 100 | loss: 0.24950465559959412 | test loss: 0.28618595004081726\n",
      "Epoch: 100 | loss: 0.3692156970500946 | test loss: 0.2859942317008972\n",
      "Epoch: 100 | loss: 0.2898300886154175 | test loss: 0.2858867943286896\n",
      "Epoch: 100 | loss: 0.31559327244758606 | test loss: 0.286281555891037\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "import torch.optim as optim\n",
    "\n",
    "model = Classify()\n",
    "\n",
    "epochs = 100\n",
    "\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "loss_train = []\n",
    "loss_test = []\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    running_train_loss = 0\n",
    "    running_test_loss = 0\n",
    "\n",
    "    x_trbh, x_tsbh, y_trbh, y_tsbh = batchify(x_train, x_test, y_train, y_test, 20)\n",
    "    for i in range(x_trbh.shape[0]):\n",
    "        \n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        pred=model.forward(x_trbh[i])\n",
    "        train_loss=criterion(pred,y_trbh[i])\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            test_pred = model.forward(x_tsbh)\n",
    "\n",
    "            test_loss = criterion(test_pred, y_tsbh)\n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        running_train_loss += train_loss.item()\n",
    "        running_test_loss += test_loss.item()\n",
    "\n",
    "        #loss_train.append(train_loss.item())\n",
    "        #loss_test.append(test_loss.item())\n",
    "       \n",
    "        print(f'Epoch: {epoch + 1} | loss: {train_loss.item()} | test loss: {test_loss.item()}' )\n",
    "\n",
    "    loss_train.append(running_train_loss/len(x_trbh))\n",
    "    loss_test.append(running_test_loss/len(x_tsbh))\n",
    "\n",
    "plt.plot(loss_train, label='train Loss')\n",
    "#plt.plot(loss_test, label='test Loss')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d70a0436c50e7b6f498bd5bbc4667e2de2959c3796a9ad1ed0d0030ffa24a30"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit ('deep_learning': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
