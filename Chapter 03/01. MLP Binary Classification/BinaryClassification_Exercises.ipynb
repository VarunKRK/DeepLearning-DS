{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35a3a5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | loss: 0.7155269980430603 | test loss: 0.7379710078239441 | accuracy: tensor([0.4667])\n",
      "Epoch: 2 | loss: 0.7150080800056458 | test loss: 0.7372234463691711 | accuracy: tensor([0.4667])\n",
      "Epoch: 3 | loss: 0.7144951820373535 | test loss: 0.736481785774231 | accuracy: tensor([0.4667])\n",
      "Epoch: 4 | loss: 0.7139883041381836 | test loss: 0.7357463240623474 | accuracy: tensor([0.4667])\n",
      "Epoch: 5 | loss: 0.7134873270988464 | test loss: 0.7350170016288757 | accuracy: tensor([0.4667])\n",
      "Epoch: 6 | loss: 0.7129924297332764 | test loss: 0.7342939972877502 | accuracy: tensor([0.4667])\n",
      "Epoch: 7 | loss: 0.7125034332275391 | test loss: 0.7335773706436157 | accuracy: tensor([0.4667])\n",
      "Epoch: 8 | loss: 0.7120208740234375 | test loss: 0.7328673005104065 | accuracy: tensor([0.4667])\n",
      "Epoch: 9 | loss: 0.7115444540977478 | test loss: 0.7321637868881226 | accuracy: tensor([0.4667])\n",
      "Epoch: 10 | loss: 0.7110745310783386 | test loss: 0.7314670085906982 | accuracy: tensor([0.4667])\n",
      "Epoch: 11 | loss: 0.7106106281280518 | test loss: 0.7307767271995544 | accuracy: tensor([0.4667])\n",
      "Epoch: 12 | loss: 0.7101532816886902 | test loss: 0.730093240737915 | accuracy: tensor([0.4667])\n",
      "Epoch: 13 | loss: 0.7097022533416748 | test loss: 0.7294166088104248 | accuracy: tensor([0.4667])\n",
      "Epoch: 14 | loss: 0.7092576026916504 | test loss: 0.7287464737892151 | accuracy: tensor([0.4667])\n",
      "Epoch: 15 | loss: 0.7088193893432617 | test loss: 0.7280834317207336 | accuracy: tensor([0.4667])\n",
      "Epoch: 16 | loss: 0.708387553691864 | test loss: 0.7274272441864014 | accuracy: tensor([0.4667])\n",
      "Epoch: 17 | loss: 0.707962155342102 | test loss: 0.7267777919769287 | accuracy: tensor([0.4667])\n",
      "Epoch: 18 | loss: 0.7075431942939758 | test loss: 0.7261354923248291 | accuracy: tensor([0.4667])\n",
      "Epoch: 19 | loss: 0.7071307301521301 | test loss: 0.7255001664161682 | accuracy: tensor([0.4667])\n",
      "Epoch: 20 | loss: 0.7067248821258545 | test loss: 0.7248718738555908 | accuracy: tensor([0.4667])\n",
      "Epoch: 21 | loss: 0.7063254714012146 | test loss: 0.7242509126663208 | accuracy: tensor([0.4667])\n",
      "Epoch: 22 | loss: 0.7059326767921448 | test loss: 0.7236371636390686 | accuracy: tensor([0.4667])\n",
      "Epoch: 23 | loss: 0.705546498298645 | test loss: 0.7230305075645447 | accuracy: tensor([0.4667])\n",
      "Epoch: 24 | loss: 0.7051668167114258 | test loss: 0.7224312424659729 | accuracy: tensor([0.4667])\n",
      "Epoch: 25 | loss: 0.7047938704490662 | test loss: 0.7218394875526428 | accuracy: tensor([0.4667])\n",
      "Epoch: 26 | loss: 0.7044275403022766 | test loss: 0.7212549448013306 | accuracy: tensor([0.4667])\n",
      "Epoch: 27 | loss: 0.7040677666664124 | test loss: 0.7206779718399048 | accuracy: tensor([0.4667])\n",
      "Epoch: 28 | loss: 0.7037146687507629 | test loss: 0.7201083302497864 | accuracy: tensor([0.4667])\n",
      "Epoch: 29 | loss: 0.7033681869506836 | test loss: 0.7195463180541992 | accuracy: tensor([0.4667])\n",
      "Epoch: 30 | loss: 0.7030282616615295 | test loss: 0.7189916968345642 | accuracy: tensor([0.4667])\n",
      "Epoch: 31 | loss: 0.7026949524879456 | test loss: 0.71844482421875 | accuracy: tensor([0.4667])\n",
      "Epoch: 32 | loss: 0.7023683190345764 | test loss: 0.7179052829742432 | accuracy: tensor([0.4667])\n",
      "Epoch: 33 | loss: 0.7020482420921326 | test loss: 0.7173734903335571 | accuracy: tensor([0.4667])\n",
      "Epoch: 34 | loss: 0.701734721660614 | test loss: 0.7168492674827576 | accuracy: tensor([0.4667])\n",
      "Epoch: 35 | loss: 0.701427698135376 | test loss: 0.7163325548171997 | accuracy: tensor([0.4667])\n",
      "Epoch: 36 | loss: 0.7011272311210632 | test loss: 0.7158234715461731 | accuracy: tensor([0.4667])\n",
      "Epoch: 37 | loss: 0.7008331418037415 | test loss: 0.715321958065033 | accuracy: tensor([0.4667])\n",
      "Epoch: 38 | loss: 0.7005456686019897 | test loss: 0.7148281335830688 | accuracy: tensor([0.4667])\n",
      "Epoch: 39 | loss: 0.7002644538879395 | test loss: 0.7143418788909912 | accuracy: tensor([0.4667])\n",
      "Epoch: 40 | loss: 0.6999896764755249 | test loss: 0.7138631939888 | accuracy: tensor([0.4667])\n",
      "Epoch: 41 | loss: 0.6997213363647461 | test loss: 0.7133921384811401 | accuracy: tensor([0.4667])\n",
      "Epoch: 42 | loss: 0.6994591951370239 | test loss: 0.7129286527633667 | accuracy: tensor([0.4667])\n",
      "Epoch: 43 | loss: 0.6992033123970032 | test loss: 0.7124727368354797 | accuracy: tensor([0.4667])\n",
      "Epoch: 44 | loss: 0.6989536285400391 | test loss: 0.7120243906974792 | accuracy: tensor([0.4667])\n",
      "Epoch: 45 | loss: 0.698710024356842 | test loss: 0.7115835547447205 | accuracy: tensor([0.4667])\n",
      "Epoch: 46 | loss: 0.6984726786613464 | test loss: 0.7111502885818481 | accuracy: tensor([0.4667])\n",
      "Epoch: 47 | loss: 0.6982412934303284 | test loss: 0.7107245326042175 | accuracy: tensor([0.4667])\n",
      "Epoch: 48 | loss: 0.6980159282684326 | test loss: 0.7103061676025391 | accuracy: tensor([0.4667])\n",
      "Epoch: 49 | loss: 0.6977965831756592 | test loss: 0.7098953127861023 | accuracy: tensor([0.4667])\n",
      "Epoch: 50 | loss: 0.6975829601287842 | test loss: 0.7094918489456177 | accuracy: tensor([0.4667])\n",
      "Epoch: 51 | loss: 0.6973752975463867 | test loss: 0.7090958952903748 | accuracy: tensor([0.4667])\n",
      "Epoch: 52 | loss: 0.6971731185913086 | test loss: 0.7087072134017944 | accuracy: tensor([0.4667])\n",
      "Epoch: 53 | loss: 0.696976900100708 | test loss: 0.7083259224891663 | accuracy: tensor([0.4667])\n",
      "Epoch: 54 | loss: 0.6967860460281372 | test loss: 0.7079518437385559 | accuracy: tensor([0.4667])\n",
      "Epoch: 55 | loss: 0.6966009140014648 | test loss: 0.7075851559638977 | accuracy: tensor([0.4667])\n",
      "Epoch: 56 | loss: 0.6964211463928223 | test loss: 0.707225501537323 | accuracy: tensor([0.4667])\n",
      "Epoch: 57 | loss: 0.6962466835975647 | test loss: 0.7068730592727661 | accuracy: tensor([0.4667])\n",
      "Epoch: 58 | loss: 0.6960775256156921 | test loss: 0.7065277099609375 | accuracy: tensor([0.4667])\n",
      "Epoch: 59 | loss: 0.6959136724472046 | test loss: 0.7061895132064819 | accuracy: tensor([0.4667])\n",
      "Epoch: 60 | loss: 0.695754885673523 | test loss: 0.7058582305908203 | accuracy: tensor([0.4667])\n",
      "Epoch: 61 | loss: 0.6956009864807129 | test loss: 0.7055339217185974 | accuracy: tensor([0.4667])\n",
      "Epoch: 62 | loss: 0.6954521536827087 | test loss: 0.7052165269851685 | accuracy: tensor([0.4667])\n",
      "Epoch: 63 | loss: 0.6953080892562866 | test loss: 0.7049058079719543 | accuracy: tensor([0.4667])\n",
      "Epoch: 64 | loss: 0.6951687932014465 | test loss: 0.704602062702179 | accuracy: tensor([0.4667])\n",
      "Epoch: 65 | loss: 0.6950342655181885 | test loss: 0.7043048739433289 | accuracy: tensor([0.4667])\n",
      "Epoch: 66 | loss: 0.694904088973999 | test loss: 0.704014241695404 | accuracy: tensor([0.4667])\n",
      "Epoch: 67 | loss: 0.694778561592102 | test loss: 0.7037304043769836 | accuracy: tensor([0.4667])\n",
      "Epoch: 68 | loss: 0.6946572661399841 | test loss: 0.7034528255462646 | accuracy: tensor([0.4667])\n",
      "Epoch: 69 | loss: 0.69454026222229 | test loss: 0.7031816244125366 | accuracy: tensor([0.4667])\n",
      "Epoch: 70 | loss: 0.694427490234375 | test loss: 0.7029168605804443 | accuracy: tensor([0.4667])\n",
      "Epoch: 71 | loss: 0.6943186521530151 | test loss: 0.7026582956314087 | accuracy: tensor([0.4667])\n",
      "Epoch: 72 | loss: 0.6942138075828552 | test loss: 0.7024059295654297 | accuracy: tensor([0.4667])\n",
      "Epoch: 73 | loss: 0.6941127777099609 | test loss: 0.7021595239639282 | accuracy: tensor([0.4667])\n",
      "Epoch: 74 | loss: 0.6940154433250427 | test loss: 0.7019191384315491 | accuracy: tensor([0.4667])\n",
      "Epoch: 75 | loss: 0.6939217448234558 | test loss: 0.7016845941543579 | accuracy: tensor([0.4667])\n",
      "Epoch: 76 | loss: 0.6938316822052002 | test loss: 0.7014559507369995 | accuracy: tensor([0.4667])\n",
      "Epoch: 77 | loss: 0.693744957447052 | test loss: 0.7012328505516052 | accuracy: tensor([0.4667])\n",
      "Epoch: 78 | loss: 0.6936615705490112 | test loss: 0.7010154724121094 | accuracy: tensor([0.4667])\n",
      "Epoch: 79 | loss: 0.6935813426971436 | test loss: 0.7008035778999329 | accuracy: tensor([0.4667])\n",
      "Epoch: 80 | loss: 0.6935041546821594 | test loss: 0.7005969882011414 | accuracy: tensor([0.4667])\n",
      "Epoch: 81 | loss: 0.6934301257133484 | test loss: 0.7003958821296692 | accuracy: tensor([0.4667])\n",
      "Epoch: 82 | loss: 0.693358838558197 | test loss: 0.7001998424530029 | accuracy: tensor([0.4667])\n",
      "Epoch: 83 | loss: 0.6932904124259949 | test loss: 0.7000090479850769 | accuracy: tensor([0.4667])\n",
      "Epoch: 84 | loss: 0.6932246088981628 | test loss: 0.6998231410980225 | accuracy: tensor([0.4667])\n",
      "Epoch: 85 | loss: 0.6931614875793457 | test loss: 0.6996421813964844 | accuracy: tensor([0.4667])\n",
      "Epoch: 86 | loss: 0.6931007504463196 | test loss: 0.6994660496711731 | accuracy: tensor([0.4667])\n",
      "Epoch: 87 | loss: 0.6930424571037292 | test loss: 0.6992946863174438 | accuracy: tensor([0.4667])\n",
      "Epoch: 88 | loss: 0.6929864287376404 | test loss: 0.6991278529167175 | accuracy: tensor([0.4667])\n",
      "Epoch: 89 | loss: 0.6929326057434082 | test loss: 0.6989654302597046 | accuracy: tensor([0.4667])\n",
      "Epoch: 90 | loss: 0.6928808689117432 | test loss: 0.6988075375556946 | accuracy: tensor([0.4667])\n",
      "Epoch: 91 | loss: 0.6928310990333557 | test loss: 0.6986539363861084 | accuracy: tensor([0.4667])\n",
      "Epoch: 92 | loss: 0.6927833557128906 | test loss: 0.6985043883323669 | accuracy: tensor([0.4667])\n",
      "Epoch: 93 | loss: 0.6927372813224792 | test loss: 0.6983591318130493 | accuracy: tensor([0.4667])\n",
      "Epoch: 94 | loss: 0.6926931142807007 | test loss: 0.6982177495956421 | accuracy: tensor([0.4667])\n",
      "Epoch: 95 | loss: 0.6926506757736206 | test loss: 0.69808030128479 | accuracy: tensor([0.4667])\n",
      "Epoch: 96 | loss: 0.6926095485687256 | test loss: 0.6979466676712036 | accuracy: tensor([0.4667])\n",
      "Epoch: 97 | loss: 0.692570149898529 | test loss: 0.6978167295455933 | accuracy: tensor([0.4667])\n",
      "Epoch: 98 | loss: 0.6925320029258728 | test loss: 0.6976903080940247 | accuracy: tensor([0.4667])\n",
      "Epoch: 99 | loss: 0.6924954056739807 | test loss: 0.6975674033164978 | accuracy: tensor([0.4667])\n",
      "Epoch: 100 | loss: 0.6924600601196289 | test loss: 0.6974480152130127 | accuracy: tensor([0.4667])\n",
      "Epoch: 101 | loss: 0.6924257278442383 | test loss: 0.6973318457603455 | accuracy: tensor([0.4667])\n",
      "Epoch: 102 | loss: 0.6923927068710327 | test loss: 0.6972189545631409 | accuracy: tensor([0.4667])\n",
      "Epoch: 103 | loss: 0.6923606991767883 | test loss: 0.6971091628074646 | accuracy: tensor([0.4667])\n",
      "Epoch: 104 | loss: 0.6923297047615051 | test loss: 0.6970024108886719 | accuracy: tensor([0.4667])\n",
      "Epoch: 105 | loss: 0.6922996640205383 | test loss: 0.6968986392021179 | accuracy: tensor([0.4667])\n",
      "Epoch: 106 | loss: 0.6922705769538879 | test loss: 0.6967976689338684 | accuracy: tensor([0.4667])\n",
      "Epoch: 107 | loss: 0.6922422051429749 | test loss: 0.6966996788978577 | accuracy: tensor([0.4667])\n",
      "Epoch: 108 | loss: 0.6922147274017334 | test loss: 0.6966041922569275 | accuracy: tensor([0.4667])\n",
      "Epoch: 109 | loss: 0.6921879053115845 | test loss: 0.696511447429657 | accuracy: tensor([0.4667])\n",
      "Epoch: 110 | loss: 0.6921617984771729 | test loss: 0.6964210867881775 | accuracy: tensor([0.4667])\n",
      "Epoch: 111 | loss: 0.692136287689209 | test loss: 0.6963332295417786 | accuracy: tensor([0.4667])\n",
      "Epoch: 112 | loss: 0.6921113133430481 | test loss: 0.6962477564811707 | accuracy: tensor([0.4667])\n",
      "Epoch: 113 | loss: 0.6920869946479797 | test loss: 0.6961645483970642 | accuracy: tensor([0.4667])\n",
      "Epoch: 114 | loss: 0.6920630931854248 | test loss: 0.6960837244987488 | accuracy: tensor([0.4667])\n",
      "Epoch: 115 | loss: 0.6920396089553833 | test loss: 0.6960048675537109 | accuracy: tensor([0.4667])\n",
      "Epoch: 116 | loss: 0.6920165419578552 | test loss: 0.695928156375885 | accuracy: tensor([0.4667])\n",
      "Epoch: 117 | loss: 0.6919939517974854 | test loss: 0.6958533525466919 | accuracy: tensor([0.4667])\n",
      "Epoch: 118 | loss: 0.6919716000556946 | test loss: 0.6957805752754211 | accuracy: tensor([0.4667])\n",
      "Epoch: 119 | loss: 0.6919496059417725 | test loss: 0.695709764957428 | accuracy: tensor([0.4667])\n",
      "Epoch: 120 | loss: 0.6919277906417847 | test loss: 0.6956405639648438 | accuracy: tensor([0.4667])\n",
      "Epoch: 121 | loss: 0.6919062733650208 | test loss: 0.6955731511116028 | accuracy: tensor([0.4667])\n",
      "Epoch: 122 | loss: 0.6918849945068359 | test loss: 0.6955073475837708 | accuracy: tensor([0.4667])\n",
      "Epoch: 123 | loss: 0.6918638348579407 | test loss: 0.6954432129859924 | accuracy: tensor([0.4667])\n",
      "Epoch: 124 | loss: 0.6918428540229797 | test loss: 0.6953807473182678 | accuracy: tensor([0.4667])\n",
      "Epoch: 125 | loss: 0.6918219923973083 | test loss: 0.6953195929527283 | accuracy: tensor([0.4667])\n",
      "Epoch: 126 | loss: 0.6918013691902161 | test loss: 0.6952598690986633 | accuracy: tensor([0.4667])\n",
      "Epoch: 127 | loss: 0.6917806267738342 | test loss: 0.695201575756073 | accuracy: tensor([0.4667])\n",
      "Epoch: 128 | loss: 0.6917600631713867 | test loss: 0.6951445937156677 | accuracy: tensor([0.4667])\n",
      "Epoch: 129 | loss: 0.6917394399642944 | test loss: 0.6950889229774475 | accuracy: tensor([0.4667])\n",
      "Epoch: 130 | loss: 0.6917189359664917 | test loss: 0.6950342655181885 | accuracy: tensor([0.4667])\n",
      "Epoch: 131 | loss: 0.6916983723640442 | test loss: 0.694980800151825 | accuracy: tensor([0.4667])\n",
      "Epoch: 132 | loss: 0.6916778683662415 | test loss: 0.6949285268783569 | accuracy: tensor([0.4667])\n",
      "Epoch: 133 | loss: 0.6916572451591492 | test loss: 0.6948773264884949 | accuracy: tensor([0.4667])\n",
      "Epoch: 134 | loss: 0.6916366815567017 | test loss: 0.6948270201683044 | accuracy: tensor([0.4667])\n",
      "Epoch: 135 | loss: 0.6916159391403198 | test loss: 0.6947777271270752 | accuracy: tensor([0.4667])\n",
      "Epoch: 136 | loss: 0.691595196723938 | test loss: 0.6947293877601624 | accuracy: tensor([0.4667])\n",
      "Epoch: 137 | loss: 0.6915743350982666 | test loss: 0.6946818828582764 | accuracy: tensor([0.4667])\n",
      "Epoch: 138 | loss: 0.6915533542633057 | test loss: 0.6946350336074829 | accuracy: tensor([0.4667])\n",
      "Epoch: 139 | loss: 0.6915322542190552 | test loss: 0.6945890784263611 | accuracy: tensor([0.4667])\n",
      "Epoch: 140 | loss: 0.6915110349655151 | test loss: 0.6945438385009766 | accuracy: tensor([0.4667])\n",
      "Epoch: 141 | loss: 0.6914897561073303 | test loss: 0.6944992542266846 | accuracy: tensor([0.4667])\n",
      "Epoch: 142 | loss: 0.6914681792259216 | test loss: 0.6944553852081299 | accuracy: tensor([0.4667])\n",
      "Epoch: 143 | loss: 0.6914464831352234 | test loss: 0.694412112236023 | accuracy: tensor([0.4667])\n",
      "Epoch: 144 | loss: 0.6914246082305908 | test loss: 0.6943693161010742 | accuracy: tensor([0.4667])\n",
      "Epoch: 145 | loss: 0.6914027333259583 | test loss: 0.694327175617218 | accuracy: tensor([0.4667])\n",
      "Epoch: 146 | loss: 0.6913803815841675 | test loss: 0.6942854523658752 | accuracy: tensor([0.4667])\n",
      "Epoch: 147 | loss: 0.6913579702377319 | test loss: 0.6942442655563354 | accuracy: tensor([0.4667])\n",
      "Epoch: 148 | loss: 0.6913353800773621 | test loss: 0.6942034363746643 | accuracy: tensor([0.4667])\n",
      "Epoch: 149 | loss: 0.6913126111030579 | test loss: 0.6941629648208618 | accuracy: tensor([0.4667])\n",
      "Epoch: 150 | loss: 0.691289484500885 | test loss: 0.694122850894928 | accuracy: tensor([0.4667])\n",
      "Epoch: 151 | loss: 0.6912661790847778 | test loss: 0.6940830945968628 | accuracy: tensor([0.4667])\n",
      "Epoch: 152 | loss: 0.6912426948547363 | test loss: 0.6940435767173767 | accuracy: tensor([0.4667])\n",
      "Epoch: 153 | loss: 0.691218912601471 | test loss: 0.6940044164657593 | accuracy: tensor([0.4667])\n",
      "Epoch: 154 | loss: 0.6911948919296265 | test loss: 0.6939653754234314 | accuracy: tensor([0.4667])\n",
      "Epoch: 155 | loss: 0.6911706924438477 | test loss: 0.6939266920089722 | accuracy: tensor([0.4667])\n",
      "Epoch: 156 | loss: 0.691146194934845 | test loss: 0.6938879489898682 | accuracy: tensor([0.4667])\n",
      "Epoch: 157 | loss: 0.6911212801933289 | test loss: 0.693849503993988 | accuracy: tensor([0.4667])\n",
      "Epoch: 158 | loss: 0.6910962462425232 | test loss: 0.6938111186027527 | accuracy: tensor([0.4667])\n",
      "Epoch: 159 | loss: 0.6910708546638489 | test loss: 0.6937729120254517 | accuracy: tensor([0.4667])\n",
      "Epoch: 160 | loss: 0.6910452246665955 | test loss: 0.6937345862388611 | accuracy: tensor([0.4667])\n",
      "Epoch: 161 | loss: 0.6910192370414734 | test loss: 0.6936964392662048 | accuracy: tensor([0.4667])\n",
      "Epoch: 162 | loss: 0.6909930109977722 | test loss: 0.6936582326889038 | accuracy: tensor([0.4667])\n",
      "Epoch: 163 | loss: 0.6909665465354919 | test loss: 0.6936200261116028 | accuracy: tensor([0.4667])\n",
      "Epoch: 164 | loss: 0.6909396052360535 | test loss: 0.6935818195343018 | accuracy: tensor([0.4667])\n",
      "Epoch: 165 | loss: 0.6909124851226807 | test loss: 0.693543553352356 | accuracy: tensor([0.4667])\n",
      "Epoch: 166 | loss: 0.6908849477767944 | test loss: 0.6935052275657654 | accuracy: tensor([0.4667])\n",
      "Epoch: 167 | loss: 0.6908571720123291 | test loss: 0.6934667825698853 | accuracy: tensor([0.4667])\n",
      "Epoch: 168 | loss: 0.6908289194107056 | test loss: 0.693428099155426 | accuracy: tensor([0.4667])\n",
      "Epoch: 169 | loss: 0.6908004283905029 | test loss: 0.6933892965316772 | accuracy: tensor([0.4667])\n",
      "Epoch: 170 | loss: 0.6907715201377869 | test loss: 0.6933504939079285 | accuracy: tensor([0.4667])\n",
      "Epoch: 171 | loss: 0.6907423734664917 | test loss: 0.6933113932609558 | accuracy: tensor([0.4667])\n",
      "Epoch: 172 | loss: 0.6907128691673279 | test loss: 0.6932719945907593 | accuracy: tensor([0.4667])\n",
      "Epoch: 173 | loss: 0.6906829476356506 | test loss: 0.6932325959205627 | accuracy: tensor([0.4667])\n",
      "Epoch: 174 | loss: 0.69065260887146 | test loss: 0.6931927800178528 | accuracy: tensor([0.4667])\n",
      "Epoch: 175 | loss: 0.6906219720840454 | test loss: 0.6931527256965637 | accuracy: tensor([0.4667])\n",
      "Epoch: 176 | loss: 0.6905909180641174 | test loss: 0.693112313747406 | accuracy: tensor([0.4667])\n",
      "Epoch: 177 | loss: 0.6905595064163208 | test loss: 0.693071722984314 | accuracy: tensor([0.4667])\n",
      "Epoch: 178 | loss: 0.690527617931366 | test loss: 0.6930307149887085 | accuracy: tensor([0.4667])\n",
      "Epoch: 179 | loss: 0.6904954314231873 | test loss: 0.6929895877838135 | accuracy: tensor([0.4667])\n",
      "Epoch: 180 | loss: 0.6904628276824951 | test loss: 0.6929479241371155 | accuracy: tensor([0.4667])\n",
      "Epoch: 181 | loss: 0.6904297471046448 | test loss: 0.692905843257904 | accuracy: tensor([0.4667])\n",
      "Epoch: 182 | loss: 0.6903963088989258 | test loss: 0.6928634643554688 | accuracy: tensor([0.4667])\n",
      "Epoch: 183 | loss: 0.6903623938560486 | test loss: 0.69282066822052 | accuracy: tensor([0.4667])\n",
      "Epoch: 184 | loss: 0.6903281211853027 | test loss: 0.6927775740623474 | accuracy: tensor([0.4667])\n",
      "Epoch: 185 | loss: 0.6902933716773987 | test loss: 0.692733883857727 | accuracy: tensor([0.4667])\n",
      "Epoch: 186 | loss: 0.6902581453323364 | test loss: 0.692689836025238 | accuracy: tensor([0.4667])\n",
      "Epoch: 187 | loss: 0.6902225017547607 | test loss: 0.6926453113555908 | accuracy: tensor([0.4667])\n",
      "Epoch: 188 | loss: 0.6901865005493164 | test loss: 0.6926003098487854 | accuracy: tensor([0.4667])\n",
      "Epoch: 189 | loss: 0.6901499032974243 | test loss: 0.6925548315048218 | accuracy: tensor([0.4667])\n",
      "Epoch: 190 | loss: 0.6901128888130188 | test loss: 0.6925088167190552 | accuracy: tensor([0.4667])\n",
      "Epoch: 191 | loss: 0.6900753378868103 | test loss: 0.6924623250961304 | accuracy: tensor([0.4667])\n",
      "Epoch: 192 | loss: 0.6900373697280884 | test loss: 0.6924153566360474 | accuracy: tensor([0.4667])\n",
      "Epoch: 193 | loss: 0.6899988651275635 | test loss: 0.6923677921295166 | accuracy: tensor([0.4667])\n",
      "Epoch: 194 | loss: 0.6899598836898804 | test loss: 0.6923196911811829 | accuracy: tensor([0.4667])\n",
      "Epoch: 195 | loss: 0.6899204254150391 | test loss: 0.6922709941864014 | accuracy: tensor([0.4667])\n",
      "Epoch: 196 | loss: 0.68988037109375 | test loss: 0.6922217011451721 | accuracy: tensor([0.4667])\n",
      "Epoch: 197 | loss: 0.6898399591445923 | test loss: 0.6921719312667847 | accuracy: tensor([0.4667])\n",
      "Epoch: 198 | loss: 0.6897987723350525 | test loss: 0.6921215057373047 | accuracy: tensor([0.4667])\n",
      "Epoch: 199 | loss: 0.689757227897644 | test loss: 0.6920703053474426 | accuracy: tensor([0.4667])\n",
      "Epoch: 200 | loss: 0.6897149682044983 | test loss: 0.6920186281204224 | accuracy: tensor([0.4667])\n",
      "Epoch: 201 | loss: 0.6896722912788391 | test loss: 0.6919662952423096 | accuracy: tensor([0.4667])\n",
      "Epoch: 202 | loss: 0.6896290183067322 | test loss: 0.6919133067131042 | accuracy: tensor([0.4667])\n",
      "Epoch: 203 | loss: 0.6895852088928223 | test loss: 0.6918595433235168 | accuracy: tensor([0.4667])\n",
      "Epoch: 204 | loss: 0.6895408034324646 | test loss: 0.6918051838874817 | accuracy: tensor([0.4667])\n",
      "Epoch: 205 | loss: 0.6894957423210144 | test loss: 0.6917502880096436 | accuracy: tensor([0.4667])\n",
      "Epoch: 206 | loss: 0.6894500851631165 | test loss: 0.691694438457489 | accuracy: tensor([0.4667])\n",
      "Epoch: 207 | loss: 0.6894038915634155 | test loss: 0.6916379332542419 | accuracy: tensor([0.4667])\n",
      "Epoch: 208 | loss: 0.6893570423126221 | test loss: 0.6915807127952576 | accuracy: tensor([0.4667])\n",
      "Epoch: 209 | loss: 0.6893095970153809 | test loss: 0.6915227770805359 | accuracy: tensor([0.4667])\n",
      "Epoch: 210 | loss: 0.6892614364624023 | test loss: 0.6914641261100769 | accuracy: tensor([0.4667])\n",
      "Epoch: 211 | loss: 0.6892127394676208 | test loss: 0.6914047002792358 | accuracy: tensor([0.4667])\n",
      "Epoch: 212 | loss: 0.6891632080078125 | test loss: 0.6913444399833679 | accuracy: tensor([0.4667])\n",
      "Epoch: 213 | loss: 0.6891131401062012 | test loss: 0.6912834048271179 | accuracy: tensor([0.4667])\n",
      "Epoch: 214 | loss: 0.6890624165534973 | test loss: 0.6912214756011963 | accuracy: tensor([0.4667])\n",
      "Epoch: 215 | loss: 0.6890109181404114 | test loss: 0.6911589503288269 | accuracy: tensor([0.4667])\n",
      "Epoch: 216 | loss: 0.6889588236808777 | test loss: 0.6910953521728516 | accuracy: tensor([0.4667])\n",
      "Epoch: 217 | loss: 0.6889058947563171 | test loss: 0.6910310983657837 | accuracy: tensor([0.4667])\n",
      "Epoch: 218 | loss: 0.6888523697853088 | test loss: 0.6909657716751099 | accuracy: tensor([0.4667])\n",
      "Epoch: 219 | loss: 0.6887980103492737 | test loss: 0.6908998489379883 | accuracy: tensor([0.4667])\n",
      "Epoch: 220 | loss: 0.6887429356575012 | test loss: 0.6908328533172607 | accuracy: tensor([0.4667])\n",
      "Epoch: 221 | loss: 0.6886871457099915 | test loss: 0.6907650232315063 | accuracy: tensor([0.4667])\n",
      "Epoch: 222 | loss: 0.6886305212974548 | test loss: 0.6906962990760803 | accuracy: tensor([0.4667])\n",
      "Epoch: 223 | loss: 0.6885731220245361 | test loss: 0.6906267404556274 | accuracy: tensor([0.4667])\n",
      "Epoch: 224 | loss: 0.6885150074958801 | test loss: 0.690555989742279 | accuracy: tensor([0.4667])\n",
      "Epoch: 225 | loss: 0.6884560585021973 | test loss: 0.6904844641685486 | accuracy: tensor([0.4667])\n",
      "Epoch: 226 | loss: 0.6883962750434875 | test loss: 0.6904119253158569 | accuracy: tensor([0.4667])\n",
      "Epoch: 227 | loss: 0.6883357167243958 | test loss: 0.6903384327888489 | accuracy: tensor([0.4667])\n",
      "Epoch: 228 | loss: 0.6882742047309875 | test loss: 0.6902640461921692 | accuracy: tensor([0.4667])\n",
      "Epoch: 229 | loss: 0.6882119178771973 | test loss: 0.6901885867118835 | accuracy: tensor([0.4667])\n",
      "Epoch: 230 | loss: 0.6881487965583801 | test loss: 0.6901119351387024 | accuracy: tensor([0.4667])\n",
      "Epoch: 231 | loss: 0.6880847215652466 | test loss: 0.6900345087051392 | accuracy: tensor([0.4667])\n",
      "Epoch: 232 | loss: 0.688019871711731 | test loss: 0.6899559497833252 | accuracy: tensor([0.4667])\n",
      "Epoch: 233 | loss: 0.6879539489746094 | test loss: 0.6898762583732605 | accuracy: tensor([0.4667])\n",
      "Epoch: 234 | loss: 0.6878871917724609 | test loss: 0.6897954940795898 | accuracy: tensor([0.4667])\n",
      "Epoch: 235 | loss: 0.6878194808959961 | test loss: 0.6897137761116028 | accuracy: tensor([0.4667])\n",
      "Epoch: 236 | loss: 0.6877508163452148 | test loss: 0.6896308064460754 | accuracy: tensor([0.4667])\n",
      "Epoch: 237 | loss: 0.687681257724762 | test loss: 0.6895466446876526 | accuracy: tensor([0.4667])\n",
      "Epoch: 238 | loss: 0.6876107454299927 | test loss: 0.6894614696502686 | accuracy: tensor([0.4667])\n",
      "Epoch: 239 | loss: 0.6875391006469727 | test loss: 0.6893750429153442 | accuracy: tensor([0.4667])\n",
      "Epoch: 240 | loss: 0.687466561794281 | test loss: 0.6892874836921692 | accuracy: tensor([0.4667])\n",
      "Epoch: 241 | loss: 0.6873928904533386 | test loss: 0.6891987919807434 | accuracy: tensor([0.4667])\n",
      "Epoch: 242 | loss: 0.6873182654380798 | test loss: 0.6891089081764221 | accuracy: tensor([0.4667])\n",
      "Epoch: 243 | loss: 0.6872425675392151 | test loss: 0.6890177130699158 | accuracy: tensor([0.4667])\n",
      "Epoch: 244 | loss: 0.6871657967567444 | test loss: 0.6889253258705139 | accuracy: tensor([0.4667])\n",
      "Epoch: 245 | loss: 0.6870880126953125 | test loss: 0.6888315677642822 | accuracy: tensor([0.4667])\n",
      "Epoch: 246 | loss: 0.6870089769363403 | test loss: 0.688736617565155 | accuracy: tensor([0.4667])\n",
      "Epoch: 247 | loss: 0.686928927898407 | test loss: 0.6886404156684875 | accuracy: tensor([0.4667])\n",
      "Epoch: 248 | loss: 0.6868478059768677 | test loss: 0.688542902469635 | accuracy: tensor([0.4667])\n",
      "Epoch: 249 | loss: 0.6867654323577881 | test loss: 0.6884440183639526 | accuracy: tensor([0.4667])\n",
      "Epoch: 250 | loss: 0.6866819262504578 | test loss: 0.6883437037467957 | accuracy: tensor([0.4667])\n",
      "Epoch: 251 | loss: 0.6865971684455872 | test loss: 0.6882420182228088 | accuracy: tensor([0.4667])\n",
      "Epoch: 252 | loss: 0.6865112781524658 | test loss: 0.6881390810012817 | accuracy: tensor([0.4667])\n",
      "Epoch: 253 | loss: 0.6864241361618042 | test loss: 0.6880345940589905 | accuracy: tensor([0.4667])\n",
      "Epoch: 254 | loss: 0.6863357424736023 | test loss: 0.6879287362098694 | accuracy: tensor([0.4667])\n",
      "Epoch: 255 | loss: 0.6862461566925049 | test loss: 0.6878213882446289 | accuracy: tensor([0.4667])\n",
      "Epoch: 256 | loss: 0.6861552596092224 | test loss: 0.6877126097679138 | accuracy: tensor([0.4667])\n",
      "Epoch: 257 | loss: 0.6860631108283997 | test loss: 0.6876023411750793 | accuracy: tensor([0.4667])\n",
      "Epoch: 258 | loss: 0.6859696507453918 | test loss: 0.6874905824661255 | accuracy: tensor([0.4667])\n",
      "Epoch: 259 | loss: 0.6858748197555542 | test loss: 0.6873772144317627 | accuracy: tensor([0.4667])\n",
      "Epoch: 260 | loss: 0.6857786178588867 | test loss: 0.6872623562812805 | accuracy: tensor([0.4667])\n",
      "Epoch: 261 | loss: 0.6856811046600342 | test loss: 0.6871459484100342 | accuracy: tensor([0.4667])\n",
      "Epoch: 262 | loss: 0.685582160949707 | test loss: 0.6870278716087341 | accuracy: tensor([0.4667])\n",
      "Epoch: 263 | loss: 0.6854817271232605 | test loss: 0.6869080662727356 | accuracy: tensor([0.4667])\n",
      "Epoch: 264 | loss: 0.6853799819946289 | test loss: 0.6867867708206177 | accuracy: tensor([0.4667])\n",
      "Epoch: 265 | loss: 0.6852767467498779 | test loss: 0.686663806438446 | accuracy: tensor([0.4667])\n",
      "Epoch: 266 | loss: 0.6851720809936523 | test loss: 0.6865391135215759 | accuracy: tensor([0.4667])\n",
      "Epoch: 267 | loss: 0.6850658059120178 | test loss: 0.6864128112792969 | accuracy: tensor([0.4667])\n",
      "Epoch: 268 | loss: 0.6849581003189087 | test loss: 0.686284601688385 | accuracy: tensor([0.4667])\n",
      "Epoch: 269 | loss: 0.6848488450050354 | test loss: 0.6861546635627747 | accuracy: tensor([0.4667])\n",
      "Epoch: 270 | loss: 0.684738039970398 | test loss: 0.6860230565071106 | accuracy: tensor([0.4667])\n",
      "Epoch: 271 | loss: 0.6846255660057068 | test loss: 0.6858895421028137 | accuracy: tensor([0.4667])\n",
      "Epoch: 272 | loss: 0.6845116019248962 | test loss: 0.6857542395591736 | accuracy: tensor([0.4667])\n",
      "Epoch: 273 | loss: 0.6843959093093872 | test loss: 0.6856170892715454 | accuracy: tensor([0.4667])\n",
      "Epoch: 274 | loss: 0.6842787265777588 | test loss: 0.6854779720306396 | accuracy: tensor([0.4667])\n",
      "Epoch: 275 | loss: 0.6841596364974976 | test loss: 0.6853370070457458 | accuracy: tensor([0.4667])\n",
      "Epoch: 276 | loss: 0.6840389966964722 | test loss: 0.6851941347122192 | accuracy: tensor([0.4667])\n",
      "Epoch: 277 | loss: 0.683916449546814 | test loss: 0.6850492358207703 | accuracy: tensor([0.4667])\n",
      "Epoch: 278 | loss: 0.6837922930717468 | test loss: 0.6849023103713989 | accuracy: tensor([0.4667])\n",
      "Epoch: 279 | loss: 0.6836663484573364 | test loss: 0.6847535371780396 | accuracy: tensor([0.5000])\n",
      "Epoch: 280 | loss: 0.6835386753082275 | test loss: 0.6846027374267578 | accuracy: tensor([0.5000])\n",
      "Epoch: 281 | loss: 0.6834090352058411 | test loss: 0.6844496130943298 | accuracy: tensor([0.5000])\n",
      "Epoch: 282 | loss: 0.6832775473594666 | test loss: 0.6842946410179138 | accuracy: tensor([0.5000])\n",
      "Epoch: 283 | loss: 0.6831442713737488 | test loss: 0.6841374039649963 | accuracy: tensor([0.5000])\n",
      "Epoch: 284 | loss: 0.6830090284347534 | test loss: 0.6839780807495117 | accuracy: tensor([0.5000])\n",
      "Epoch: 285 | loss: 0.6828718185424805 | test loss: 0.6838166117668152 | accuracy: tensor([0.5000])\n",
      "Epoch: 286 | loss: 0.6827327013015747 | test loss: 0.683652937412262 | accuracy: tensor([0.5000])\n",
      "Epoch: 287 | loss: 0.6825915575027466 | test loss: 0.6834869980812073 | accuracy: tensor([0.5000])\n",
      "Epoch: 288 | loss: 0.6824484467506409 | test loss: 0.6833187341690063 | accuracy: tensor([0.5667])\n",
      "Epoch: 289 | loss: 0.6823033094406128 | test loss: 0.683148205280304 | accuracy: tensor([0.5667])\n",
      "Epoch: 290 | loss: 0.682155966758728 | test loss: 0.6829754710197449 | accuracy: tensor([0.6000])\n",
      "Epoch: 291 | loss: 0.6820065379142761 | test loss: 0.68280029296875 | accuracy: tensor([0.6000])\n",
      "Epoch: 292 | loss: 0.6818550229072571 | test loss: 0.6826227903366089 | accuracy: tensor([0.6000])\n",
      "Epoch: 293 | loss: 0.6817013621330261 | test loss: 0.6824427247047424 | accuracy: tensor([0.6000])\n",
      "Epoch: 294 | loss: 0.6815453767776489 | test loss: 0.682260274887085 | accuracy: tensor([0.6000])\n",
      "Epoch: 295 | loss: 0.6813873052597046 | test loss: 0.6820753812789917 | accuracy: tensor([0.6333])\n",
      "Epoch: 296 | loss: 0.6812268495559692 | test loss: 0.6818879246711731 | accuracy: tensor([0.6333])\n",
      "Epoch: 297 | loss: 0.6810641884803772 | test loss: 0.6816979646682739 | accuracy: tensor([0.6667])\n",
      "Epoch: 298 | loss: 0.6808991432189941 | test loss: 0.6815055012702942 | accuracy: tensor([0.7000])\n",
      "Epoch: 299 | loss: 0.6807317733764648 | test loss: 0.6813103556632996 | accuracy: tensor([0.7000])\n",
      "Epoch: 300 | loss: 0.6805620193481445 | test loss: 0.68111252784729 | accuracy: tensor([0.7000])\n",
      "Epoch: 301 | loss: 0.6803898215293884 | test loss: 0.6809120178222656 | accuracy: tensor([0.7333])\n",
      "Epoch: 302 | loss: 0.680215060710907 | test loss: 0.6807088255882263 | accuracy: tensor([0.7333])\n",
      "Epoch: 303 | loss: 0.6800379157066345 | test loss: 0.6805029511451721 | accuracy: tensor([0.7333])\n",
      "Epoch: 304 | loss: 0.6798582077026367 | test loss: 0.6802940964698792 | accuracy: tensor([0.7333])\n",
      "Epoch: 305 | loss: 0.6796759366989136 | test loss: 0.6800825595855713 | accuracy: tensor([0.7333])\n",
      "Epoch: 306 | loss: 0.6794911026954651 | test loss: 0.6798680424690247 | accuracy: tensor([0.7333])\n",
      "Epoch: 307 | loss: 0.6793035268783569 | test loss: 0.679650604724884 | accuracy: tensor([0.7333])\n",
      "Epoch: 308 | loss: 0.6791133284568787 | test loss: 0.679430365562439 | accuracy: tensor([0.7333])\n",
      "Epoch: 309 | loss: 0.6789204478263855 | test loss: 0.6792070865631104 | accuracy: tensor([0.7667])\n",
      "Epoch: 310 | loss: 0.6787246465682983 | test loss: 0.6789807677268982 | accuracy: tensor([0.8000])\n",
      "Epoch: 311 | loss: 0.6785262227058411 | test loss: 0.6787513494491577 | accuracy: tensor([0.8000])\n",
      "Epoch: 312 | loss: 0.6783248782157898 | test loss: 0.6785188317298889 | accuracy: tensor([0.8000])\n",
      "Epoch: 313 | loss: 0.6781206727027893 | test loss: 0.678283154964447 | accuracy: tensor([0.8333])\n",
      "Epoch: 314 | loss: 0.6779136061668396 | test loss: 0.6780443787574768 | accuracy: tensor([0.8333])\n",
      "Epoch: 315 | loss: 0.6777034997940063 | test loss: 0.6778022050857544 | accuracy: tensor([0.8333])\n",
      "Epoch: 316 | loss: 0.6774904727935791 | test loss: 0.6775568127632141 | accuracy: tensor([0.8333])\n",
      "Epoch: 317 | loss: 0.6772743463516235 | test loss: 0.6773081421852112 | accuracy: tensor([0.8333])\n",
      "Epoch: 318 | loss: 0.6770550608634949 | test loss: 0.677056074142456 | accuracy: tensor([0.8333])\n",
      "Epoch: 319 | loss: 0.6768327951431274 | test loss: 0.6768004894256592 | accuracy: tensor([0.8333])\n",
      "Epoch: 320 | loss: 0.6766073107719421 | test loss: 0.6765414476394653 | accuracy: tensor([0.8333])\n",
      "Epoch: 321 | loss: 0.6763785481452942 | test loss: 0.6762790679931641 | accuracy: tensor([0.8333])\n",
      "Epoch: 322 | loss: 0.6761466264724731 | test loss: 0.6760129332542419 | accuracy: tensor([0.8333])\n",
      "Epoch: 323 | loss: 0.6759112477302551 | test loss: 0.6757432222366333 | accuracy: tensor([0.8333])\n",
      "Epoch: 324 | loss: 0.6756726503372192 | test loss: 0.6754699349403381 | accuracy: tensor([0.8333])\n",
      "Epoch: 325 | loss: 0.6754305958747864 | test loss: 0.6751927137374878 | accuracy: tensor([0.8333])\n",
      "Epoch: 326 | loss: 0.6751851439476013 | test loss: 0.6749119162559509 | accuracy: tensor([0.8333])\n",
      "Epoch: 327 | loss: 0.6749361157417297 | test loss: 0.6746272444725037 | accuracy: tensor([0.8333])\n",
      "Epoch: 328 | loss: 0.6746836304664612 | test loss: 0.6743386387825012 | accuracy: tensor([0.8333])\n",
      "Epoch: 329 | loss: 0.6744274497032166 | test loss: 0.6740461587905884 | accuracy: tensor([0.8333])\n",
      "Epoch: 330 | loss: 0.6741676926612854 | test loss: 0.6737498044967651 | accuracy: tensor([0.8333])\n",
      "Epoch: 331 | loss: 0.6739041805267334 | test loss: 0.6734493374824524 | accuracy: tensor([0.8333])\n",
      "Epoch: 332 | loss: 0.6736370325088501 | test loss: 0.6731446385383606 | accuracy: tensor([0.8333])\n",
      "Epoch: 333 | loss: 0.6733660101890564 | test loss: 0.6728359460830688 | accuracy: tensor([0.8333])\n",
      "Epoch: 334 | loss: 0.6730910539627075 | test loss: 0.6725229620933533 | accuracy: tensor([0.8333])\n",
      "Epoch: 335 | loss: 0.672812283039093 | test loss: 0.6722057461738586 | accuracy: tensor([0.8667])\n",
      "Epoch: 336 | loss: 0.6725295186042786 | test loss: 0.6718842387199402 | accuracy: tensor([0.8667])\n",
      "Epoch: 337 | loss: 0.6722428202629089 | test loss: 0.6715582609176636 | accuracy: tensor([0.8667])\n",
      "Epoch: 338 | loss: 0.6719518899917603 | test loss: 0.6712278723716736 | accuracy: tensor([0.8667])\n",
      "Epoch: 339 | loss: 0.6716569662094116 | test loss: 0.6708929538726807 | accuracy: tensor([0.8667])\n",
      "Epoch: 340 | loss: 0.6713577508926392 | test loss: 0.6705535650253296 | accuracy: tensor([0.9000])\n",
      "Epoch: 341 | loss: 0.6710543036460876 | test loss: 0.6702094674110413 | accuracy: tensor([0.9000])\n",
      "Epoch: 342 | loss: 0.6707466840744019 | test loss: 0.6698606610298157 | accuracy: tensor([0.9333])\n",
      "Epoch: 343 | loss: 0.6704345941543579 | test loss: 0.6695070862770081 | accuracy: tensor([0.9333])\n",
      "Epoch: 344 | loss: 0.6701181530952454 | test loss: 0.6691487431526184 | accuracy: tensor([0.9333])\n",
      "Epoch: 345 | loss: 0.6697970628738403 | test loss: 0.6687855124473572 | accuracy: tensor([0.9333])\n",
      "Epoch: 346 | loss: 0.6694716215133667 | test loss: 0.6684172749519348 | accuracy: tensor([0.9333])\n",
      "Epoch: 347 | loss: 0.6691414713859558 | test loss: 0.6680440306663513 | accuracy: tensor([0.9333])\n",
      "Epoch: 348 | loss: 0.668806791305542 | test loss: 0.6676656603813171 | accuracy: tensor([0.9333])\n",
      "Epoch: 349 | loss: 0.6684672832489014 | test loss: 0.667282223701477 | accuracy: tensor([0.9333])\n",
      "Epoch: 350 | loss: 0.6681229472160339 | test loss: 0.6668936610221863 | accuracy: tensor([0.9333])\n",
      "Epoch: 351 | loss: 0.6677738428115845 | test loss: 0.6664994955062866 | accuracy: tensor([0.9333])\n",
      "Epoch: 352 | loss: 0.6674197316169739 | test loss: 0.6661002039909363 | accuracy: tensor([0.9667])\n",
      "Epoch: 353 | loss: 0.6670607924461365 | test loss: 0.6656953692436218 | accuracy: tensor([0.9667])\n",
      "Epoch: 354 | loss: 0.6666966080665588 | test loss: 0.6652851104736328 | accuracy: tensor([0.9667])\n",
      "Epoch: 355 | loss: 0.6663274765014648 | test loss: 0.6648692488670349 | accuracy: tensor([0.9667])\n",
      "Epoch: 356 | loss: 0.6659530401229858 | test loss: 0.6644477844238281 | accuracy: tensor([0.9667])\n",
      "Epoch: 357 | loss: 0.6655734181404114 | test loss: 0.6640204787254333 | accuracy: tensor([0.9667])\n",
      "Epoch: 358 | loss: 0.6651884317398071 | test loss: 0.6635874509811401 | accuracy: tensor([0.9667])\n",
      "Epoch: 359 | loss: 0.6647980809211731 | test loss: 0.6631486415863037 | accuracy: tensor([0.9667])\n",
      "Epoch: 360 | loss: 0.6644023060798645 | test loss: 0.6627036929130554 | accuracy: tensor([0.9667])\n",
      "Epoch: 361 | loss: 0.6640009880065918 | test loss: 0.6622528433799744 | accuracy: tensor([0.9667])\n",
      "Epoch: 362 | loss: 0.6635940670967102 | test loss: 0.6617958545684814 | accuracy: tensor([0.9667])\n",
      "Epoch: 363 | loss: 0.6631814241409302 | test loss: 0.6613327860832214 | accuracy: tensor([0.9667])\n",
      "Epoch: 364 | loss: 0.6627631783485413 | test loss: 0.6608632802963257 | accuracy: tensor([0.9667])\n",
      "Epoch: 365 | loss: 0.6623390316963196 | test loss: 0.6603876352310181 | accuracy: tensor([0.9667])\n",
      "Epoch: 366 | loss: 0.6619089841842651 | test loss: 0.6599054932594299 | accuracy: tensor([0.9667])\n",
      "Epoch: 367 | loss: 0.6614730954170227 | test loss: 0.659416913986206 | accuracy: tensor([0.9667])\n",
      "Epoch: 368 | loss: 0.6610310673713684 | test loss: 0.6589218378067017 | accuracy: tensor([0.9667])\n",
      "Epoch: 369 | loss: 0.6605830192565918 | test loss: 0.6584199070930481 | accuracy: tensor([0.9667])\n",
      "Epoch: 370 | loss: 0.6601287722587585 | test loss: 0.6579114198684692 | accuracy: tensor([0.9667])\n",
      "Epoch: 371 | loss: 0.6596683263778687 | test loss: 0.657396137714386 | accuracy: tensor([0.9667])\n",
      "Epoch: 372 | loss: 0.6592015624046326 | test loss: 0.6568739414215088 | accuracy: tensor([0.9667])\n",
      "Epoch: 373 | loss: 0.6587283611297607 | test loss: 0.6563447713851929 | accuracy: tensor([0.9667])\n",
      "Epoch: 374 | loss: 0.6582487225532532 | test loss: 0.6558086276054382 | accuracy: tensor([0.9667])\n",
      "Epoch: 375 | loss: 0.6577625274658203 | test loss: 0.6552653908729553 | accuracy: tensor([0.9667])\n",
      "Epoch: 376 | loss: 0.6572697758674622 | test loss: 0.6547148823738098 | accuracy: tensor([0.9667])\n",
      "Epoch: 377 | loss: 0.6567702889442444 | test loss: 0.6541570425033569 | accuracy: tensor([0.9667])\n",
      "Epoch: 378 | loss: 0.6562641263008118 | test loss: 0.6535919308662415 | accuracy: tensor([0.9667])\n",
      "Epoch: 379 | loss: 0.6557510495185852 | test loss: 0.6530193090438843 | accuracy: tensor([0.9667])\n",
      "Epoch: 380 | loss: 0.6552310585975647 | test loss: 0.6524392366409302 | accuracy: tensor([0.9667])\n",
      "Epoch: 381 | loss: 0.6547040939331055 | test loss: 0.6518515348434448 | accuracy: tensor([0.9667])\n",
      "Epoch: 382 | loss: 0.6541700959205627 | test loss: 0.6512561440467834 | accuracy: tensor([0.9667])\n",
      "Epoch: 383 | loss: 0.6536290049552917 | test loss: 0.6506529450416565 | accuracy: tensor([0.9667])\n",
      "Epoch: 384 | loss: 0.6530806422233582 | test loss: 0.650041937828064 | accuracy: tensor([0.9667])\n",
      "Epoch: 385 | loss: 0.6525250673294067 | test loss: 0.6494230031967163 | accuracy: tensor([0.9667])\n",
      "Epoch: 386 | loss: 0.651962161064148 | test loss: 0.648796021938324 | accuracy: tensor([0.9667])\n",
      "Epoch: 387 | loss: 0.6513917446136475 | test loss: 0.6481609344482422 | accuracy: tensor([0.9667])\n",
      "Epoch: 388 | loss: 0.6508138179779053 | test loss: 0.647517740726471 | accuracy: tensor([0.9667])\n",
      "Epoch: 389 | loss: 0.6502283811569214 | test loss: 0.6468661427497864 | accuracy: tensor([0.9667])\n",
      "Epoch: 390 | loss: 0.6496353149414062 | test loss: 0.6462064385414124 | accuracy: tensor([0.9667])\n",
      "Epoch: 391 | loss: 0.6490345001220703 | test loss: 0.6455380916595459 | accuracy: tensor([0.9667])\n",
      "Epoch: 392 | loss: 0.6484259366989136 | test loss: 0.6448614001274109 | accuracy: tensor([0.9667])\n",
      "Epoch: 393 | loss: 0.6478094458580017 | test loss: 0.6441760659217834 | accuracy: tensor([0.9667])\n",
      "Epoch: 394 | loss: 0.6471850872039795 | test loss: 0.6434821486473083 | accuracy: tensor([0.9667])\n",
      "Epoch: 395 | loss: 0.6465527415275574 | test loss: 0.6427794098854065 | accuracy: tensor([0.9667])\n",
      "Epoch: 396 | loss: 0.6459122896194458 | test loss: 0.6420679092407227 | accuracy: tensor([0.9667])\n",
      "Epoch: 397 | loss: 0.645263671875 | test loss: 0.6413475871086121 | accuracy: tensor([0.9667])\n",
      "Epoch: 398 | loss: 0.6446069478988647 | test loss: 0.6406182646751404 | accuracy: tensor([0.9667])\n",
      "Epoch: 399 | loss: 0.6439418196678162 | test loss: 0.6398798823356628 | accuracy: tensor([0.9667])\n",
      "Epoch: 400 | loss: 0.6432685256004333 | test loss: 0.6391324400901794 | accuracy: tensor([0.9667])\n",
      "Epoch: 401 | loss: 0.6425867676734924 | test loss: 0.6383758783340454 | accuracy: tensor([0.9667])\n",
      "Epoch: 402 | loss: 0.6418965458869934 | test loss: 0.6376100182533264 | accuracy: tensor([0.9667])\n",
      "Epoch: 403 | loss: 0.641197919845581 | test loss: 0.6368348598480225 | accuracy: tensor([0.9667])\n",
      "Epoch: 404 | loss: 0.6404905915260315 | test loss: 0.6360503435134888 | accuracy: tensor([0.9667])\n",
      "Epoch: 405 | loss: 0.6397746801376343 | test loss: 0.6352564692497253 | accuracy: tensor([0.9667])\n",
      "Epoch: 406 | loss: 0.6390500664710999 | test loss: 0.6344529986381531 | accuracy: tensor([0.9667])\n",
      "Epoch: 407 | loss: 0.6383167505264282 | test loss: 0.6336400508880615 | accuracy: tensor([0.9667])\n",
      "Epoch: 408 | loss: 0.6375746726989746 | test loss: 0.6328174471855164 | accuracy: tensor([0.9667])\n",
      "Epoch: 409 | loss: 0.6368237137794495 | test loss: 0.6319852471351624 | accuracy: tensor([0.9667])\n",
      "Epoch: 410 | loss: 0.6360638737678528 | test loss: 0.63114333152771 | accuracy: tensor([0.9667])\n",
      "Epoch: 411 | loss: 0.6352950930595398 | test loss: 0.6302916407585144 | accuracy: tensor([0.9667])\n",
      "Epoch: 412 | loss: 0.6345173716545105 | test loss: 0.6294300556182861 | accuracy: tensor([0.9667])\n",
      "Epoch: 413 | loss: 0.6337305307388306 | test loss: 0.6285586953163147 | accuracy: tensor([0.9667])\n",
      "Epoch: 414 | loss: 0.6329346895217896 | test loss: 0.6276773810386658 | accuracy: tensor([0.9667])\n",
      "Epoch: 415 | loss: 0.6321298480033875 | test loss: 0.6267861723899841 | accuracy: tensor([0.9667])\n",
      "Epoch: 416 | loss: 0.6313157081604004 | test loss: 0.6258849501609802 | accuracy: tensor([0.9667])\n",
      "Epoch: 417 | loss: 0.6304925084114075 | test loss: 0.624973714351654 | accuracy: tensor([0.9667])\n",
      "Epoch: 418 | loss: 0.6296600699424744 | test loss: 0.6240524649620056 | accuracy: tensor([0.9667])\n",
      "Epoch: 419 | loss: 0.6288183927536011 | test loss: 0.6231210231781006 | accuracy: tensor([0.9667])\n",
      "Epoch: 420 | loss: 0.6279675364494324 | test loss: 0.6221796274185181 | accuracy: tensor([0.9667])\n",
      "Epoch: 421 | loss: 0.6271073222160339 | test loss: 0.621228039264679 | accuracy: tensor([0.9667])\n",
      "Epoch: 422 | loss: 0.6262378692626953 | test loss: 0.6202662587165833 | accuracy: tensor([0.9667])\n",
      "Epoch: 423 | loss: 0.6253591179847717 | test loss: 0.6192943453788757 | accuracy: tensor([0.9667])\n",
      "Epoch: 424 | loss: 0.624471127986908 | test loss: 0.6183122992515564 | accuracy: tensor([0.9667])\n",
      "Epoch: 425 | loss: 0.6235736012458801 | test loss: 0.6173200607299805 | accuracy: tensor([0.9667])\n",
      "Epoch: 426 | loss: 0.6226667761802673 | test loss: 0.6163175702095032 | accuracy: tensor([0.9667])\n",
      "Epoch: 427 | loss: 0.6217506527900696 | test loss: 0.6153048872947693 | accuracy: tensor([0.9667])\n",
      "Epoch: 428 | loss: 0.6208251118659973 | test loss: 0.614281952381134 | accuracy: tensor([0.9667])\n",
      "Epoch: 429 | loss: 0.6198903322219849 | test loss: 0.613248884677887 | accuracy: tensor([0.9667])\n",
      "Epoch: 430 | loss: 0.6189461350440979 | test loss: 0.6122056245803833 | accuracy: tensor([0.9667])\n",
      "Epoch: 431 | loss: 0.6179925203323364 | test loss: 0.6111521124839783 | accuracy: tensor([0.9667])\n",
      "Epoch: 432 | loss: 0.6170296669006348 | test loss: 0.6100885272026062 | accuracy: tensor([0.9667])\n",
      "Epoch: 433 | loss: 0.6160574555397034 | test loss: 0.6090146899223328 | accuracy: tensor([0.9667])\n",
      "Epoch: 434 | loss: 0.615075945854187 | test loss: 0.6079308986663818 | accuracy: tensor([0.9667])\n",
      "Epoch: 435 | loss: 0.6140851974487305 | test loss: 0.6068368554115295 | accuracy: tensor([0.9667])\n",
      "Epoch: 436 | loss: 0.6130850911140442 | test loss: 0.6057327389717102 | accuracy: tensor([0.9667])\n",
      "Epoch: 437 | loss: 0.6120757460594177 | test loss: 0.604618489742279 | accuracy: tensor([0.9667])\n",
      "Epoch: 438 | loss: 0.6110571622848511 | test loss: 0.60349440574646 | accuracy: tensor([0.9667])\n",
      "Epoch: 439 | loss: 0.610029399394989 | test loss: 0.6023601293563843 | accuracy: tensor([0.9667])\n",
      "Epoch: 440 | loss: 0.6089925765991211 | test loss: 0.6012161374092102 | accuracy: tensor([0.9667])\n",
      "Epoch: 441 | loss: 0.6079465746879578 | test loss: 0.6000621914863586 | accuracy: tensor([0.9667])\n",
      "Epoch: 442 | loss: 0.606891393661499 | test loss: 0.5988982915878296 | accuracy: tensor([0.9667])\n",
      "Epoch: 443 | loss: 0.6058273315429688 | test loss: 0.5977246761322021 | accuracy: tensor([0.9667])\n",
      "Epoch: 444 | loss: 0.6047542691230774 | test loss: 0.5965414047241211 | accuracy: tensor([0.9667])\n",
      "Epoch: 445 | loss: 0.603672206401825 | test loss: 0.5953483581542969 | accuracy: tensor([0.9667])\n",
      "Epoch: 446 | loss: 0.602581262588501 | test loss: 0.5941457152366638 | accuracy: tensor([0.9667])\n",
      "Epoch: 447 | loss: 0.6014814972877502 | test loss: 0.5929335355758667 | accuracy: tensor([0.9667])\n",
      "Epoch: 448 | loss: 0.6003730297088623 | test loss: 0.5917119383811951 | accuracy: tensor([0.9667])\n",
      "Epoch: 449 | loss: 0.5992559194564819 | test loss: 0.5904808640480042 | accuracy: tensor([0.9667])\n",
      "Epoch: 450 | loss: 0.5981300473213196 | test loss: 0.5892404317855835 | accuracy: tensor([0.9667])\n",
      "Epoch: 451 | loss: 0.5969956517219543 | test loss: 0.5879908800125122 | accuracy: tensor([0.9667])\n",
      "Epoch: 452 | loss: 0.5958528518676758 | test loss: 0.5867321491241455 | accuracy: tensor([0.9667])\n",
      "Epoch: 453 | loss: 0.5947015285491943 | test loss: 0.585464358329773 | accuracy: tensor([0.9667])\n",
      "Epoch: 454 | loss: 0.5935419797897339 | test loss: 0.5841875672340393 | accuracy: tensor([0.9667])\n",
      "Epoch: 455 | loss: 0.5923742055892944 | test loss: 0.5829019546508789 | accuracy: tensor([0.9667])\n",
      "Epoch: 456 | loss: 0.591198205947876 | test loss: 0.5816075205802917 | accuracy: tensor([0.9667])\n",
      "Epoch: 457 | loss: 0.5900140404701233 | test loss: 0.5803043246269226 | accuracy: tensor([0.9667])\n",
      "Epoch: 458 | loss: 0.5888220071792603 | test loss: 0.5789926648139954 | accuracy: tensor([0.9667])\n",
      "Epoch: 459 | loss: 0.5876220464706421 | test loss: 0.5776724219322205 | accuracy: tensor([0.9667])\n",
      "Epoch: 460 | loss: 0.5864143371582031 | test loss: 0.5763438940048218 | accuracy: tensor([0.9667])\n",
      "Epoch: 461 | loss: 0.5851990580558777 | test loss: 0.5750071406364441 | accuracy: tensor([0.9667])\n",
      "Epoch: 462 | loss: 0.5839760303497314 | test loss: 0.5736622214317322 | accuracy: tensor([0.9667])\n",
      "Epoch: 463 | loss: 0.5827455520629883 | test loss: 0.5723093748092651 | accuracy: tensor([0.9667])\n",
      "Epoch: 464 | loss: 0.5815077424049377 | test loss: 0.5709486603736877 | accuracy: tensor([0.9667])\n",
      "Epoch: 465 | loss: 0.5802626609802246 | test loss: 0.569580078125 | accuracy: tensor([0.9667])\n",
      "Epoch: 466 | loss: 0.5790104269981384 | test loss: 0.5682039260864258 | accuracy: tensor([0.9667])\n",
      "Epoch: 467 | loss: 0.577751100063324 | test loss: 0.5668202638626099 | accuracy: tensor([0.9667])\n",
      "Epoch: 468 | loss: 0.5764849781990051 | test loss: 0.5654292702674866 | accuracy: tensor([0.9667])\n",
      "Epoch: 469 | loss: 0.5752120614051819 | test loss: 0.5640310049057007 | accuracy: tensor([0.9667])\n",
      "Epoch: 470 | loss: 0.573932409286499 | test loss: 0.5626257061958313 | accuracy: tensor([0.9667])\n",
      "Epoch: 471 | loss: 0.5726463198661804 | test loss: 0.5612133741378784 | accuracy: tensor([0.9667])\n",
      "Epoch: 472 | loss: 0.5713537335395813 | test loss: 0.5597942471504211 | accuracy: tensor([0.9667])\n",
      "Epoch: 473 | loss: 0.5700549483299255 | test loss: 0.5583685040473938 | accuracy: tensor([0.9667])\n",
      "Epoch: 474 | loss: 0.5687500238418579 | test loss: 0.5569362640380859 | accuracy: tensor([0.9667])\n",
      "Epoch: 475 | loss: 0.567439079284668 | test loss: 0.5554977059364319 | accuracy: tensor([0.9667])\n",
      "Epoch: 476 | loss: 0.5661221742630005 | test loss: 0.5540527105331421 | accuracy: tensor([0.9667])\n",
      "Epoch: 477 | loss: 0.5647996068000793 | test loss: 0.5526018738746643 | accuracy: tensor([0.9667])\n",
      "Epoch: 478 | loss: 0.5634714365005493 | test loss: 0.5511450171470642 | accuracy: tensor([0.9667])\n",
      "Epoch: 479 | loss: 0.5621378421783447 | test loss: 0.5496824979782104 | accuracy: tensor([0.9667])\n",
      "Epoch: 480 | loss: 0.5607988834381104 | test loss: 0.548214316368103 | accuracy: tensor([0.9667])\n",
      "Epoch: 481 | loss: 0.5594547986984253 | test loss: 0.546740710735321 | accuracy: tensor([0.9667])\n",
      "Epoch: 482 | loss: 0.5581057071685791 | test loss: 0.545261800289154 | accuracy: tensor([0.9667])\n",
      "Epoch: 483 | loss: 0.5567516684532166 | test loss: 0.5437778234481812 | accuracy: tensor([0.9667])\n",
      "Epoch: 484 | loss: 0.5553929209709167 | test loss: 0.5422889590263367 | accuracy: tensor([0.9667])\n",
      "Epoch: 485 | loss: 0.554029643535614 | test loss: 0.5407952666282654 | accuracy: tensor([0.9667])\n",
      "Epoch: 486 | loss: 0.5526618361473083 | test loss: 0.5392969250679016 | accuracy: tensor([0.9667])\n",
      "Epoch: 487 | loss: 0.5512899160385132 | test loss: 0.5377940535545349 | accuracy: tensor([0.9667])\n",
      "Epoch: 488 | loss: 0.549913763999939 | test loss: 0.5362870097160339 | accuracy: tensor([0.9667])\n",
      "Epoch: 489 | loss: 0.5485336184501648 | test loss: 0.5347757935523987 | accuracy: tensor([0.9667])\n",
      "Epoch: 490 | loss: 0.5471498370170593 | test loss: 0.5332606434822083 | accuracy: tensor([0.9667])\n",
      "Epoch: 491 | loss: 0.5457621216773987 | test loss: 0.5317416787147522 | accuracy: tensor([0.9667])\n",
      "Epoch: 492 | loss: 0.5443710684776306 | test loss: 0.5302190780639648 | accuracy: tensor([0.9667])\n",
      "Epoch: 493 | loss: 0.5429766178131104 | test loss: 0.5286930203437805 | accuracy: tensor([0.9667])\n",
      "Epoch: 494 | loss: 0.541579008102417 | test loss: 0.5271637439727783 | accuracy: tensor([0.9667])\n",
      "Epoch: 495 | loss: 0.5401783585548401 | test loss: 0.525631308555603 | accuracy: tensor([0.9667])\n",
      "Epoch: 496 | loss: 0.5387747287750244 | test loss: 0.5240958333015442 | accuracy: tensor([0.9667])\n",
      "Epoch: 497 | loss: 0.5373684763908386 | test loss: 0.5225576758384705 | accuracy: tensor([0.9667])\n",
      "Epoch: 498 | loss: 0.5359595417976379 | test loss: 0.5210169553756714 | accuracy: tensor([0.9667])\n",
      "Epoch: 499 | loss: 0.5345482230186462 | test loss: 0.5194737315177917 | accuracy: tensor([0.9667])\n",
      "Epoch: 500 | loss: 0.5331346988677979 | test loss: 0.5179282426834106 | accuracy: tensor([0.9667])\n",
      "Epoch: 501 | loss: 0.5317190289497375 | test loss: 0.5163806080818176 | accuracy: tensor([0.9667])\n",
      "Epoch: 502 | loss: 0.5303014516830444 | test loss: 0.5148310661315918 | accuracy: tensor([0.9667])\n",
      "Epoch: 503 | loss: 0.5288820266723633 | test loss: 0.5132797360420227 | accuracy: tensor([0.9667])\n",
      "Epoch: 504 | loss: 0.5274609327316284 | test loss: 0.5117267370223999 | accuracy: tensor([0.9667])\n",
      "Epoch: 505 | loss: 0.5260383486747742 | test loss: 0.510172426700592 | accuracy: tensor([0.9667])\n",
      "Epoch: 506 | loss: 0.5246145725250244 | test loss: 0.5086166858673096 | accuracy: tensor([0.9667])\n",
      "Epoch: 507 | loss: 0.5231894254684448 | test loss: 0.5070598721504211 | accuracy: tensor([0.9667])\n",
      "Epoch: 508 | loss: 0.521763265132904 | test loss: 0.5055021643638611 | accuracy: tensor([0.9667])\n",
      "Epoch: 509 | loss: 0.520336389541626 | test loss: 0.5039435625076294 | accuracy: tensor([0.9667])\n",
      "Epoch: 510 | loss: 0.518908679485321 | test loss: 0.5023842453956604 | accuracy: tensor([0.9667])\n",
      "Epoch: 511 | loss: 0.5174804329872131 | test loss: 0.5008245706558228 | accuracy: tensor([0.9667])\n",
      "Epoch: 512 | loss: 0.516051709651947 | test loss: 0.4992646276950836 | accuracy: tensor([0.9667])\n",
      "Epoch: 513 | loss: 0.514622688293457 | test loss: 0.4977043867111206 | accuracy: tensor([0.9667])\n",
      "Epoch: 514 | loss: 0.5131935477256775 | test loss: 0.49614417552948 | accuracy: tensor([0.9667])\n",
      "Epoch: 515 | loss: 0.511764407157898 | test loss: 0.4945840835571289 | accuracy: tensor([0.9667])\n",
      "Epoch: 516 | loss: 0.510335385799408 | test loss: 0.49302420020103455 | accuracy: tensor([0.9667])\n",
      "Epoch: 517 | loss: 0.5089067220687866 | test loss: 0.4914648234844208 | accuracy: tensor([0.9667])\n",
      "Epoch: 518 | loss: 0.5074784755706787 | test loss: 0.4899060130119324 | accuracy: tensor([0.9667])\n",
      "Epoch: 519 | loss: 0.5060508251190186 | test loss: 0.48834794759750366 | accuracy: tensor([0.9667])\n",
      "Epoch: 520 | loss: 0.5046238899230957 | test loss: 0.4867907464504242 | accuracy: tensor([0.9667])\n",
      "Epoch: 521 | loss: 0.5031976699829102 | test loss: 0.4852345883846283 | accuracy: tensor([0.9667])\n",
      "Epoch: 522 | loss: 0.5017725229263306 | test loss: 0.48367950320243835 | accuracy: tensor([0.9667])\n",
      "Epoch: 523 | loss: 0.5003485679626465 | test loss: 0.48212575912475586 | accuracy: tensor([0.9667])\n",
      "Epoch: 524 | loss: 0.4989257752895355 | test loss: 0.48057347536087036 | accuracy: tensor([0.9667])\n",
      "Epoch: 525 | loss: 0.4975043833255768 | test loss: 0.4790225923061371 | accuracy: tensor([0.9667])\n",
      "Epoch: 526 | loss: 0.4960845410823822 | test loss: 0.47747355699539185 | accuracy: tensor([0.9667])\n",
      "Epoch: 527 | loss: 0.4946662485599518 | test loss: 0.47592633962631226 | accuracy: tensor([0.9667])\n",
      "Epoch: 528 | loss: 0.493249773979187 | test loss: 0.4743810296058655 | accuracy: tensor([0.9667])\n",
      "Epoch: 529 | loss: 0.49183517694473267 | test loss: 0.47283780574798584 | accuracy: tensor([0.9667])\n",
      "Epoch: 530 | loss: 0.4904226064682007 | test loss: 0.4712967276573181 | accuracy: tensor([0.9667])\n",
      "Epoch: 531 | loss: 0.4890121519565582 | test loss: 0.4697580635547638 | accuracy: tensor([0.9667])\n",
      "Epoch: 532 | loss: 0.48760393261909485 | test loss: 0.4682218134403229 | accuracy: tensor([0.9667])\n",
      "Epoch: 533 | loss: 0.4861980974674225 | test loss: 0.4666881561279297 | accuracy: tensor([0.9667])\n",
      "Epoch: 534 | loss: 0.4847946763038635 | test loss: 0.4651572108268738 | accuracy: tensor([0.9667])\n",
      "Epoch: 535 | loss: 0.48339390754699707 | test loss: 0.4636290371417999 | accuracy: tensor([0.9667])\n",
      "Epoch: 536 | loss: 0.4819958209991455 | test loss: 0.4621037244796753 | accuracy: tensor([0.9667])\n",
      "Epoch: 537 | loss: 0.480600506067276 | test loss: 0.4605814516544342 | accuracy: tensor([0.9667])\n",
      "Epoch: 538 | loss: 0.4792081415653229 | test loss: 0.45906227827072144 | accuracy: tensor([0.9667])\n",
      "Epoch: 539 | loss: 0.47781872749328613 | test loss: 0.4575464129447937 | accuracy: tensor([0.9667])\n",
      "Epoch: 540 | loss: 0.47643253207206726 | test loss: 0.4560338854789734 | accuracy: tensor([0.9667])\n",
      "Epoch: 541 | loss: 0.47504952549934387 | test loss: 0.45452478528022766 | accuracy: tensor([0.9667])\n",
      "Epoch: 542 | loss: 0.4736698269844055 | test loss: 0.4530191719532013 | accuracy: tensor([0.9667])\n",
      "Epoch: 543 | loss: 0.47229358553886414 | test loss: 0.4515172243118286 | accuracy: tensor([0.9667])\n",
      "Epoch: 544 | loss: 0.47092077136039734 | test loss: 0.45001891255378723 | accuracy: tensor([0.9667])\n",
      "Epoch: 545 | loss: 0.469551682472229 | test loss: 0.44852450489997864 | accuracy: tensor([0.9667])\n",
      "Epoch: 546 | loss: 0.4681861996650696 | test loss: 0.44703400135040283 | accuracy: tensor([0.9667])\n",
      "Epoch: 547 | loss: 0.4668244421482086 | test loss: 0.445547491312027 | accuracy: tensor([0.9667])\n",
      "Epoch: 548 | loss: 0.4654666781425476 | test loss: 0.44406503438949585 | accuracy: tensor([0.9667])\n",
      "Epoch: 549 | loss: 0.464112788438797 | test loss: 0.442586749792099 | accuracy: tensor([0.9667])\n",
      "Epoch: 550 | loss: 0.4627629816532135 | test loss: 0.4411127269268036 | accuracy: tensor([0.9667])\n",
      "Epoch: 551 | loss: 0.46141722798347473 | test loss: 0.4396430253982544 | accuracy: tensor([0.9667])\n",
      "Epoch: 552 | loss: 0.4600757658481598 | test loss: 0.4381777346134186 | accuracy: tensor([0.9667])\n",
      "Epoch: 553 | loss: 0.4587384760379791 | test loss: 0.4367169141769409 | accuracy: tensor([0.9667])\n",
      "Epoch: 554 | loss: 0.45740553736686707 | test loss: 0.4352606534957886 | accuracy: tensor([0.9667])\n",
      "Epoch: 555 | loss: 0.45607703924179077 | test loss: 0.4338090121746063 | accuracy: tensor([0.9667])\n",
      "Epoch: 556 | loss: 0.45475301146507263 | test loss: 0.43236204981803894 | accuracy: tensor([0.9667])\n",
      "Epoch: 557 | loss: 0.4534335732460022 | test loss: 0.4309197962284088 | accuracy: tensor([0.9667])\n",
      "Epoch: 558 | loss: 0.45211872458457947 | test loss: 0.42948246002197266 | accuracy: tensor([0.9667])\n",
      "Epoch: 559 | loss: 0.4508085548877716 | test loss: 0.4280499219894409 | accuracy: tensor([0.9667])\n",
      "Epoch: 560 | loss: 0.44950318336486816 | test loss: 0.42662233114242554 | accuracy: tensor([0.9667])\n",
      "Epoch: 561 | loss: 0.44820258021354675 | test loss: 0.42519980669021606 | accuracy: tensor([0.9667])\n",
      "Epoch: 562 | loss: 0.44690683484077454 | test loss: 0.42378225922584534 | accuracy: tensor([0.9667])\n",
      "Epoch: 563 | loss: 0.4456160068511963 | test loss: 0.4223698377609253 | accuracy: tensor([0.9667])\n",
      "Epoch: 564 | loss: 0.4443301558494568 | test loss: 0.4209626019001007 | accuracy: tensor([0.9667])\n",
      "Epoch: 565 | loss: 0.4430493414402008 | test loss: 0.4195605516433716 | accuracy: tensor([0.9667])\n",
      "Epoch: 566 | loss: 0.4417736232280731 | test loss: 0.41816383600234985 | accuracy: tensor([0.9667])\n",
      "Epoch: 567 | loss: 0.4405030608177185 | test loss: 0.41677239537239075 | accuracy: tensor([0.9667])\n",
      "Epoch: 568 | loss: 0.43923765420913696 | test loss: 0.41538628935813904 | accuracy: tensor([0.9667])\n",
      "Epoch: 569 | loss: 0.4379774332046509 | test loss: 0.4140056073665619 | accuracy: tensor([0.9667])\n",
      "Epoch: 570 | loss: 0.4367225170135498 | test loss: 0.4126302897930145 | accuracy: tensor([0.9667])\n",
      "Epoch: 571 | loss: 0.43547290563583374 | test loss: 0.4112605154514313 | accuracy: tensor([0.9667])\n",
      "Epoch: 572 | loss: 0.4342286288738251 | test loss: 0.40989625453948975 | accuracy: tensor([0.9667])\n",
      "Epoch: 573 | loss: 0.4329897165298462 | test loss: 0.40853753685951233 | accuracy: tensor([0.9667])\n",
      "Epoch: 574 | loss: 0.43175625801086426 | test loss: 0.4071844518184662 | accuracy: tensor([0.9667])\n",
      "Epoch: 575 | loss: 0.4305282533168793 | test loss: 0.40583688020706177 | accuracy: tensor([0.9667])\n",
      "Epoch: 576 | loss: 0.429305762052536 | test loss: 0.4044950604438782 | accuracy: tensor([0.9667])\n",
      "Epoch: 577 | loss: 0.4280887544155121 | test loss: 0.4031588137149811 | accuracy: tensor([0.9667])\n",
      "Epoch: 578 | loss: 0.42687737941741943 | test loss: 0.4018283784389496 | accuracy: tensor([0.9667])\n",
      "Epoch: 579 | loss: 0.42567145824432373 | test loss: 0.40050363540649414 | accuracy: tensor([0.9667])\n",
      "Epoch: 580 | loss: 0.42447125911712646 | test loss: 0.3991846442222595 | accuracy: tensor([0.9667])\n",
      "Epoch: 581 | loss: 0.4232766926288605 | test loss: 0.3978714644908905 | accuracy: tensor([0.9667])\n",
      "Epoch: 582 | loss: 0.42208775877952576 | test loss: 0.3965640962123871 | accuracy: tensor([0.9667])\n",
      "Epoch: 583 | loss: 0.4209044873714447 | test loss: 0.3952625095844269 | accuracy: tensor([0.9667])\n",
      "Epoch: 584 | loss: 0.4197269380092621 | test loss: 0.39396676421165466 | accuracy: tensor([0.9667])\n",
      "Epoch: 585 | loss: 0.4185551404953003 | test loss: 0.3926769495010376 | accuracy: tensor([0.9667])\n",
      "Epoch: 586 | loss: 0.41738906502723694 | test loss: 0.3913929760456085 | accuracy: tensor([0.9667])\n",
      "Epoch: 587 | loss: 0.4162287712097168 | test loss: 0.3901148736476898 | accuracy: tensor([0.9667])\n",
      "Epoch: 588 | loss: 0.4150741994380951 | test loss: 0.38884270191192627 | accuracy: tensor([0.9667])\n",
      "Epoch: 589 | loss: 0.4139254689216614 | test loss: 0.3875764310359955 | accuracy: tensor([0.9667])\n",
      "Epoch: 590 | loss: 0.41278254985809326 | test loss: 0.38631612062454224 | accuracy: tensor([0.9667])\n",
      "Epoch: 591 | loss: 0.41164538264274597 | test loss: 0.38506177067756653 | accuracy: tensor([0.9667])\n",
      "Epoch: 592 | loss: 0.41051405668258667 | test loss: 0.3838133215904236 | accuracy: tensor([0.9667])\n",
      "Epoch: 593 | loss: 0.4093886613845825 | test loss: 0.3825708329677582 | accuracy: tensor([0.9667])\n",
      "Epoch: 594 | loss: 0.4082690179347992 | test loss: 0.3813343048095703 | accuracy: tensor([0.9667])\n",
      "Epoch: 595 | loss: 0.40715527534484863 | test loss: 0.38010373711586 | accuracy: tensor([0.9667])\n",
      "Epoch: 596 | loss: 0.40604734420776367 | test loss: 0.378879189491272 | accuracy: tensor([0.9667])\n",
      "Epoch: 597 | loss: 0.4049453139305115 | test loss: 0.3776606619358063 | accuracy: tensor([0.9667])\n",
      "Epoch: 598 | loss: 0.40384912490844727 | test loss: 0.3764480650424957 | accuracy: tensor([0.9667])\n",
      "Epoch: 599 | loss: 0.4027588367462158 | test loss: 0.37524136900901794 | accuracy: tensor([0.9667])\n",
      "Epoch: 600 | loss: 0.40167438983917236 | test loss: 0.37404078245162964 | accuracy: tensor([0.9667])\n",
      "Epoch: 601 | loss: 0.4005958139896393 | test loss: 0.3728460967540741 | accuracy: tensor([0.9667])\n",
      "Epoch: 602 | loss: 0.3995230793952942 | test loss: 0.3716574013233185 | accuracy: tensor([0.9667])\n",
      "Epoch: 603 | loss: 0.398456335067749 | test loss: 0.3704747259616852 | accuracy: tensor([0.9667])\n",
      "Epoch: 604 | loss: 0.3973952829837799 | test loss: 0.36929798126220703 | accuracy: tensor([0.9667])\n",
      "Epoch: 605 | loss: 0.3963402211666107 | test loss: 0.3681272268295288 | accuracy: tensor([0.9667])\n",
      "Epoch: 606 | loss: 0.39529091119766235 | test loss: 0.3669624328613281 | accuracy: tensor([0.9667])\n",
      "Epoch: 607 | loss: 0.39424756169319153 | test loss: 0.3658035397529602 | accuracy: tensor([0.9667])\n",
      "Epoch: 608 | loss: 0.3932099938392639 | test loss: 0.3646506667137146 | accuracy: tensor([0.9667])\n",
      "Epoch: 609 | loss: 0.39217832684516907 | test loss: 0.363503634929657 | accuracy: tensor([0.9667])\n",
      "Epoch: 610 | loss: 0.39115241169929504 | test loss: 0.36236268281936646 | accuracy: tensor([0.9667])\n",
      "Epoch: 611 | loss: 0.390132337808609 | test loss: 0.36122754216194153 | accuracy: tensor([0.9667])\n",
      "Epoch: 612 | loss: 0.38911810517311096 | test loss: 0.36009833216667175 | accuracy: tensor([0.9667])\n",
      "Epoch: 613 | loss: 0.38810965418815613 | test loss: 0.35897502303123474 | accuracy: tensor([0.9667])\n",
      "Epoch: 614 | loss: 0.3871070444583893 | test loss: 0.3578576445579529 | accuracy: tensor([0.9667])\n",
      "Epoch: 615 | loss: 0.3861101269721985 | test loss: 0.3567461669445038 | accuracy: tensor([0.9667])\n",
      "Epoch: 616 | loss: 0.3851189911365509 | test loss: 0.3556405305862427 | accuracy: tensor([0.9667])\n",
      "Epoch: 617 | loss: 0.38413363695144653 | test loss: 0.3545406758785248 | accuracy: tensor([0.9667])\n",
      "Epoch: 618 | loss: 0.383154034614563 | test loss: 0.35344675183296204 | accuracy: tensor([0.9667])\n",
      "Epoch: 619 | loss: 0.38218018412590027 | test loss: 0.3523586392402649 | accuracy: tensor([0.9667])\n",
      "Epoch: 620 | loss: 0.3812119960784912 | test loss: 0.35127633810043335 | accuracy: tensor([0.9667])\n",
      "Epoch: 621 | loss: 0.3802495002746582 | test loss: 0.35019978880882263 | accuracy: tensor([0.9667])\n",
      "Epoch: 622 | loss: 0.37929269671440125 | test loss: 0.3491290509700775 | accuracy: tensor([0.9667])\n",
      "Epoch: 623 | loss: 0.37834155559539795 | test loss: 0.34806403517723083 | accuracy: tensor([0.9667])\n",
      "Epoch: 624 | loss: 0.3773960471153259 | test loss: 0.34700483083724976 | accuracy: tensor([0.9667])\n",
      "Epoch: 625 | loss: 0.3764561712741852 | test loss: 0.3459513485431671 | accuracy: tensor([0.9667])\n",
      "Epoch: 626 | loss: 0.3755219280719757 | test loss: 0.34490349888801575 | accuracy: tensor([0.9667])\n",
      "Epoch: 627 | loss: 0.37459325790405273 | test loss: 0.3438614308834076 | accuracy: tensor([0.9667])\n",
      "Epoch: 628 | loss: 0.37367019057273865 | test loss: 0.3428249955177307 | accuracy: tensor([0.9667])\n",
      "Epoch: 629 | loss: 0.3727526366710663 | test loss: 0.3417941927909851 | accuracy: tensor([0.9667])\n",
      "Epoch: 630 | loss: 0.37184059619903564 | test loss: 0.34076908230781555 | accuracy: tensor([0.9667])\n",
      "Epoch: 631 | loss: 0.37093406915664673 | test loss: 0.33974960446357727 | accuracy: tensor([0.9667])\n",
      "Epoch: 632 | loss: 0.3700330853462219 | test loss: 0.3387356102466583 | accuracy: tensor([0.9667])\n",
      "Epoch: 633 | loss: 0.3691375255584717 | test loss: 0.33772721886634827 | accuracy: tensor([0.9667])\n",
      "Epoch: 634 | loss: 0.3682474195957184 | test loss: 0.3367244303226471 | accuracy: tensor([0.9667])\n",
      "Epoch: 635 | loss: 0.36736273765563965 | test loss: 0.33572715520858765 | accuracy: tensor([0.9667])\n",
      "Epoch: 636 | loss: 0.3664834499359131 | test loss: 0.3347353935241699 | accuracy: tensor([0.9667])\n",
      "Epoch: 637 | loss: 0.3656095862388611 | test loss: 0.3337491452693939 | accuracy: tensor([0.9667])\n",
      "Epoch: 638 | loss: 0.3647409677505493 | test loss: 0.3327683210372925 | accuracy: tensor([0.9667])\n",
      "Epoch: 639 | loss: 0.3638777732849121 | test loss: 0.331792950630188 | accuracy: tensor([0.9667])\n",
      "Epoch: 640 | loss: 0.36301982402801514 | test loss: 0.33082300424575806 | accuracy: tensor([0.9667])\n",
      "Epoch: 641 | loss: 0.36216720938682556 | test loss: 0.3298584222793579 | accuracy: tensor([0.9667])\n",
      "Epoch: 642 | loss: 0.361319899559021 | test loss: 0.3288992643356323 | accuracy: tensor([0.9667])\n",
      "Epoch: 643 | loss: 0.3604777157306671 | test loss: 0.32794538140296936 | accuracy: tensor([0.9667])\n",
      "Epoch: 644 | loss: 0.35964077711105347 | test loss: 0.32699689269065857 | accuracy: tensor([0.9667])\n",
      "Epoch: 645 | loss: 0.3588089942932129 | test loss: 0.3260536789894104 | accuracy: tensor([0.9667])\n",
      "Epoch: 646 | loss: 0.35798242688179016 | test loss: 0.32511574029922485 | accuracy: tensor([0.9667])\n",
      "Epoch: 647 | loss: 0.3571609854698181 | test loss: 0.3241831064224243 | accuracy: tensor([0.9667])\n",
      "Epoch: 648 | loss: 0.35634467005729675 | test loss: 0.32325562834739685 | accuracy: tensor([0.9667])\n",
      "Epoch: 649 | loss: 0.3555333614349365 | test loss: 0.3223333954811096 | accuracy: tensor([0.9667])\n",
      "Epoch: 650 | loss: 0.3547271192073822 | test loss: 0.32141634821891785 | accuracy: tensor([0.9667])\n",
      "Epoch: 651 | loss: 0.3539259433746338 | test loss: 0.32050445675849915 | accuracy: tensor([0.9667])\n",
      "Epoch: 652 | loss: 0.3531298041343689 | test loss: 0.31959766149520874 | accuracy: tensor([0.9667])\n",
      "Epoch: 653 | loss: 0.3523385524749756 | test loss: 0.318695992231369 | accuracy: tensor([0.9667])\n",
      "Epoch: 654 | loss: 0.3515523076057434 | test loss: 0.3177993893623352 | accuracy: tensor([0.9667])\n",
      "Epoch: 655 | loss: 0.3507709205150604 | test loss: 0.3169078230857849 | accuracy: tensor([0.9667])\n",
      "Epoch: 656 | loss: 0.3499945104122162 | test loss: 0.3160213232040405 | accuracy: tensor([0.9667])\n",
      "Epoch: 657 | loss: 0.34922295808792114 | test loss: 0.3151398301124573 | accuracy: tensor([0.9667])\n",
      "Epoch: 658 | loss: 0.34845614433288574 | test loss: 0.3142632842063904 | accuracy: tensor([0.9667])\n",
      "Epoch: 659 | loss: 0.3476942479610443 | test loss: 0.31339168548583984 | accuracy: tensor([0.9667])\n",
      "Epoch: 660 | loss: 0.34693706035614014 | test loss: 0.31252506375312805 | accuracy: tensor([0.9667])\n",
      "Epoch: 661 | loss: 0.34618470072746277 | test loss: 0.31166332960128784 | accuracy: tensor([0.9667])\n",
      "Epoch: 662 | loss: 0.34543704986572266 | test loss: 0.31080642342567444 | accuracy: tensor([0.9667])\n",
      "Epoch: 663 | loss: 0.3446940779685974 | test loss: 0.30995437502861023 | accuracy: tensor([0.9667])\n",
      "Epoch: 664 | loss: 0.3439558446407318 | test loss: 0.3091071844100952 | accuracy: tensor([0.9667])\n",
      "Epoch: 665 | loss: 0.34322214126586914 | test loss: 0.30826476216316223 | accuracy: tensor([0.9667])\n",
      "Epoch: 666 | loss: 0.3424932062625885 | test loss: 0.3074271082878113 | accuracy: tensor([0.9667])\n",
      "Epoch: 667 | loss: 0.341768741607666 | test loss: 0.30659425258636475 | accuracy: tensor([0.9667])\n",
      "Epoch: 668 | loss: 0.3410488963127136 | test loss: 0.3057660460472107 | accuracy: tensor([0.9667])\n",
      "Epoch: 669 | loss: 0.34033358097076416 | test loss: 0.3049425780773163 | accuracy: tensor([0.9667])\n",
      "Epoch: 670 | loss: 0.33962282538414 | test loss: 0.30412372946739197 | accuracy: tensor([0.9667])\n",
      "Epoch: 671 | loss: 0.33891645073890686 | test loss: 0.3033095598220825 | accuracy: tensor([0.9667])\n",
      "Epoch: 672 | loss: 0.33821460604667664 | test loss: 0.3024999499320984 | accuracy: tensor([0.9667])\n",
      "Epoch: 673 | loss: 0.3375171720981598 | test loss: 0.30169498920440674 | accuracy: tensor([0.9667])\n",
      "Epoch: 674 | loss: 0.33682408928871155 | test loss: 0.300894558429718 | accuracy: tensor([0.9667])\n",
      "Epoch: 675 | loss: 0.3361354172229767 | test loss: 0.300098717212677 | accuracy: tensor([0.9667])\n",
      "Epoch: 676 | loss: 0.3354510962963104 | test loss: 0.29930734634399414 | accuracy: tensor([0.9667])\n",
      "Epoch: 677 | loss: 0.334771066904068 | test loss: 0.29852041602134705 | accuracy: tensor([0.9667])\n",
      "Epoch: 678 | loss: 0.3340953290462494 | test loss: 0.2977379560470581 | accuracy: tensor([0.9667])\n",
      "Epoch: 679 | loss: 0.3334238827228546 | test loss: 0.29695993661880493 | accuracy: tensor([0.9667])\n",
      "Epoch: 680 | loss: 0.3327566683292389 | test loss: 0.2961863577365875 | accuracy: tensor([0.9667])\n",
      "Epoch: 681 | loss: 0.33209359645843506 | test loss: 0.2954171597957611 | accuracy: tensor([0.9667])\n",
      "Epoch: 682 | loss: 0.3314347565174103 | test loss: 0.2946522831916809 | accuracy: tensor([0.9667])\n",
      "Epoch: 683 | loss: 0.330780029296875 | test loss: 0.2938917279243469 | accuracy: tensor([0.9667])\n",
      "Epoch: 684 | loss: 0.330129474401474 | test loss: 0.29313549399375916 | accuracy: tensor([0.9667])\n",
      "Epoch: 685 | loss: 0.32948294281959534 | test loss: 0.2923835515975952 | accuracy: tensor([0.9667])\n",
      "Epoch: 686 | loss: 0.32884058356285095 | test loss: 0.2916358411312103 | accuracy: tensor([0.9667])\n",
      "Epoch: 687 | loss: 0.32820215821266174 | test loss: 0.2908923029899597 | accuracy: tensor([0.9667])\n",
      "Epoch: 688 | loss: 0.32756781578063965 | test loss: 0.29015302658081055 | accuracy: tensor([0.9667])\n",
      "Epoch: 689 | loss: 0.3269374668598175 | test loss: 0.28941789269447327 | accuracy: tensor([0.9667])\n",
      "Epoch: 690 | loss: 0.32631102204322815 | test loss: 0.28868696093559265 | accuracy: tensor([0.9667])\n",
      "Epoch: 691 | loss: 0.32568854093551636 | test loss: 0.28796008229255676 | accuracy: tensor([0.9667])\n",
      "Epoch: 692 | loss: 0.32506999373435974 | test loss: 0.28723734617233276 | accuracy: tensor([0.9667])\n",
      "Epoch: 693 | loss: 0.3244553208351135 | test loss: 0.2865186929702759 | accuracy: tensor([0.9667])\n",
      "Epoch: 694 | loss: 0.3238444924354553 | test loss: 0.2858040928840637 | accuracy: tensor([0.9667])\n",
      "Epoch: 695 | loss: 0.32323747873306274 | test loss: 0.2850934565067291 | accuracy: tensor([0.9667])\n",
      "Epoch: 696 | loss: 0.3226342797279358 | test loss: 0.28438684344291687 | accuracy: tensor([0.9667])\n",
      "Epoch: 697 | loss: 0.3220348656177521 | test loss: 0.28368428349494934 | accuracy: tensor([0.9667])\n",
      "Epoch: 698 | loss: 0.3214392066001892 | test loss: 0.2829855978488922 | accuracy: tensor([0.9667])\n",
      "Epoch: 699 | loss: 0.32084721326828003 | test loss: 0.28229084610939026 | accuracy: tensor([0.9667])\n",
      "Epoch: 700 | loss: 0.3202590048313141 | test loss: 0.2815999984741211 | accuracy: tensor([0.9667])\n",
      "Epoch: 701 | loss: 0.3196743428707123 | test loss: 0.28091296553611755 | accuracy: tensor([0.9667])\n",
      "Epoch: 702 | loss: 0.3190934658050537 | test loss: 0.2802298963069916 | accuracy: tensor([0.9667])\n",
      "Epoch: 703 | loss: 0.31851619482040405 | test loss: 0.27955058217048645 | accuracy: tensor([0.9667])\n",
      "Epoch: 704 | loss: 0.31794247031211853 | test loss: 0.27887508273124695 | accuracy: tensor([0.9667])\n",
      "Epoch: 705 | loss: 0.31737229228019714 | test loss: 0.27820339798927307 | accuracy: tensor([0.9667])\n",
      "Epoch: 706 | loss: 0.31680575013160706 | test loss: 0.27753543853759766 | accuracy: tensor([0.9667])\n",
      "Epoch: 707 | loss: 0.31624266505241394 | test loss: 0.2768712043762207 | accuracy: tensor([0.9667])\n",
      "Epoch: 708 | loss: 0.3156830370426178 | test loss: 0.2762106657028198 | accuracy: tensor([0.9667])\n",
      "Epoch: 709 | loss: 0.3151269555091858 | test loss: 0.2755538523197174 | accuracy: tensor([0.9667])\n",
      "Epoch: 710 | loss: 0.31457433104515076 | test loss: 0.2749007046222687 | accuracy: tensor([0.9667])\n",
      "Epoch: 711 | loss: 0.31402507424354553 | test loss: 0.27425113320350647 | accuracy: tensor([0.9667])\n",
      "Epoch: 712 | loss: 0.3134792745113373 | test loss: 0.2736052870750427 | accuracy: tensor([0.9667])\n",
      "Epoch: 713 | loss: 0.31293684244155884 | test loss: 0.27296292781829834 | accuracy: tensor([0.9667])\n",
      "Epoch: 714 | loss: 0.31239771842956543 | test loss: 0.27232417464256287 | accuracy: tensor([0.9667])\n",
      "Epoch: 715 | loss: 0.31186190247535706 | test loss: 0.27168896794319153 | accuracy: tensor([0.9667])\n",
      "Epoch: 716 | loss: 0.3113294243812561 | test loss: 0.2710573077201843 | accuracy: tensor([0.9667])\n",
      "Epoch: 717 | loss: 0.3108002543449402 | test loss: 0.2704291343688965 | accuracy: tensor([0.9667])\n",
      "Epoch: 718 | loss: 0.3102743327617645 | test loss: 0.269804447889328 | accuracy: tensor([0.9667])\n",
      "Epoch: 719 | loss: 0.30975162982940674 | test loss: 0.2691831886768341 | accuracy: tensor([0.9667])\n",
      "Epoch: 720 | loss: 0.30923211574554443 | test loss: 0.2685653567314148 | accuracy: tensor([0.9667])\n",
      "Epoch: 721 | loss: 0.3087158501148224 | test loss: 0.26795095205307007 | accuracy: tensor([0.9667])\n",
      "Epoch: 722 | loss: 0.30820268392562866 | test loss: 0.26733991503715515 | accuracy: tensor([0.9667])\n",
      "Epoch: 723 | loss: 0.30769267678260803 | test loss: 0.2667323052883148 | accuracy: tensor([0.9667])\n",
      "Epoch: 724 | loss: 0.3071857988834381 | test loss: 0.26612797379493713 | accuracy: tensor([0.9667])\n",
      "Epoch: 725 | loss: 0.3066819906234741 | test loss: 0.26552703976631165 | accuracy: tensor([0.9667])\n",
      "Epoch: 726 | loss: 0.30618131160736084 | test loss: 0.2649293541908264 | accuracy: tensor([0.9667])\n",
      "Epoch: 727 | loss: 0.30568361282348633 | test loss: 0.26433494687080383 | accuracy: tensor([0.9667])\n",
      "Epoch: 728 | loss: 0.30518898367881775 | test loss: 0.2637438178062439 | accuracy: tensor([0.9667])\n",
      "Epoch: 729 | loss: 0.3046973645687103 | test loss: 0.2631559371948242 | accuracy: tensor([0.9667])\n",
      "Epoch: 730 | loss: 0.3042086958885193 | test loss: 0.2625712454319 | accuracy: tensor([0.9667])\n",
      "Epoch: 731 | loss: 0.3037230372428894 | test loss: 0.2619897723197937 | accuracy: tensor([0.9667])\n",
      "Epoch: 732 | loss: 0.3032402992248535 | test loss: 0.2614114582538605 | accuracy: tensor([0.9667])\n",
      "Epoch: 733 | loss: 0.302760511636734 | test loss: 0.26083630323410034 | accuracy: tensor([0.9667])\n",
      "Epoch: 734 | loss: 0.30228355526924133 | test loss: 0.2602643072605133 | accuracy: tensor([0.9667])\n",
      "Epoch: 735 | loss: 0.30180948972702026 | test loss: 0.2596954107284546 | accuracy: tensor([0.9667])\n",
      "Epoch: 736 | loss: 0.3013383448123932 | test loss: 0.2591295838356018 | accuracy: tensor([0.9667])\n",
      "Epoch: 737 | loss: 0.30087003111839294 | test loss: 0.25856685638427734 | accuracy: tensor([0.9667])\n",
      "Epoch: 738 | loss: 0.30040445923805237 | test loss: 0.2580071687698364 | accuracy: tensor([0.9667])\n",
      "Epoch: 739 | loss: 0.299941748380661 | test loss: 0.25745052099227905 | accuracy: tensor([0.9667])\n",
      "Epoch: 740 | loss: 0.2994818389415741 | test loss: 0.25689685344696045 | accuracy: tensor([0.9667])\n",
      "Epoch: 741 | loss: 0.2990246117115021 | test loss: 0.2563462257385254 | accuracy: tensor([0.9667])\n",
      "Epoch: 742 | loss: 0.2985701560974121 | test loss: 0.2557985484600067 | accuracy: tensor([0.9667])\n",
      "Epoch: 743 | loss: 0.2981184124946594 | test loss: 0.2552538514137268 | accuracy: tensor([0.9667])\n",
      "Epoch: 744 | loss: 0.29766935110092163 | test loss: 0.2547120749950409 | accuracy: tensor([0.9667])\n",
      "Epoch: 745 | loss: 0.29722294211387634 | test loss: 0.254173219203949 | accuracy: tensor([0.9667])\n",
      "Epoch: 746 | loss: 0.2967792749404907 | test loss: 0.2536372244358063 | accuracy: tensor([0.9667])\n",
      "Epoch: 747 | loss: 0.29633817076683044 | test loss: 0.2531041204929352 | accuracy: tensor([0.9667])\n",
      "Epoch: 748 | loss: 0.29589971899986267 | test loss: 0.2525739073753357 | accuracy: tensor([0.9667])\n",
      "Epoch: 749 | loss: 0.29546383023262024 | test loss: 0.25204649567604065 | accuracy: tensor([0.9667])\n",
      "Epoch: 750 | loss: 0.2950305640697479 | test loss: 0.2515219449996948 | accuracy: tensor([0.9667])\n",
      "Epoch: 751 | loss: 0.29459983110427856 | test loss: 0.25100016593933105 | accuracy: tensor([0.9667])\n",
      "Epoch: 752 | loss: 0.29417166113853455 | test loss: 0.25048118829727173 | accuracy: tensor([0.9667])\n",
      "Epoch: 753 | loss: 0.29374605417251587 | test loss: 0.24996496737003326 | accuracy: tensor([0.9667])\n",
      "Epoch: 754 | loss: 0.2933228313922882 | test loss: 0.24945147335529327 | accuracy: tensor([0.9667])\n",
      "Epoch: 755 | loss: 0.2929021716117859 | test loss: 0.24894075095653534 | accuracy: tensor([0.9667])\n",
      "Epoch: 756 | loss: 0.29248398542404175 | test loss: 0.2484326958656311 | accuracy: tensor([0.9667])\n",
      "Epoch: 757 | loss: 0.2920682430267334 | test loss: 0.24792735278606415 | accuracy: tensor([0.9667])\n",
      "Epoch: 758 | loss: 0.29165491461753845 | test loss: 0.2474246770143509 | accuracy: tensor([0.9667])\n",
      "Epoch: 759 | loss: 0.2912440299987793 | test loss: 0.24692465364933014 | accuracy: tensor([0.9667])\n",
      "Epoch: 760 | loss: 0.29083552956581116 | test loss: 0.2464272677898407 | accuracy: tensor([0.9667])\n",
      "Epoch: 761 | loss: 0.2904294431209564 | test loss: 0.24593253433704376 | accuracy: tensor([0.9667])\n",
      "Epoch: 762 | loss: 0.29002565145492554 | test loss: 0.24544039368629456 | accuracy: tensor([0.9667])\n",
      "Epoch: 763 | loss: 0.28962424397468567 | test loss: 0.24495086073875427 | accuracy: tensor([0.9667])\n",
      "Epoch: 764 | loss: 0.2892252206802368 | test loss: 0.24446387588977814 | accuracy: tensor([0.9667])\n",
      "Epoch: 765 | loss: 0.28882840275764465 | test loss: 0.24397943913936615 | accuracy: tensor([0.9667])\n",
      "Epoch: 766 | loss: 0.2884339392185211 | test loss: 0.2434975802898407 | accuracy: tensor([0.9667])\n",
      "Epoch: 767 | loss: 0.28804174065589905 | test loss: 0.24301819503307343 | accuracy: tensor([0.9667])\n",
      "Epoch: 768 | loss: 0.28765183687210083 | test loss: 0.2425413429737091 | accuracy: tensor([0.9667])\n",
      "Epoch: 769 | loss: 0.2872641086578369 | test loss: 0.24206699430942535 | accuracy: tensor([0.9667])\n",
      "Epoch: 770 | loss: 0.28687870502471924 | test loss: 0.2415950745344162 | accuracy: tensor([0.9667])\n",
      "Epoch: 771 | loss: 0.2864954173564911 | test loss: 0.24112559854984283 | accuracy: tensor([0.9667])\n",
      "Epoch: 772 | loss: 0.2861143946647644 | test loss: 0.24065862596035004 | accuracy: tensor([0.9667])\n",
      "Epoch: 773 | loss: 0.28573551774024963 | test loss: 0.24019402265548706 | accuracy: tensor([0.9667])\n",
      "Epoch: 774 | loss: 0.28535881638526917 | test loss: 0.23973187804222107 | accuracy: tensor([0.9667])\n",
      "Epoch: 775 | loss: 0.2849842607975006 | test loss: 0.2392720878124237 | accuracy: tensor([0.9667])\n",
      "Epoch: 776 | loss: 0.28461185097694397 | test loss: 0.23881466686725616 | accuracy: tensor([0.9667])\n",
      "Epoch: 777 | loss: 0.28424152731895447 | test loss: 0.23835961520671844 | accuracy: tensor([0.9667])\n",
      "Epoch: 778 | loss: 0.28387337923049927 | test loss: 0.23790691792964935 | accuracy: tensor([0.9667])\n",
      "Epoch: 779 | loss: 0.28350725769996643 | test loss: 0.2374565452337265 | accuracy: tensor([0.9667])\n",
      "Epoch: 780 | loss: 0.28314319252967834 | test loss: 0.23700851202011108 | accuracy: tensor([0.9667])\n",
      "Epoch: 781 | loss: 0.2827812433242798 | test loss: 0.23656272888183594 | accuracy: tensor([0.9667])\n",
      "Epoch: 782 | loss: 0.2824212908744812 | test loss: 0.23611925542354584 | accuracy: tensor([0.9667])\n",
      "Epoch: 783 | loss: 0.28206342458724976 | test loss: 0.235678032040596 | accuracy: tensor([0.9667])\n",
      "Epoch: 784 | loss: 0.2817075550556183 | test loss: 0.23523910343647003 | accuracy: tensor([0.9667])\n",
      "Epoch: 785 | loss: 0.281353622674942 | test loss: 0.23480238020420074 | accuracy: tensor([0.9667])\n",
      "Epoch: 786 | loss: 0.2810017466545105 | test loss: 0.23436787724494934 | accuracy: tensor([0.9667])\n",
      "Epoch: 787 | loss: 0.2806517779827118 | test loss: 0.23393559455871582 | accuracy: tensor([0.9667])\n",
      "Epoch: 788 | loss: 0.28030383586883545 | test loss: 0.23350553214550018 | accuracy: tensor([0.9667])\n",
      "Epoch: 789 | loss: 0.2799578011035919 | test loss: 0.23307760059833527 | accuracy: tensor([0.9667])\n",
      "Epoch: 790 | loss: 0.2796136736869812 | test loss: 0.23265187442302704 | accuracy: tensor([0.9667])\n",
      "Epoch: 791 | loss: 0.27927151322364807 | test loss: 0.23222827911376953 | accuracy: tensor([0.9667])\n",
      "Epoch: 792 | loss: 0.278931200504303 | test loss: 0.23180684447288513 | accuracy: tensor([0.9667])\n",
      "Epoch: 793 | loss: 0.27859288454055786 | test loss: 0.23138754069805145 | accuracy: tensor([0.9667])\n",
      "Epoch: 794 | loss: 0.27825629711151123 | test loss: 0.2309703528881073 | accuracy: tensor([0.9667])\n",
      "Epoch: 795 | loss: 0.2779216468334198 | test loss: 0.2305552214384079 | accuracy: tensor([0.9667])\n",
      "Epoch: 796 | loss: 0.2775888442993164 | test loss: 0.23014216125011444 | accuracy: tensor([0.9667])\n",
      "Epoch: 797 | loss: 0.27725785970687866 | test loss: 0.2297312319278717 | accuracy: tensor([0.9667])\n",
      "Epoch: 798 | loss: 0.27692872285842896 | test loss: 0.22932232916355133 | accuracy: tensor([0.9667])\n",
      "Epoch: 799 | loss: 0.2766013443470001 | test loss: 0.22891545295715332 | accuracy: tensor([0.9667])\n",
      "Epoch: 800 | loss: 0.2762758135795593 | test loss: 0.22851061820983887 | accuracy: tensor([0.9667])\n",
      "Epoch: 801 | loss: 0.2759520411491394 | test loss: 0.22810779511928558 | accuracy: tensor([0.9667])\n",
      "Epoch: 802 | loss: 0.27563002705574036 | test loss: 0.22770699858665466 | accuracy: tensor([0.9667])\n",
      "Epoch: 803 | loss: 0.27530980110168457 | test loss: 0.22730818390846252 | accuracy: tensor([0.9667])\n",
      "Epoch: 804 | loss: 0.27499133348464966 | test loss: 0.22691133618354797 | accuracy: tensor([0.9667])\n",
      "Epoch: 805 | loss: 0.27467453479766846 | test loss: 0.226516455411911 | accuracy: tensor([0.9667])\n",
      "Epoch: 806 | loss: 0.27435949444770813 | test loss: 0.22612351179122925 | accuracy: tensor([0.9667])\n",
      "Epoch: 807 | loss: 0.2740461528301239 | test loss: 0.22573252022266388 | accuracy: tensor([0.9667])\n",
      "Epoch: 808 | loss: 0.27373456954956055 | test loss: 0.2253434658050537 | accuracy: tensor([0.9667])\n",
      "Epoch: 809 | loss: 0.2734246253967285 | test loss: 0.22495627403259277 | accuracy: tensor([0.9667])\n",
      "Epoch: 810 | loss: 0.2731162905693054 | test loss: 0.22457101941108704 | accuracy: tensor([0.9667])\n",
      "Epoch: 811 | loss: 0.2728096842765808 | test loss: 0.2241876721382141 | accuracy: tensor([0.9667])\n",
      "Epoch: 812 | loss: 0.2725047171115875 | test loss: 0.22380618751049042 | accuracy: tensor([0.9667])\n",
      "Epoch: 813 | loss: 0.27220138907432556 | test loss: 0.22342656552791595 | accuracy: tensor([0.9667])\n",
      "Epoch: 814 | loss: 0.2718997001647949 | test loss: 0.22304879128932953 | accuracy: tensor([0.9667])\n",
      "Epoch: 815 | loss: 0.27159959077835083 | test loss: 0.22267289459705353 | accuracy: tensor([0.9667])\n",
      "Epoch: 816 | loss: 0.27130112051963806 | test loss: 0.2222987711429596 | accuracy: tensor([0.9667])\n",
      "Epoch: 817 | loss: 0.27100419998168945 | test loss: 0.2219264954328537 | accuracy: tensor([0.9667])\n",
      "Epoch: 818 | loss: 0.2707088887691498 | test loss: 0.22155599296092987 | accuracy: tensor([0.9667])\n",
      "Epoch: 819 | loss: 0.27041515707969666 | test loss: 0.22118733823299408 | accuracy: tensor([0.9667])\n",
      "Epoch: 820 | loss: 0.2701230049133301 | test loss: 0.22082044184207916 | accuracy: tensor([0.9667])\n",
      "Epoch: 821 | loss: 0.2698323428630829 | test loss: 0.2204553186893463 | accuracy: tensor([0.9667])\n",
      "Epoch: 822 | loss: 0.26954323053359985 | test loss: 0.22009190917015076 | accuracy: tensor([0.9667])\n",
      "Epoch: 823 | loss: 0.26925572752952576 | test loss: 0.21973031759262085 | accuracy: tensor([0.9667])\n",
      "Epoch: 824 | loss: 0.26896965503692627 | test loss: 0.21937043964862823 | accuracy: tensor([0.9667])\n",
      "Epoch: 825 | loss: 0.26868510246276855 | test loss: 0.2190122902393341 | accuracy: tensor([0.9667])\n",
      "Epoch: 826 | loss: 0.2684020400047302 | test loss: 0.21865586936473846 | accuracy: tensor([0.9667])\n",
      "Epoch: 827 | loss: 0.26812052726745605 | test loss: 0.21830108761787415 | accuracy: tensor([0.9667])\n",
      "Epoch: 828 | loss: 0.2678404450416565 | test loss: 0.2179480940103531 | accuracy: tensor([0.9667])\n",
      "Epoch: 829 | loss: 0.26756179332733154 | test loss: 0.21759672462940216 | accuracy: tensor([0.9667])\n",
      "Epoch: 830 | loss: 0.26728466153144836 | test loss: 0.21724700927734375 | accuracy: tensor([0.9667])\n",
      "Epoch: 831 | loss: 0.2670089304447174 | test loss: 0.21689900755882263 | accuracy: tensor([0.9667])\n",
      "Epoch: 832 | loss: 0.26673462986946106 | test loss: 0.21655260026454926 | accuracy: tensor([0.9667])\n",
      "Epoch: 833 | loss: 0.2664617896080017 | test loss: 0.2162078469991684 | accuracy: tensor([0.9667])\n",
      "Epoch: 834 | loss: 0.26619037985801697 | test loss: 0.21586479246616364 | accuracy: tensor([0.9667])\n",
      "Epoch: 835 | loss: 0.2659202814102173 | test loss: 0.21552325785160065 | accuracy: tensor([0.9667])\n",
      "Epoch: 836 | loss: 0.265651673078537 | test loss: 0.21518340706825256 | accuracy: tensor([0.9667])\n",
      "Epoch: 837 | loss: 0.2653844356536865 | test loss: 0.21484510600566864 | accuracy: tensor([0.9667])\n",
      "Epoch: 838 | loss: 0.2651185393333435 | test loss: 0.21450839936733246 | accuracy: tensor([0.9667])\n",
      "Epoch: 839 | loss: 0.2648540437221527 | test loss: 0.21417327225208282 | accuracy: tensor([0.9667])\n",
      "Epoch: 840 | loss: 0.26459088921546936 | test loss: 0.21383973956108093 | accuracy: tensor([0.9667])\n",
      "Epoch: 841 | loss: 0.26432910561561584 | test loss: 0.21350771188735962 | accuracy: tensor([0.9667])\n",
      "Epoch: 842 | loss: 0.2640686333179474 | test loss: 0.21317729353904724 | accuracy: tensor([0.9667])\n",
      "Epoch: 843 | loss: 0.2638095021247864 | test loss: 0.21284839510917664 | accuracy: tensor([0.9667])\n",
      "Epoch: 844 | loss: 0.2635516822338104 | test loss: 0.2125210165977478 | accuracy: tensor([0.9667])\n",
      "Epoch: 845 | loss: 0.2632952332496643 | test loss: 0.21219515800476074 | accuracy: tensor([0.9667])\n",
      "Epoch: 846 | loss: 0.2630400061607361 | test loss: 0.21187080442905426 | accuracy: tensor([0.9667])\n",
      "Epoch: 847 | loss: 0.2627861499786377 | test loss: 0.21154794096946716 | accuracy: tensor([0.9667])\n",
      "Epoch: 848 | loss: 0.2625335156917572 | test loss: 0.21122661232948303 | accuracy: tensor([0.9667])\n",
      "Epoch: 849 | loss: 0.26228222250938416 | test loss: 0.2109067291021347 | accuracy: tensor([0.9667])\n",
      "Epoch: 850 | loss: 0.2620321214199066 | test loss: 0.21058832108974457 | accuracy: tensor([0.9667])\n",
      "Epoch: 851 | loss: 0.2617833614349365 | test loss: 0.21027138829231262 | accuracy: tensor([0.9667])\n",
      "Epoch: 852 | loss: 0.26153579354286194 | test loss: 0.20995591580867767 | accuracy: tensor([0.9667])\n",
      "Epoch: 853 | loss: 0.26128947734832764 | test loss: 0.20964187383651733 | accuracy: tensor([0.9667])\n",
      "Epoch: 854 | loss: 0.2610444128513336 | test loss: 0.20932923257350922 | accuracy: tensor([0.9667])\n",
      "Epoch: 855 | loss: 0.2608005702495575 | test loss: 0.2090180665254593 | accuracy: tensor([0.9667])\n",
      "Epoch: 856 | loss: 0.26055797934532166 | test loss: 0.20870831608772278 | accuracy: tensor([0.9667])\n",
      "Epoch: 857 | loss: 0.2603165805339813 | test loss: 0.2083999663591385 | accuracy: tensor([0.9667])\n",
      "Epoch: 858 | loss: 0.2600763738155365 | test loss: 0.20809300243854523 | accuracy: tensor([0.9667])\n",
      "Epoch: 859 | loss: 0.2598373591899872 | test loss: 0.20778745412826538 | accuracy: tensor([0.9667])\n",
      "Epoch: 860 | loss: 0.2595995366573334 | test loss: 0.20748330652713776 | accuracy: tensor([0.9667])\n",
      "Epoch: 861 | loss: 0.2593629062175751 | test loss: 0.207180455327034 | accuracy: tensor([0.9667])\n",
      "Epoch: 862 | loss: 0.2591274380683899 | test loss: 0.20687904953956604 | accuracy: tensor([0.9667])\n",
      "Epoch: 863 | loss: 0.2588931620121002 | test loss: 0.20657895505428314 | accuracy: tensor([0.9667])\n",
      "Epoch: 864 | loss: 0.2586599886417389 | test loss: 0.20628021657466888 | accuracy: tensor([0.9667])\n",
      "Epoch: 865 | loss: 0.2584279775619507 | test loss: 0.20598284900188446 | accuracy: tensor([0.9667])\n",
      "Epoch: 866 | loss: 0.2581971287727356 | test loss: 0.2056868076324463 | accuracy: tensor([0.9667])\n",
      "Epoch: 867 | loss: 0.25796744227409363 | test loss: 0.20539207756519318 | accuracy: tensor([0.9667])\n",
      "Epoch: 868 | loss: 0.25773885846138 | test loss: 0.20509865880012512 | accuracy: tensor([0.9667])\n",
      "Epoch: 869 | loss: 0.25751134753227234 | test loss: 0.20480655133724213 | accuracy: tensor([0.9667])\n",
      "Epoch: 870 | loss: 0.2572849690914154 | test loss: 0.204515740275383 | accuracy: tensor([0.9667])\n",
      "Epoch: 871 | loss: 0.2570597529411316 | test loss: 0.20422624051570892 | accuracy: tensor([0.9667])\n",
      "Epoch: 872 | loss: 0.25683557987213135 | test loss: 0.20393797755241394 | accuracy: tensor([0.9667])\n",
      "Epoch: 873 | loss: 0.25661250948905945 | test loss: 0.20365102589130402 | accuracy: tensor([0.9667])\n",
      "Epoch: 874 | loss: 0.2563905119895935 | test loss: 0.20336534082889557 | accuracy: tensor([0.9667])\n",
      "Epoch: 875 | loss: 0.2561695873737335 | test loss: 0.2030809223651886 | accuracy: tensor([0.9667])\n",
      "Epoch: 876 | loss: 0.2559497654438019 | test loss: 0.2027977555990219 | accuracy: tensor([0.9667])\n",
      "Epoch: 877 | loss: 0.2557309567928314 | test loss: 0.2025158554315567 | accuracy: tensor([0.9667])\n",
      "Epoch: 878 | loss: 0.2555132806301117 | test loss: 0.2022351622581482 | accuracy: tensor([0.9667])\n",
      "Epoch: 879 | loss: 0.2552965581417084 | test loss: 0.20195570588111877 | accuracy: tensor([0.9667])\n",
      "Epoch: 880 | loss: 0.255080908536911 | test loss: 0.20167745649814606 | accuracy: tensor([0.9667])\n",
      "Epoch: 881 | loss: 0.2548663020133972 | test loss: 0.20140044391155243 | accuracy: tensor([0.9667])\n",
      "Epoch: 882 | loss: 0.254652738571167 | test loss: 0.20112460851669312 | accuracy: tensor([0.9667])\n",
      "Epoch: 883 | loss: 0.25444015860557556 | test loss: 0.2008500099182129 | accuracy: tensor([0.9667])\n",
      "Epoch: 884 | loss: 0.2542286515235901 | test loss: 0.20057658851146698 | accuracy: tensor([0.9667])\n",
      "Epoch: 885 | loss: 0.254018098115921 | test loss: 0.20030437409877777 | accuracy: tensor([0.9667])\n",
      "Epoch: 886 | loss: 0.2538085877895355 | test loss: 0.20003332197666168 | accuracy: tensor([0.9667])\n",
      "Epoch: 887 | loss: 0.2536000609397888 | test loss: 0.1997634470462799 | accuracy: tensor([0.9667])\n",
      "Epoch: 888 | loss: 0.2533925175666809 | test loss: 0.19949470460414886 | accuracy: tensor([0.9667])\n",
      "Epoch: 889 | loss: 0.2531859576702118 | test loss: 0.19922716915607452 | accuracy: tensor([0.9667])\n",
      "Epoch: 890 | loss: 0.25298038125038147 | test loss: 0.1989607810974121 | accuracy: tensor([0.9667])\n",
      "Epoch: 891 | loss: 0.25277575850486755 | test loss: 0.19869549572467804 | accuracy: tensor([0.9667])\n",
      "Epoch: 892 | loss: 0.25257211923599243 | test loss: 0.1984313577413559 | accuracy: tensor([0.9667])\n",
      "Epoch: 893 | loss: 0.2523694336414337 | test loss: 0.19816838204860687 | accuracy: tensor([0.9667])\n",
      "Epoch: 894 | loss: 0.2521677017211914 | test loss: 0.19790655374526978 | accuracy: tensor([0.9667])\n",
      "Epoch: 895 | loss: 0.2519668936729431 | test loss: 0.19764575362205505 | accuracy: tensor([0.9667])\n",
      "Epoch: 896 | loss: 0.25176703929901123 | test loss: 0.19738616049289703 | accuracy: tensor([0.9667])\n",
      "Epoch: 897 | loss: 0.25156810879707336 | test loss: 0.19712762534618378 | accuracy: tensor([0.9667])\n",
      "Epoch: 898 | loss: 0.2513701319694519 | test loss: 0.19687014818191528 | accuracy: tensor([0.9667])\n",
      "Epoch: 899 | loss: 0.2511730492115021 | test loss: 0.19661380350589752 | accuracy: tensor([0.9667])\n",
      "Epoch: 900 | loss: 0.25097692012786865 | test loss: 0.1963585466146469 | accuracy: tensor([0.9667])\n",
      "Epoch: 901 | loss: 0.2507816553115845 | test loss: 0.19610436260700226 | accuracy: tensor([0.9667])\n",
      "Epoch: 902 | loss: 0.2505873143672943 | test loss: 0.19585128128528595 | accuracy: tensor([0.9667])\n",
      "Epoch: 903 | loss: 0.25039389729499817 | test loss: 0.19559922814369202 | accuracy: tensor([0.9667])\n",
      "Epoch: 904 | loss: 0.25020134449005127 | test loss: 0.19534824788570404 | accuracy: tensor([0.9667])\n",
      "Epoch: 905 | loss: 0.250009685754776 | test loss: 0.19509829580783844 | accuracy: tensor([0.9667])\n",
      "Epoch: 906 | loss: 0.24981887638568878 | test loss: 0.1948494166135788 | accuracy: tensor([0.9667])\n",
      "Epoch: 907 | loss: 0.24962899088859558 | test loss: 0.19460153579711914 | accuracy: tensor([0.9667])\n",
      "Epoch: 908 | loss: 0.24943996965885162 | test loss: 0.19435471296310425 | accuracy: tensor([0.9667])\n",
      "Epoch: 909 | loss: 0.2492518275976181 | test loss: 0.19410894811153412 | accuracy: tensor([0.9667])\n",
      "Epoch: 910 | loss: 0.24906452000141144 | test loss: 0.19386416673660278 | accuracy: tensor([0.9667])\n",
      "Epoch: 911 | loss: 0.24887807667255402 | test loss: 0.19362042844295502 | accuracy: tensor([0.9667])\n",
      "Epoch: 912 | loss: 0.24869248270988464 | test loss: 0.19337768852710724 | accuracy: tensor([0.9667])\n",
      "Epoch: 913 | loss: 0.24850773811340332 | test loss: 0.19313594698905945 | accuracy: tensor([0.9667])\n",
      "Epoch: 914 | loss: 0.24832382798194885 | test loss: 0.19289520382881165 | accuracy: tensor([0.9667])\n",
      "Epoch: 915 | loss: 0.24814073741436005 | test loss: 0.19265548884868622 | accuracy: tensor([0.9667])\n",
      "Epoch: 916 | loss: 0.2479584813117981 | test loss: 0.192416712641716 | accuracy: tensor([0.9667])\n",
      "Epoch: 917 | loss: 0.247777059674263 | test loss: 0.19217897951602936 | accuracy: tensor([0.9667])\n",
      "Epoch: 918 | loss: 0.24759647250175476 | test loss: 0.19194218516349792 | accuracy: tensor([0.9667])\n",
      "Epoch: 919 | loss: 0.247416689991951 | test loss: 0.1917063593864441 | accuracy: tensor([0.9667])\n",
      "Epoch: 920 | loss: 0.2472376972436905 | test loss: 0.19147150218486786 | accuracy: tensor([0.9667])\n",
      "Epoch: 921 | loss: 0.24705949425697327 | test loss: 0.19123758375644684 | accuracy: tensor([0.9667])\n",
      "Epoch: 922 | loss: 0.2468821406364441 | test loss: 0.19100460410118103 | accuracy: tensor([0.9667])\n",
      "Epoch: 923 | loss: 0.2467055767774582 | test loss: 0.1907726228237152 | accuracy: tensor([0.9667])\n",
      "Epoch: 924 | loss: 0.24652977287769318 | test loss: 0.1905415803194046 | accuracy: tensor([0.9667])\n",
      "Epoch: 925 | loss: 0.24635475873947144 | test loss: 0.19031144678592682 | accuracy: tensor([0.9667])\n",
      "Epoch: 926 | loss: 0.24618050456047058 | test loss: 0.19008226692676544 | accuracy: tensor([0.9667])\n",
      "Epoch: 927 | loss: 0.246007040143013 | test loss: 0.1898539811372757 | accuracy: tensor([0.9667])\n",
      "Epoch: 928 | loss: 0.24583441019058228 | test loss: 0.18962664902210236 | accuracy: tensor([0.9667])\n",
      "Epoch: 929 | loss: 0.24566242098808289 | test loss: 0.18940022587776184 | accuracy: tensor([0.9667])\n",
      "Epoch: 930 | loss: 0.24549126625061035 | test loss: 0.18917469680309296 | accuracy: tensor([0.9667])\n",
      "Epoch: 931 | loss: 0.2453208714723587 | test loss: 0.1889500916004181 | accuracy: tensor([0.9667])\n",
      "Epoch: 932 | loss: 0.24515122175216675 | test loss: 0.18872638046741486 | accuracy: tensor([0.9667])\n",
      "Epoch: 933 | loss: 0.24498231709003448 | test loss: 0.18850357830524445 | accuracy: tensor([0.9667])\n",
      "Epoch: 934 | loss: 0.24481415748596191 | test loss: 0.18828167021274567 | accuracy: tensor([0.9667])\n",
      "Epoch: 935 | loss: 0.24464672803878784 | test loss: 0.18806062638759613 | accuracy: tensor([0.9667])\n",
      "Epoch: 936 | loss: 0.24448005855083466 | test loss: 0.18784046173095703 | accuracy: tensor([0.9667])\n",
      "Epoch: 937 | loss: 0.24431408941745758 | test loss: 0.18762117624282837 | accuracy: tensor([0.9667])\n",
      "Epoch: 938 | loss: 0.24414880573749542 | test loss: 0.18740272521972656 | accuracy: tensor([0.9667])\n",
      "Epoch: 939 | loss: 0.24398429691791534 | test loss: 0.18718522787094116 | accuracy: tensor([0.9667])\n",
      "Epoch: 940 | loss: 0.24382051825523376 | test loss: 0.18696853518486023 | accuracy: tensor([0.9667])\n",
      "Epoch: 941 | loss: 0.2436574101448059 | test loss: 0.18675270676612854 | accuracy: tensor([0.9667])\n",
      "Epoch: 942 | loss: 0.24349504709243774 | test loss: 0.1865377128124237 | accuracy: tensor([0.9667])\n",
      "Epoch: 943 | loss: 0.2433333545923233 | test loss: 0.18632358312606812 | accuracy: tensor([0.9667])\n",
      "Epoch: 944 | loss: 0.24317237734794617 | test loss: 0.18611030280590057 | accuracy: tensor([0.9667])\n",
      "Epoch: 945 | loss: 0.24301207065582275 | test loss: 0.1858978569507599 | accuracy: tensor([0.9667])\n",
      "Epoch: 946 | loss: 0.24285247921943665 | test loss: 0.18568621575832367 | accuracy: tensor([0.9667])\n",
      "Epoch: 947 | loss: 0.24269354343414307 | test loss: 0.1854754090309143 | accuracy: tensor([0.9667])\n",
      "Epoch: 948 | loss: 0.24253535270690918 | test loss: 0.185265451669693 | accuracy: tensor([0.9667])\n",
      "Epoch: 949 | loss: 0.24237780272960663 | test loss: 0.18505629897117615 | accuracy: tensor([0.9667])\n",
      "Epoch: 950 | loss: 0.2422209084033966 | test loss: 0.18484799563884735 | accuracy: tensor([0.9667])\n",
      "Epoch: 951 | loss: 0.2420646995306015 | test loss: 0.18464045226573944 | accuracy: tensor([0.9667])\n",
      "Epoch: 952 | loss: 0.24190916121006012 | test loss: 0.1844337284564972 | accuracy: tensor([0.9667])\n",
      "Epoch: 953 | loss: 0.24175429344177246 | test loss: 0.18422777950763702 | accuracy: tensor([0.9667])\n",
      "Epoch: 954 | loss: 0.24160009622573853 | test loss: 0.1840226799249649 | accuracy: tensor([0.9667])\n",
      "Epoch: 955 | loss: 0.24144652485847473 | test loss: 0.18381834030151367 | accuracy: tensor([0.9667])\n",
      "Epoch: 956 | loss: 0.24129357933998108 | test loss: 0.1836147904396057 | accuracy: tensor([0.9667])\n",
      "Epoch: 957 | loss: 0.24114134907722473 | test loss: 0.18341204524040222 | accuracy: tensor([0.9667])\n",
      "Epoch: 958 | loss: 0.24098968505859375 | test loss: 0.18321006000041962 | accuracy: tensor([0.9667])\n",
      "Epoch: 959 | loss: 0.24083870649337769 | test loss: 0.18300886452198029 | accuracy: tensor([0.9667])\n",
      "Epoch: 960 | loss: 0.24068835377693176 | test loss: 0.18280842900276184 | accuracy: tensor([0.9667])\n",
      "Epoch: 961 | loss: 0.24053865671157837 | test loss: 0.1826086938381195 | accuracy: tensor([0.9667])\n",
      "Epoch: 962 | loss: 0.24038955569267273 | test loss: 0.18240980803966522 | accuracy: tensor([0.9667])\n",
      "Epoch: 963 | loss: 0.24024108052253723 | test loss: 0.18221165239810944 | accuracy: tensor([0.9667])\n",
      "Epoch: 964 | loss: 0.24009323120117188 | test loss: 0.18201427161693573 | accuracy: tensor([0.9667])\n",
      "Epoch: 965 | loss: 0.23994597792625427 | test loss: 0.18181760609149933 | accuracy: tensor([0.9667])\n",
      "Epoch: 966 | loss: 0.2397993952035904 | test loss: 0.1816217005252838 | accuracy: tensor([0.9667])\n",
      "Epoch: 967 | loss: 0.23965336382389069 | test loss: 0.18142655491828918 | accuracy: tensor([0.9667])\n",
      "Epoch: 968 | loss: 0.23950797319412231 | test loss: 0.18123213946819305 | accuracy: tensor([0.9667])\n",
      "Epoch: 969 | loss: 0.2393631488084793 | test loss: 0.18103843927383423 | accuracy: tensor([0.9667])\n",
      "Epoch: 970 | loss: 0.23921895027160645 | test loss: 0.1808454841375351 | accuracy: tensor([0.9667])\n",
      "Epoch: 971 | loss: 0.23907536268234253 | test loss: 0.18065325915813446 | accuracy: tensor([0.9667])\n",
      "Epoch: 972 | loss: 0.23893237113952637 | test loss: 0.1804617941379547 | accuracy: tensor([0.9667])\n",
      "Epoch: 973 | loss: 0.23878994584083557 | test loss: 0.1802709698677063 | accuracy: tensor([0.9667])\n",
      "Epoch: 974 | loss: 0.23864811658859253 | test loss: 0.18008093535900116 | accuracy: tensor([0.9667])\n",
      "Epoch: 975 | loss: 0.23850683867931366 | test loss: 0.17989154160022736 | accuracy: tensor([0.9667])\n",
      "Epoch: 976 | loss: 0.23836620151996613 | test loss: 0.17970292270183563 | accuracy: tensor([0.9667])\n",
      "Epoch: 977 | loss: 0.2382260411977768 | test loss: 0.17951497435569763 | accuracy: tensor([0.9667])\n",
      "Epoch: 978 | loss: 0.23808653652668 | test loss: 0.17932772636413574 | accuracy: tensor([0.9667])\n",
      "Epoch: 979 | loss: 0.23794756829738617 | test loss: 0.17914117872714996 | accuracy: tensor([0.9667])\n",
      "Epoch: 980 | loss: 0.2378091812133789 | test loss: 0.1789553314447403 | accuracy: tensor([0.9667])\n",
      "Epoch: 981 | loss: 0.23767133057117462 | test loss: 0.17877015471458435 | accuracy: tensor([0.9667])\n",
      "Epoch: 982 | loss: 0.23753409087657928 | test loss: 0.1785857081413269 | accuracy: tensor([0.9667])\n",
      "Epoch: 983 | loss: 0.23739735782146454 | test loss: 0.1784018725156784 | accuracy: tensor([0.9667])\n",
      "Epoch: 984 | loss: 0.23726120591163635 | test loss: 0.1782187521457672 | accuracy: tensor([0.9667])\n",
      "Epoch: 985 | loss: 0.23712556064128876 | test loss: 0.17803630232810974 | accuracy: tensor([0.9667])\n",
      "Epoch: 986 | loss: 0.2369905263185501 | test loss: 0.1778545379638672 | accuracy: tensor([0.9667])\n",
      "Epoch: 987 | loss: 0.23685596883296967 | test loss: 0.17767341434955597 | accuracy: tensor([0.9667])\n",
      "Epoch: 988 | loss: 0.23672199249267578 | test loss: 0.17749296128749847 | accuracy: tensor([0.9667])\n",
      "Epoch: 989 | loss: 0.23658853769302368 | test loss: 0.1773131936788559 | accuracy: tensor([0.9667])\n",
      "Epoch: 990 | loss: 0.23645564913749695 | test loss: 0.17713405191898346 | accuracy: tensor([0.9667])\n",
      "Epoch: 991 | loss: 0.23632319271564484 | test loss: 0.17695559561252594 | accuracy: tensor([0.9667])\n",
      "Epoch: 992 | loss: 0.23619134724140167 | test loss: 0.17677778005599976 | accuracy: tensor([0.9667])\n",
      "Epoch: 993 | loss: 0.2360600084066391 | test loss: 0.1766006350517273 | accuracy: tensor([0.9667])\n",
      "Epoch: 994 | loss: 0.2359292209148407 | test loss: 0.17642410099506378 | accuracy: tensor([0.9667])\n",
      "Epoch: 995 | loss: 0.2357989102602005 | test loss: 0.1762482225894928 | accuracy: tensor([0.9667])\n",
      "Epoch: 996 | loss: 0.23566913604736328 | test loss: 0.17607298493385315 | accuracy: tensor([0.9667])\n",
      "Epoch: 997 | loss: 0.23553989827632904 | test loss: 0.17589835822582245 | accuracy: tensor([0.9667])\n",
      "Epoch: 998 | loss: 0.23541109263896942 | test loss: 0.17572440207004547 | accuracy: tensor([0.9667])\n",
      "Epoch: 999 | loss: 0.23528286814689636 | test loss: 0.17555107176303864 | accuracy: tensor([0.9667])\n",
      "Epoch: 1000 | loss: 0.23515507578849792 | test loss: 0.17537832260131836 | accuracy: tensor([0.9667])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df = pd.read_csv(r'D:\\DATASCIENCE\\DeepLearning-DS\\Chapter 03\\01. MLP Binary Classification\\data.csv', header=None)\n",
    "\n",
    "# print(df.head())\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_layer = nn.Linear(2, 16)  # input = num of features, out up to you\n",
    "        self.hidden1 = nn.Linear(16, 8) # input = out of previus layer, output up to you\n",
    "        self.hidden2 = nn.Linear(8, 4) # input = out of previus layer, output up to you\n",
    "        self.output = nn.Linear(4, 1) # input = out of previus layer, it depends on the task\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x): #forwardpass\n",
    "        first_layer = self.input_layer(x)\n",
    "        act1 = self.sigmoid(first_layer)\n",
    "        second_layer = self.hidden1(act1)\n",
    "        act2 = self.sigmoid(second_layer)\n",
    "        third_layer = self.hidden2(act2)\n",
    "        act3 = self.sigmoid(third_layer)\n",
    "        out_layer = self.output(act3)\n",
    "        # prediction = self.sigmoid(out_layer)\n",
    "        return self.sigmoid(out_layer)\n",
    "\n",
    "\n",
    "model = Classifier()\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = T.optim.Adam(model.parameters(), lr=1e-3) # 0.001 1 * 10^-3\n",
    "\n",
    "X = T.from_numpy(df[[0, 1]].values).float()\n",
    "y = T.from_numpy(df[[2]].values).float()\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=73)\n",
    "\n",
    "epochs = 1000\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    optimizer.zero_grad()   # 1st step: reset the gradients\n",
    "\n",
    "    pred = model.forward(x_train) #2nd step: make the prediction\n",
    "\n",
    "    train_loss = criterion(pred, y_train)   #3rd step: compute the loss\n",
    "\n",
    "    train_loss.backward() #4th step: backward pass\n",
    "\n",
    "    optimizer.step() #5th step: save the weights\n",
    "\n",
    "    model.eval()\n",
    "    with T.no_grad():\n",
    "        test_pred = model.forward(x_test)\n",
    "\n",
    "        test_loss = criterion(test_pred, y_test)\n",
    "\n",
    "        classes = test_pred > 0.5\n",
    "\n",
    "        acc = sum(classes == y_test) / classes.shape[0]\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    train_losses.append(train_loss.item())\n",
    "    test_losses.append(test_loss.item())\n",
    "    accuracies.append(acc)\n",
    "    print(f'Epoch: {epoch + 1} | loss: {train_loss.item()} | test loss: {test_loss.item()} | accuracy: {acc}')\n",
    "\n",
    "\n",
    "plt.plot(train_losses, label='train Loss')\n",
    "plt.plot(test_losses, label='test Loss')\n",
    "plt.plot(accuracies, label='accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b49991",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0.78051</th>\n",
       "      <th>-0.063669</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.28774</td>\n",
       "      <td>0.29139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.40714</td>\n",
       "      <td>0.17878</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.29230</td>\n",
       "      <td>0.42170</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.50922</td>\n",
       "      <td>0.35256</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.27785</td>\n",
       "      <td>0.10802</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>0.77029</td>\n",
       "      <td>0.70140</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>0.73156</td>\n",
       "      <td>0.71782</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>0.44556</td>\n",
       "      <td>0.57991</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0.85275</td>\n",
       "      <td>0.85987</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0.51912</td>\n",
       "      <td>0.62359</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows  3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0.78051  -0.063669  1\n",
       "0   0.28774    0.29139  1\n",
       "1   0.40714    0.17878  1\n",
       "2   0.29230    0.42170  1\n",
       "3   0.50922    0.35256  1\n",
       "4   0.27785    0.10802  1\n",
       "..      ...        ... ..\n",
       "94  0.77029    0.70140  0\n",
       "95  0.73156    0.71782  0\n",
       "96  0.44556    0.57991  0\n",
       "97  0.85275    0.85987  0\n",
       "98  0.51912    0.62359  0\n",
       "\n",
       "[99 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hazardous-weather",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:47:37.551793Z",
     "start_time": "2021-05-26T07:47:35.439944Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-racing",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:50:32.482086Z",
     "start_time": "2021-05-26T07:50:32.456893Z"
    }
   },
   "outputs": [],
   "source": [
    "# create a neural network class inheriting from the nn.Module\n",
    "# Call it NeuralNetwork and make, and use \"pass\" in the constructor\n",
    "# so that it doesn't give an error\n",
    "# Instantiate one instance of it in variable net\n",
    "\n",
    "net = 0\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        pass\n",
    "\n",
    "net = NeuralNetwork()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-honor",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:51:28.420569Z",
     "start_time": "2021-05-26T07:51:28.412916Z"
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(net, NeuralNetwork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-syndrome",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:11.203531Z",
     "start_time": "2021-05-26T07:56:11.199729Z"
    }
   },
   "outputs": [],
   "source": [
    "#Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim and num_hidden, respectively the dimension of \n",
    "# the input and the number of hidden neurons\n",
    "# use pass again\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    pass\n",
    "    def __init__(self, input_dim, num_hidden):\n",
    "        super(NeuralNetwork).__init__()\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-macro",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T07:56:32.252906Z",
     "start_time": "2021-05-26T07:56:32.247913Z"
    }
   },
   "outputs": [],
   "source": [
    "assert NeuralNetwork(input_dim=10, num_hidden=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bigger-inclusion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:27.491588Z",
     "start_time": "2021-05-26T08:04:27.484159Z"
    }
   },
   "outputs": [],
   "source": [
    "#Rewrite the NeuralNetwork class so that the constructor receives\n",
    "# as input the input_dim, num_hidden1 and num_hidden2, respectively the dimension of \n",
    "# the input and the number of hidden neurons for the first fully connected\n",
    "# layer and the second. Define the attributes in the constructor\n",
    "# that consists of the layers, call them fc1, fc2 and fc3 and a sigmoid.\n",
    "# use pass again. Be careful to put the dimensions in the right places!\n",
    "# Since we will do a binary classification problem, fc3 will have 1 neuron\n",
    "# as output\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden1, num_hidden2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, num_hidden1)\n",
    "        self.fc2 = nn.Linear(num_hidden1, num_hidden2)\n",
    "        self.fc3 = nn.Linear(num_hidden2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        layer1 = self.fc1(x)\n",
    "        act1 = self.sigmoid(layer1)\n",
    "        layer2 = self.fc2(act1)\n",
    "        act2 = self.sigmoid(layer2)\n",
    "        layer3 = self.fc3(act2)\n",
    "        out = self.sigmoid(layer3)\n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hawaiian-noise",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-26T08:04:48.612004Z",
     "start_time": "2021-05-26T08:04:48.606773Z"
    }
   },
   "outputs": [],
   "source": [
    "net = NeuralNetwork(16, 16, 16)\n",
    "assert net.fc1\n",
    "assert net.fc2\n",
    "assert net.fc3\n",
    "assert net.sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-southeast",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the forward pass to make a reasonable use of the attributes\n",
    "#you defined before. Follow the same reasoning we used in class\n",
    "\n",
    "model = NeuralNetwork(10, 7, 8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933260ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-sacramento",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For training a model, use the following optimizer and loss\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "loss = nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lesser-bunch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a neural network (feel free to choose the num_hidden1 and num_hidden2)\n",
    "# on the dataset in data.csv file\n",
    "# You'll have fun with conflicting shapes and types and tensors, but\n",
    "# you'll get those errors anyway. Let's go into the wild and learn\n",
    "# by reading the errors and trying to understand them! :)\n",
    "# You can always use the provided Workbook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
